{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchextractor"
      ],
      "metadata": {
        "id": "F0Efql5w-XvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1549c5b3-2372-4e08-fd57-68904fd3f326"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchextractor in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchextractor) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchextractor) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->torchextractor) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->torchextractor) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->torchextractor) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from os.path import join as oj\n",
        "from datetime import datetime\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from numpy.random import randint\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import copy\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "racKclkHP83h"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and create paths for directories\n"
      ],
      "metadata": {
        "id": "apqJXlN2Qh7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RpChcjDPw0Q",
        "outputId": "75a6d19d-045e-40f7-8ca7-7642acecac2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "dir_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224\"\n",
        "#dir_path = \"/content/drive/MyDrive/Projects/ISIC_224\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = oj(dir_path, \"models\", \"initial_classifier\")\n",
        "model_training_path = oj(model_path, \"training_224\")\n",
        "data_path = oj(dir_path, \"data\")\n",
        "\n",
        "not_cancer_path = oj(data_path, \"processed\", \"no_cancer_224\")\n",
        "cancer_path = oj(data_path, \"processed\", \"cancer_224\")"
      ],
      "metadata": {
        "id": "ik7Vaay-QmVZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments for training"
      ],
      "metadata": {
        "id": "a7Y1NXBlQp8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "mean = np.asarray([0.485, 0.456, 0.406])\n",
        "std = np.asarray([0.229, 0.224, 0.225])\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.00001\n",
        "        self.momentum = 0.9\n",
        "        self.seed = 42\n",
        "        self.regularizer_rate = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "num_epochs = args.epochs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier[-1] = nn.Linear(4096, 2)\n",
        "model = model.to(device)\n",
        "params_to_update = model.classifier.parameters()"
      ],
      "metadata": {
        "id": "dBAHXN3XQosJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clean up the image directories\n",
        "\n",
        "\n",
        "\n",
        "*   Remove empty images\n",
        "*   Remove duplicates which appear in a new folder but not the original.\n",
        "*   Ensure image sizes are all 224x224\n",
        "\n"
      ],
      "metadata": {
        "id": "90U5lSQfwkyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_empty_files(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = len(list_files)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        if os.path.getsize(oj(path, list_files[i])) < 100:\n",
        "            os.remove(oj(path, list_files[i]))\n",
        "            print(\"File \" + str(i) + \"deleted!\")\n",
        "'''\n",
        "def clean_up_duplicates(path1, path2):\n",
        "    newfiles = os.listdir(path1)\n",
        "    oldfiles = os.listdir(path2)\n",
        "    diff = [f for f in newfiles if f not in oldfiles]\n",
        "    for i in tqdm(diff):\n",
        "        os.remove(oj(path1, i))\n",
        "        print(\"File \" + str(i) + \"deleted!\")\n",
        "\n",
        "def check_img_sizes(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = len(list_files)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        im = Image.open(oj(path, list_files[i]))\n",
        "        if im.width != 224 or im.height != 224:\n",
        "            print(list_files[i])\n",
        "'''\n",
        "# clean_up_empty_files(cancer_path)\n",
        "# clean_up_empty_files(not_cancer_path)\n",
        "\n",
        "# newpath = oj(data_path, \"no_cancer_224_inpainted\")\n",
        "# oldpath = oj(data_path, \"processed\", \"no_cancer_224\")\n",
        "# clean_up_duplicates(newpath, oldpath)\n",
        "\n",
        "# check_img_sizes(not_cancer_path)\n"
      ],
      "metadata": {
        "id": "9Kz8kmvRw5xo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f8ccf0dc-bca2-4d18-8f8b-8ebabd724c06"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef clean_up_duplicates(path1, path2):\\n    newfiles = os.listdir(path1)\\n    oldfiles = os.listdir(path2)\\n    diff = [f for f in newfiles if f not in oldfiles]\\n    for i in tqdm(diff):\\n        os.remove(oj(path1, i))\\n        print(\"File \" + str(i) + \"deleted!\")\\n\\ndef check_img_sizes(path):\\n    list_files= os.listdir(path)\\n    num_files = len(list_files)\\n    for i in tqdm(range(num_files)):\\n        im = Image.open(oj(path, list_files[i]))\\n        if im.width != 224 or im.height != 224:\\n            print(list_files[i])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch dataset class"
      ],
      "metadata": {
        "id": "yaVcxFBQxH14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CancerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: str = None, is_cancer: int = None, data_files = None, labels = None):\n",
        "        \"\"\"\n",
        "        Expects path and is_cancer both to be supplied if the relevant images all lie in the same directory and have the same class\n",
        "        or a list of full filepaths and list of all labels are both supplied using data_files and labels otherwise.\n",
        "        \"\"\"\n",
        "        if path:\n",
        "            self.path = path\n",
        "            self.data_files = os.listdir(self.path)\n",
        "            self.is_cancer = is_cancer\n",
        "\n",
        "        else:\n",
        "            self.path = ''\n",
        "            self.data_files = data_files\n",
        "            self.labels = labels\n",
        "            self.is_cancer = None\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Read in the image, convert to float between [0,1] and standardise.\n",
        "        img = Image.open(oj(self.path, self.data_files[i]))\n",
        "        img_array = np.asarray(img)/255.0\n",
        "        img_array -= mean[None, None, :]\n",
        "        img_array /= std[None, None, :]\n",
        "        img.close()\n",
        "        torch_img = torch.from_numpy(img_array.swapaxes(0,2).swapaxes(1,2)).float()\n",
        "        # Take the global class if supplied, otherwise extract the relevant label from the list of labels.\n",
        "        is_cancer = self.is_cancer if self.is_cancer is not None else self.labels[i]\n",
        "        return (torch_img, is_cancer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)"
      ],
      "metadata": {
        "id": "CBXypVPHxKw-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for training"
      ],
      "metadata": {
        "id": "BxGJZUclxPuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_sum(im, target, model, crit, device='cuda'):\n",
        "    '''assume that eveything is already on cuda'''\n",
        "    im.requires_grad = True\n",
        "    grad_params = torch.abs(torch.autograd.grad(crit(model(im), target), im,create_graph = True)[0].sum(dim=1)).sum()\n",
        "    return grad_params\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, resume_training=False):\n",
        "    since = time.time()\n",
        "    # train_loss_history = []\n",
        "    # train_acc_history = []\n",
        "    # train_cd_history= []\n",
        "\n",
        "    best_loss = 10.0\n",
        "    patience = 3\n",
        "    cur_patience = 0\n",
        "    if len(os.listdir(model_training_path)) > 0 and resume_training:\n",
        "        model_list = [(f, os.path.getmtime(oj(model_training_path,f))) for f in os.listdir(model_training_path) if f.endswith('.pt')]\n",
        "        model_list.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place from most to least recent\n",
        "        model_name = model_list[0][0]\n",
        "        model.classifier.load_state_dict(torch.load(oj(model_training_path, model_name)))\n",
        "        print(\"Model loaded!\")\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        optimizer.step()\n",
        "        model.train()  # Set model to training mode\n",
        "        phase = 'train'\n",
        "        running_loss = 0.0\n",
        "        running_loss_cd = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for i, (inputs, labels) in tqdm(enumerate(dataloaders[phase])):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                # need to do calc beforehand because we do need the gradients\n",
        "                if phase == 'train' and regularizer_rate !=0:\n",
        "                    inputs.requires_grad = True\n",
        "                    add_loss = gradient_sum(inputs, labels, model, criterion)\n",
        "                    if add_loss!=0:\n",
        "                        (regularizer_rate*add_loss).backward()\n",
        "                        optimizer.step()\n",
        "                    #print(torch.cuda.memory_allocated()/(np.power(10,9)))\n",
        "                    optimizer.zero_grad()\n",
        "                    running_loss_cd += add_loss.item() * inputs.size(0)\n",
        "\n",
        "                    #inputs.require_grad = False\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if phase == 'train':\n",
        "                    (loss).backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_cd_loss = running_loss_cd / dataset_sizes[phase]\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f} CD Loss : {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc, epoch_cd_loss))\n",
        "\n",
        "        # train_loss_history.append(epoch_loss)\n",
        "        # train_cd_history.append(epoch_cd_loss)\n",
        "        # train_acc_history.append(epoch_acc.item())\n",
        "        torch.save(model.classifier.state_dict(), oj(model_training_path, datetime.now().strftime(\"%Y%m%d%H%M%S\") + \".pt\"))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60)\n",
        "    )\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    return model"
      ],
      "metadata": {
        "id": "AUBHiMpAxQuX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for evaluation"
      ],
      "metadata": {
        "id": "QVDur5K-xk7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import auc,average_precision_score, roc_curve,roc_auc_score,precision_recall_curve, f1_score\n",
        "\n",
        "def get_output(model, dataset):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    model = model.eval()\n",
        "    y = []\n",
        "    y_hat = []\n",
        "    softmax= torch.nn.Softmax()\n",
        "    with torch.no_grad() :\n",
        "        for inputs, labels in data_loader:\n",
        "            y_hat.append((labels).cpu().numpy())\n",
        "            y.append(torch.nn.Softmax(dim=1)( model(inputs.cuda()))[:,1].detach().cpu().numpy()) # take the probability for cancer\n",
        "    y_hat = np.concatenate( y_hat, axis=0 )\n",
        "    y = np.concatenate( y, axis=0 )\n",
        "    return y, y_hat # in the training set the values were switched\n",
        "\n",
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models we unfortunately saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:\n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "metadata": {
        "id": "fRG4cLfuxngv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Classifier Training"
      ],
      "metadata": {
        "id": "mpDo4dzRxqZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine datasets and split to train-test"
      ],
      "metadata": {
        "id": "kEEOZ8LXxvsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_dataset = CancerDataset(path=cancer_path, is_cancer=1)\n",
        "not_cancer_dataset = CancerDataset(path=not_cancer_path, is_cancer=0)\n",
        "complete_dataset = ConcatDataset((cancer_dataset, not_cancer_dataset))\n",
        "\n",
        "num_total = len(complete_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_test = num_total - num_train\n",
        "torch.manual_seed(0);\n",
        "print(\"num_train:\", num_train)\n",
        "print(\"num_test:\", num_test)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(complete_dataset, [num_train, num_test])\n",
        "datasets = {'train' : train_dataset, 'test':test_dataset}\n",
        "dataset_sizes = {'train' : len(train_dataset), 'test':len(test_dataset)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPy29IJ_xyx2",
        "outputId": "65b58812-cf00-41e4-dcdc-794d8db6c296"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train: 9340\n",
            "num_test: 2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the specific files in the training/test sets.\n"
      ],
      "metadata": {
        "id": "_8qVGPY_x1Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, test_idx = train_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, test_files"
      ],
      "metadata": {
        "id": "HQv-2S9kx4T2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Call the function and get the full file paths.\n",
        "train_files, test_files = extract_filenames(train_dataset, test_dataset)\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "metadata": {
        "id": "5w2Me7gXx7Km"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weights for training"
      ],
      "metadata": {
        "id": "YGwaiPUqx9KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the classes are unbalanced, we need to account for this in the loss function while training"
      ],
      "metadata": {
        "id": "UrJy_VbAx_q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_ratio = len(cancer_dataset)/len(complete_dataset)\n",
        "\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1/cancer_ratio\n",
        "not_cancer_weight = 1/ not_cancer_ratio\n",
        "weights = np.asarray([not_cancer_weight, cancer_weight])\n",
        "weights /= weights.sum()\n",
        "weights = torch.tensor(weights).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = weights.double().float())\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)"
      ],
      "metadata": {
        "id": "SWRPn6oVyCBW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save the model"
      ],
      "metadata": {
        "id": "fXhY2zi3yEfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs, resume_training=False)\n",
        "pid = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "torch.save(model.classifier.state_dict(),oj(dir_path, model_path, pid + \".pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbNn0nOayD7u",
        "outputId": "3e40b1ba-3155-44a4-a1d8-96c17f91f215"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3922 Acc: 0.8481 CD Loss : 0.0000\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2763 Acc: 0.8686 CD Loss : 0.0000\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2540 Acc: 0.8703 CD Loss : 0.0000\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2453 Acc: 0.8732 CD Loss : 0.0000\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2409 Acc: 0.8733 CD Loss : 0.0000\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2375 Acc: 0.8790 CD Loss : 0.0000\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2319 Acc: 0.8816 CD Loss : 0.0000\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2293 Acc: 0.8804 CD Loss : 0.0000\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2278 Acc: 0.8831 CD Loss : 0.0000\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:22,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2274 Acc: 0.8851 CD Loss : 0.0000\n",
            "Training complete in 23m 59s\n",
            "Best val loss: 10.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc, f1 = get_auc_f1(model, test_dataset)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"F1: \", f1)\n"
      ],
      "metadata": {
        "id": "I9FRBsdkcRhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe65d37-a9a5-4157-b888-40c8064b8acc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.9525435191270193\n",
            "F1:  0.49044585987261147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results_file_path = oj(dir_path, \"auc_f1_224_no_malig_patch.txt\")\n",
        "print(results_file_path)\n",
        "with open(results_file_path, 'w') as f:\n",
        "    f.write('AUC: ' + str(auc) + \"\\n\")\n",
        "    f.write('F1: ' + str(f1) + \"\\n\")"
      ],
      "metadata": {
        "id": "HbIm8MQZcWuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac8f2aa-21a7-44e1-a77c-5dbfb0452542"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/auc_f1_224_no_malig_patch.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload the model to skip retraining and test with patched images"
      ],
      "metadata": {
        "id": "s-sL5LmyZ14p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_patch_path = '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224'\n",
        "not_cancer_path = '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/data/processed/no_cancer_224'"
      ],
      "metadata": {
        "id": "hLmXe-BKd7d6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_patch_dataset = CancerDataset(path=cancer_patch_path, is_cancer=1)\n",
        "not_cancer_dataset = CancerDataset(path=not_cancer_path, is_cancer=0)\n",
        "complete_patch_dataset = ConcatDataset((cancer_patch_dataset, not_cancer_dataset))\n",
        "\n",
        "num_total = len(complete_patch_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_test = num_total - num_train\n",
        "torch.manual_seed(0);\n",
        "print(\"num_train:\", num_train)\n",
        "print(\"num_test:\", num_test)\n",
        "\n",
        "train_patch_dataset, test_patch_dataset = torch.utils.data.random_split(complete_patch_dataset, [num_train, num_test])\n",
        "datasets = {'train' : train_patch_dataset, 'test':test_patch_dataset}\n",
        "dataset_sizes = {'train' : len(train_patch_dataset), 'test':len(test_patch_dataset)}\n",
        "\n",
        "# only with malig patches\n",
        "#cancer_patch_dataset = test_patch_dataset\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiYtVQKQcCiQ",
        "outputId": "f4d8003c-da66-41dc-f0d6-964df2d895fe"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train: 9340\n",
            "num_test: 2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, test_idx = train_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, test_files"
      ],
      "metadata": {
        "id": "rzfQCxdPcIen"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Call the function and get the full file paths.\n",
        "train_patch_files, test_patch_files = extract_filenames(train_patch_dataset, test_patch_dataset)\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "metadata": {
        "id": "ypnVoR1weu3K"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models they saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:\n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "metadata": {
        "id": "ED7P9eHcknCA"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(model, dataset, filename):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for inputs, labels in dataset:\n",
        "        inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
        "        inputs = inputs.to(device)  # Move data to appropriate device\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Append predictions to the list\n",
        "        predictions.append(predicted.item())\n",
        "\n",
        "        # Check if labels are integers or tensors\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            true_labels.append(labels.item())\n",
        "        else:\n",
        "            true_labels.append(labels)  # Assume labels are integers\n",
        "\n",
        "    # Create a DataFrame to store predictions and true labels\n",
        "    df = pd.DataFrame({\n",
        "        'Prediction': predictions,\n",
        "        'True Label': true_labels\n",
        "    })\n",
        "\n",
        "    # Save DataFrame to CSV file\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Predictions saved to {filename}\")"
      ],
      "metadata": {
        "id": "YtXdQ9JFhX_G"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier[-1] = nn.Linear(4096, 2)  # Modify classifier\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240601213813.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# cancer_dataset and complete_dataset are already defined\n",
        "cancer_ratio = len(cancer_dataset) / len(complete_dataset)\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1 / cancer_ratio\n",
        "not_cancer_weight = 1 / not_cancer_ratio\n",
        "weights = torch.tensor([not_cancer_weight, cancer_weight], device=device, dtype=torch.float)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.00001\n",
        "        self.momentum = 0.9\n",
        "        self.seed = 42\n",
        "        self.regularizer_rate = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "num_epochs = args.epochs\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "params_to_update = model.classifier.parameters()\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)\n"
      ],
      "metadata": {
        "id": "_NYeroADbgBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3da913-ecbf-47e2-aa23-9616fa20deb4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc, f1 = get_auc_f1(model, test_patch_dataset)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"F1: \", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc0o9QBgyI0e",
        "outputId": "2b3d8dca-1a0c-42d1-cec4-8adf31659b54"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.9566642722868366\n",
            "F1:  0.5233644859813084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions(model, test_patch_dataset, '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/patch_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEyz8zK4m4fN",
        "outputId": "1c6ba3e7-5935-4aa0-d215-811d5b64647a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/patch_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and extract activations from last layer"
      ],
      "metadata": {
        "id": "8BWlccI3VmOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Modify the classifier\n",
        "model.classifier[-1] = torch.nn.Linear(4096, 2)\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240601213813.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def preprocess_and_extract_activations(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path)\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Flatten the tensor before passing it to the linear layers\n",
        "    image_tensor = model.features(image_tensor)\n",
        "    image_tensor = model.avgpool(image_tensor)\n",
        "    image_tensor = torch.flatten(image_tensor, 1)  # Flatten the tensor\n",
        "\n",
        "    # Pass the tensor through the first 5 layers\n",
        "    for layer_idx, layer in enumerate(model.classifier[:5], start=1):\n",
        "        image_tensor = layer(image_tensor)\n",
        "\n",
        "    # Extract activations from the sixth linear layer\n",
        "    activations = model.classifier[5](image_tensor)\n",
        "    activations = activations.squeeze().cpu().detach().numpy()\n",
        "    #print(\"Activation size:\", activations.shape)\n",
        "\n",
        "    return activations\n",
        "\n",
        "# Function to recursively traverse folders and process images\n",
        "def process_images_in_folder(folder_path):\n",
        "    all_activations = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg')):\n",
        "                image_path = os.path.join(root, file)\n",
        "                activations = preprocess_and_extract_activations(image_path)\n",
        "                if activations is not None:\n",
        "                    all_activations.append(activations)\n",
        "    return all_activations\n",
        "\n",
        "# Folder path containing  images\n",
        "patch_cancer_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "no_patch_cancer_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224\"\n",
        "\n",
        "# Extract activations for all images in the folder\n",
        "wp_cancer_all_activations = process_images_in_folder(patch_cancer_folder_path)\n",
        "wo_cancer_all_activations = process_images_in_folder(no_patch_cancer_folder_path)\n",
        "\n",
        "if wp_cancer_all_activations:\n",
        "    print(\"wp_cancer_all_activations shape:\", np.vstack(wp_cancer_all_activations).shape)\n",
        "else:\n",
        "    print(\"No activations found in wp_cancer_all_activations\")\n",
        "\n",
        "if wo_cancer_all_activations:\n",
        "    print(\"wo_cancer_all_activations shape:\", np.vstack(wo_cancer_all_activations).shape)\n",
        "else:\n",
        "    print(\"No activations found in wo_cancer_all_activations\")\n",
        "\n",
        "#folder_path = \"/content/drive/MyDrive/Projects/ISIC_224/malignant_nopatch_224\"\n",
        "\n",
        "# Extract activations for all images in the folder\n",
        "#all_activations = process_images_in_folder(folder_path)"
      ],
      "metadata": {
        "id": "yRcHWdJ9TCT8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "97037bdb-f32e-4d68-b0ff-db3b0614c4dd"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224/ISIC_0010792.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-3db6cafe1f29>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Extract activations for all images in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mwp_cancer_all_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_images_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_cancer_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mwo_cancer_all_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_images_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_patch_cancer_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwp_cancer_all_activations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-3db6cafe1f29>\u001b[0m in \u001b[0;36mprocess_images_in_folder\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_and_extract_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mall_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-3db6cafe1f29>\u001b[0m in \u001b[0;36mpreprocess_and_extract_activations\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_and_extract_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Load and preprocess the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3283\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224/ISIC_0010792.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#np.vstack(all_activations).shape"
      ],
      "metadata": {
        "id": "PYuRXhFSlO_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save activations for malignant wit patches as npy array (Sari likes npy, I like csv)\n",
        "np.save('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/cancer_test_wp_activations.npy',np.vstack(wp_cancer_all_activations))\n",
        "np.save('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/cancer_test_wo_activations.npy',np.vstack(wo_cancer_all_activations))"
      ],
      "metadata": {
        "id": "1wckbu5FbsZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import shutil\n",
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "files= os.listdir(folder_path)\n",
        "for imgname in files:\n",
        "    shutil.copyfile('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/data/processed/cancer_224/'+imgname,'/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224/'+imgname)\n",
        "'''"
      ],
      "metadata": {
        "id": "480gt9JTgl4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "files= os.listdir(folder_path)\n",
        "num_mal_test = len(files)\n",
        "patch_no_patch = np.hstack([np.zeros((1,num_mal_test)),np.ones((1,num_mal_test))])\n",
        "wp = np.load('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/cancer_test_wp_activations.npy')\n",
        "wo = np.load('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/cancer_test_wo_activations.npy')\n",
        "wop_activations = np.vstack([wo,wp])"
      ],
      "metadata": {
        "id": "GSKFLNwzkHFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wop_activations.shape"
      ],
      "metadata": {
        "id": "iLU5HHwtlqQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_no_patch.shape"
      ],
      "metadata": {
        "id": "VCun3nAhlsPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_arrays = np.concatenate((patch_no_patch.T, wop_activations), axis=1) # 900x568\n",
        "corr = np.corrcoef(two_arrays.T)"
      ],
      "metadata": {
        "id": "ef-tZX0Jlz_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#activations_corr.shape"
      ],
      "metadata": {
        "id": "I8XztnE8mcu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "activations_corr = np.abs(corr[0][1:])\n",
        "_ = plt.hist(activations_corr, bins='auto')"
      ],
      "metadata": {
        "id": "a3Kuj1Y0l7wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.classifier)\n"
      ],
      "metadata": {
        "id": "PjyXUMDIIgSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlations(original_activations, patched_activations):\n",
        "    # Convert activation lists to arrays\n",
        "    original_activations_array = np.array([act[0] for act in original_activations])\n",
        "    patched_activations_array = np.array([act[0] for act in patched_activations])\n",
        "\n",
        "    # Ensure activations arrays have the correct shape\n",
        "    original_activations_array = original_activations_array.reshape(len(original_activations_array), -1)\n",
        "    patched_activations_array = patched_activations_array.reshape(len(patched_activations_array), -1)\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = np.corrcoef(original_activations_array.T, patched_activations_array.T)\n",
        "\n",
        "    # Extract correlations for each neuron\n",
        "    neuron_correlations = correlations[:4096, 4096:]\n",
        "\n",
        "    return neuron_correlations\n",
        "\n",
        "\n",
        "# Extract activations for original and patched MAL images\n",
        "original_activations = preprocess_and_extract_activations(model, test_dataset)\n",
        "patched_activations = preprocess_and_extract_activations(model, test_patch_dataset)\n",
        "\n",
        "# Calculate correlations for each neuron\n",
        "neuron_correlations = calculate_correlations(original_activations, patched_activations)\n",
        "\n",
        "# Print correlations for the first neuron as an example\n",
        "print(\"Correlations for the first neuron:\", neuron_correlations[0])"
      ],
      "metadata": {
        "id": "e6voBGuw5dB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}