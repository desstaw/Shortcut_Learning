{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGPSiNBsE5dw",
        "outputId": "9a06fe1a-f1a4-44d6-e18e-a48cac99df09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchextractor\n",
            "  Downloading torchextractor-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchextractor) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchextractor) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->torchextractor) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->torchextractor) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchextractor\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchextractor-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchextractor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from os.path import join as oj\n",
        "from datetime import datetime\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from numpy.random import randint\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import copy\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "NdUvujavGPYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and create paths for directories"
      ],
      "metadata": {
        "id": "EUz9jEbXGd-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "dir_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224\"\n",
        "#dir_path = \"/content/drive/MyDrive/Projects/ISIC_224\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHOHl9wdGY0x",
        "outputId": "ecc090a5-3c57-4165-8b0a-2c4a98f0a13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = oj(dir_path, \"models\", \"initial_classifier\")\n",
        "model_training_path = oj(model_path, \"training_224\")\n",
        "data_path = oj(dir_path, \"data\")\n",
        "\n",
        "not_cancer_path = oj(data_path, \"processed\", \"no_cancer_224\")\n",
        "cancer_path = oj(data_path, \"processed\", \"cancer_224\")"
      ],
      "metadata": {
        "id": "K6g6DGaiGfpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arguments for training"
      ],
      "metadata": {
        "id": "InfRNWHQGiHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mean and Standard Deviation: These are normalization parameters used for preprocessing images. They are often used in computer vision tasks to scale pixel values to a common range.\n",
        "\n",
        "2. Device Configuration: It determines whether the code should run on CPU or GPU (cuda) based on the availability of CUDA support. CUDA is a parallel computing platform and application programming interface model created by NVIDIA. If CUDA is available, the code will run on the GPU; otherwise, it will use the CPU.\n",
        "\n",
        "3. Model Initialization: It loads a pre-trained VGG16 model from torchvision.models, modifies the last layer of the classifier to output 2 classes (it seems to be for some kind of classification task), moves the model to the specified device (CPU or GPU), and sets the parameters to update during training (in this case, only the parameters of the classifier).\n",
        "\n",
        "❗❗❗*params_to_update = model.classifier.parameters(): This specifies the parameters of the model that will be updated during training. In this case, it selects only the parameters of the classifier, excluding the parameters of the feature extractor.*"
      ],
      "metadata": {
        "id": "WZDlR9A0Glip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "mean = np.asarray([0.485, 0.456, 0.406])\n",
        "std = np.asarray([0.229, 0.224, 0.225])\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.00001\n",
        "        self.momentum = 0.9\n",
        "        self.seed = 42\n",
        "        self.regularizer_rate = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "num_epochs = args.epochs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#model = models.vgg16(pretrained=True)\n",
        "model = models.alexnet(pretrained=True)\n",
        "\n",
        "model.classifier[-1] = nn.Linear(4096, 2)\n",
        "model = model.to(device)\n",
        "params_to_update = model.classifier.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS7pxmvvGhyB",
        "outputId": "96aedcff-d335-4ebb-ae45-aa3358d284a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:02<00:00, 101MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean up the image directories\n",
        "\n",
        "*   Remove empty images\n",
        "*   Remove duplicates which appear in a new folder but not the original.\n",
        "*   Ensure image sizes are all 224x224"
      ],
      "metadata": {
        "id": "1R25kga4GpLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_empty_files(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = len(list_files)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        if os.path.getsize(oj(path, list_files[i])) < 100:\n",
        "            os.remove(oj(path, list_files[i]))\n",
        "            print(\"File \" + str(i) + \"deleted!\")\n",
        "'''\n",
        "def clean_up_duplicates(path1, path2):\n",
        "    newfiles = os.listdir(path1)\n",
        "    oldfiles = os.listdir(path2)\n",
        "    diff = [f for f in newfiles if f not in oldfiles]\n",
        "    for i in tqdm(diff):\n",
        "        os.remove(oj(path1, i))\n",
        "        print(\"File \" + str(i) + \"deleted!\")\n",
        "\n",
        "def check_img_sizes(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = len(list_files)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        im = Image.open(oj(path, list_files[i]))\n",
        "        if im.width != 224 or im.height != 224:\n",
        "            print(list_files[i])\n",
        "'''\n",
        "# clean_up_empty_files(cancer_path)\n",
        "# clean_up_empty_files(not_cancer_path)\n",
        "\n",
        "# newpath = oj(data_path, \"no_cancer_224_inpainted\")\n",
        "# oldpath = oj(data_path, \"processed\", \"no_cancer_224\")\n",
        "# clean_up_duplicates(newpath, oldpath)\n",
        "\n",
        "# check_img_sizes(not_cancer_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uTJvq4eZGsxp",
        "outputId": "01515166-6ed6-4506-86b6-99a029c8e8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef clean_up_duplicates(path1, path2):\\n    newfiles = os.listdir(path1)\\n    oldfiles = os.listdir(path2)\\n    diff = [f for f in newfiles if f not in oldfiles]\\n    for i in tqdm(diff):\\n        os.remove(oj(path1, i))\\n        print(\"File \" + str(i) + \"deleted!\")\\n\\ndef check_img_sizes(path):\\n    list_files= os.listdir(path)\\n    num_files = len(list_files)\\n    for i in tqdm(range(num_files)):\\n        im = Image.open(oj(path, list_files[i]))\\n        if im.width != 224 or im.height != 224:\\n            print(list_files[i])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch dataset class\n",
        "**Purpose**:\n",
        "Defines a custom dataset class for handling cancer image data in PyTorch.\n",
        "Key Components:\n",
        "\n",
        "**Class Definition**:\n",
        "CancerDataset inherits from torch.utils.data.Dataset.\n",
        "\n",
        "Initialization `(__init__ method)`:\n",
        "*   Sets up the dataset.\n",
        "*   Accepts two ways of specifying data:\n",
        "    1. By Directory: Provide directory path and a label.\n",
        "    2. By Filepaths and Labels: Provide list of file paths and corresponding labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Get Item `(__getitem__ method)`:\n",
        "*   Loads and processes an image.\n",
        "*   Converts image to numpy array, scales pixel values to [0, 1].\n",
        "*   Standardizes the image.\n",
        "*   Converts the image to a PyTorch tensor.\n",
        "*   Returns the image tensor and label.\n",
        "\n",
        "\n",
        "Length `(__len__ method)`:\n",
        "*   Returns the total number of images in the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Workflow**:\n",
        "1. Initialization: Create an instance of CancerDataset.\n",
        "2. DataLoader: Use DataLoader to load data in batches, shuffle, and handle multi-threaded loading.\n",
        "3. Training: Iterate over DataLoader in the training loop."
      ],
      "metadata": {
        "id": "QTWADXhSGvSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CancerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: str = None, is_cancer: int = None, data_files = None, labels = None):\n",
        "        \"\"\"\n",
        "        Expects path and is_cancer both to be supplied if the relevant images all lie in the same directory and have the same class\n",
        "        or a list of full filepaths and list of all labels are both supplied using data_files and labels otherwise.\n",
        "        \"\"\"\n",
        "        if path:\n",
        "            self.path = path\n",
        "            self.data_files = os.listdir(self.path)\n",
        "            self.is_cancer = is_cancer\n",
        "\n",
        "        else:\n",
        "            self.path = ''\n",
        "            self.data_files = data_files\n",
        "            self.labels = labels\n",
        "            self.is_cancer = None\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Read in the image from the specified path and convert it to a float array with values between [0, 1]\n",
        "        img = Image.open(os.path.join(self.path, self.data_files[i]))\n",
        "        img_array = np.asarray(img) / 255.0\n",
        "\n",
        "        # Standardize the image by subtracting the mean and dividing by the standard deviation\n",
        "        img_array -= mean[None, None, :]  # mean and std should be defined elsewhere in the code\n",
        "        img_array /= std[None, None, :]\n",
        "\n",
        "        # Close the image to free up resources\n",
        "        img.close()\n",
        "\n",
        "        # Convert the numpy array to a PyTorch tensor\n",
        "        # swapaxes(0, 2) and swapaxes(1, 2) rearrange the dimensions from (height, width, channels) to (channels, height, width)\n",
        "        torch_img = torch.from_numpy(img_array.swapaxes(0, 2).swapaxes(1, 2)).float()\n",
        "\n",
        "        # Determine the label for the image: use the global class label if provided,\n",
        "        # otherwise, use the label from the labels list\n",
        "        is_cancer = self.is_cancer if self.is_cancer is not None else self.labels[i]\n",
        "\n",
        "        # Return the processed image tensor and its corresponding label\n",
        "        return (torch_img, is_cancer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)"
      ],
      "metadata": {
        "id": "_VqAbjqlGwQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions for training\n",
        "\n",
        "`gradient_sum`\n",
        "calculates the sum of gradients of the loss with respect to the input image, which can be used for regularization in training.\n",
        "\n",
        "`torch.autograd.grad` computes the gradients of the loss (defined by crit) with respect to the input image. The `model(im)` gives the model's output for the image, and target is the ground truth label.\n",
        "\n",
        "`torch.abs(...).sum(dim=1)).sum()` computes the absolute values of the gradients, sums them along the first dimension, and then sums all of them together.\n"
      ],
      "metadata": {
        "id": "UKYi7Ua8Gx64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_sum(im, target, model, crit, device='cuda'):\n",
        "    '''Calculate the sum of gradients for regularization purposes.'''\n",
        "\n",
        "    # Set requires_grad to True to enable gradient computation with respect to the input image\n",
        "    im.requires_grad = True\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the input image\n",
        "    grad_params = torch.abs(torch.autograd.grad(crit(model(im), target), im, create_graph=True)[0].sum(dim=1)).sum()\n",
        "\n",
        "    # Return the sum of absolute gradients\n",
        "    return grad_params\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, resume_training=False):\n",
        "    '''Train the model using the specified parameters and data.'''\n",
        "\n",
        "    since = time.time()  # Record the start time for training duration\n",
        "\n",
        "    # Initialize training metrics and early stopping parameters\n",
        "    best_loss = 10.0\n",
        "    patience = 3\n",
        "    cur_patience = 0\n",
        "\n",
        "    # Check if there's a need to resume training from a saved model\n",
        "    if len(os.listdir(model_training_path)) > 0 and resume_training:\n",
        "        # Get a list of saved models sorted by modification time (most recent first)\n",
        "        model_list = [(f, os.path.getmtime(os.path.join(model_training_path, f))) for f in os.listdir(model_training_path) if f.endswith('.pt')]\n",
        "        model_list.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        # Load the most recent model checkpoint\n",
        "        model_name = model_list[0][0]\n",
        "        model.classifier.load_state_dict(torch.load(os.path.join(model_training_path, model_name)))\n",
        "        print(\"Model loaded!\")\n",
        "\n",
        "    # Loop through each epoch\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Perform optimizer step (usually called after loss.backward())\n",
        "        optimizer.step()\n",
        "\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        phase = 'train'\n",
        "\n",
        "        # Initialize running statistics\n",
        "        running_loss = 0.0\n",
        "        running_loss_cd = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over the data loader for the current phase (training)\n",
        "        for i, (inputs, labels) in tqdm(enumerate(dataloaders[phase])):\n",
        "\n",
        "            # Move inputs and labels to the specified device (e.g., GPU)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Enable gradient calculation if in the training phase\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                # If using a regularizer, calculate the additional loss\n",
        "                if phase == 'train' and regularizer_rate != 0:\n",
        "                    inputs.requires_grad = True\n",
        "                    add_loss = gradient_sum(inputs, labels, model, criterion)\n",
        "\n",
        "                    # Backpropagate the additional loss if it's non-zero\n",
        "                    if add_loss != 0:\n",
        "                        (regularizer_rate * add_loss).backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # Zero the parameter gradients again\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Accumulate the additional loss\n",
        "                    running_loss_cd += add_loss.item() * inputs.size(0)\n",
        "\n",
        "                # Forward pass: compute model outputs and loss\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimization step in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Update running statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Compute epoch statistics\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_cd_loss = running_loss_cd / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f} CD Loss : {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_cd_loss))\n",
        "\n",
        "        # train_loss_history.append(epoch_loss)\n",
        "        # train_cd_history.append(epoch_cd_loss)\n",
        "        # train_acc_history.append(epoch_acc.item())\n",
        "\n",
        "        # Save the model checkpoint with model name\n",
        "        model_name = type(model).__name__\n",
        "        checkpoint_name = f\"{model_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}.pt\"\n",
        "        torch.save(model.classifier.state_dict(), os.path.join(model_training_path, checkpoint_name))\n",
        "\n",
        "\n",
        "    # Print the total training duration\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # Return the trained model\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ejnm-lVR4SG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for evaluation"
      ],
      "metadata": {
        "id": "OZP4k9w4G1mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import auc,average_precision_score, roc_curve,roc_auc_score,precision_recall_curve, f1_score\n",
        "\n",
        "def get_output(model, dataset):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    model = model.eval()\n",
        "    y = []\n",
        "    y_hat = []\n",
        "    softmax= torch.nn.Softmax()\n",
        "    with torch.no_grad() :\n",
        "        for inputs, labels in data_loader:\n",
        "            y_hat.append((labels).cpu().numpy())\n",
        "            y.append(torch.nn.Softmax(dim=1)( model(inputs.cuda()))[:,1].detach().cpu().numpy()) # take the probability for cancer\n",
        "    y_hat = np.concatenate( y_hat, axis=0 )\n",
        "    y = np.concatenate( y, axis=0 )\n",
        "    return y, y_hat # in the training set the values were switched\n",
        "\n",
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models we unfortunately saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:\n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "metadata": {
        "id": "8ScRZyr4G2Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Classifier Training"
      ],
      "metadata": {
        "id": "GKwFfyqwG37g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine datasets and split to train-test"
      ],
      "metadata": {
        "id": "m0wUdaUlG64B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_dataset = CancerDataset(path=cancer_path, is_cancer=1)\n",
        "not_cancer_dataset = CancerDataset(path=not_cancer_path, is_cancer=0)\n",
        "complete_dataset = ConcatDataset((cancer_dataset, not_cancer_dataset))\n",
        "\n",
        "num_total = len(complete_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_test = num_total - num_train\n",
        "torch.manual_seed(0);\n",
        "print(\"num_train:\", num_train)\n",
        "print(\"num_test:\", num_test)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(complete_dataset, [num_train, num_test])\n",
        "datasets = {'train' : train_dataset, 'test':test_dataset}\n",
        "dataset_sizes = {'train' : len(train_dataset), 'test':len(test_dataset)}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FupteMxwG8EZ",
        "outputId": "4e6f5b43-b829-4b4a-8d9b-3b652f4122e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train: 9340\n",
            "num_test: 2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the specific files in the training/test sets.\n"
      ],
      "metadata": {
        "id": "5qCgrVJPG9v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, test_idx = train_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, test_files"
      ],
      "metadata": {
        "id": "DhT-q2R8G-pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Call the function and get the full file paths.\n",
        "train_files, test_files = extract_filenames(train_dataset, test_dataset)\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "metadata": {
        "id": "B7w6HV-bG_yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights for training"
      ],
      "metadata": {
        "id": "vaVvxjeDHA_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the classes are unbalanced, we need to account for this in the loss function while training"
      ],
      "metadata": {
        "id": "GVPYE9zpHDPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_ratio = len(cancer_dataset)/len(complete_dataset)\n",
        "\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1/cancer_ratio\n",
        "not_cancer_weight = 1/ not_cancer_ratio\n",
        "weights = np.asarray([not_cancer_weight, cancer_weight])\n",
        "weights /= weights.sum()\n",
        "weights = torch.tensor(weights).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = weights.double().float())\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)"
      ],
      "metadata": {
        "id": "_jsX2Kf4HEn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save the model"
      ],
      "metadata": {
        "id": "ydm-FlIwHF-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs, resume_training=False)\n",
        "pid = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "torch.save(model.classifier.state_dict(),oj(dir_path, model_path, pid + \".pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq5i1BZTHHFh",
        "outputId": "7faa89e3-71c1-4e09-9df8-be667909472a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "583it [09:01,  2.93it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "584it [09:01,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2821 Acc: 0.8679 CD Loss : 0.0000\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [01:04,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2394 Acc: 0.8807 CD Loss : 0.0000\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [01:03,  9.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2296 Acc: 0.8828 CD Loss : 0.0000\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [01:11,  8.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2198 Acc: 0.8892 CD Loss : 0.0000\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [01:05,  8.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2151 Acc: 0.8892 CD Loss : 0.0000\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [00:51, 11.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2147 Acc: 0.8891 CD Loss : 0.0000\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [01:01,  9.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2096 Acc: 0.8974 CD Loss : 0.0000\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [00:54, 10.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2130 Acc: 0.8909 CD Loss : 0.0000\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [00:50, 11.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2121 Acc: 0.8934 CD Loss : 0.0000\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [00:51, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2060 Acc: 0.8940 CD Loss : 0.0000\n",
            "Training complete in 18m 7s\n",
            "Best val loss: 10.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc, f1 = get_auc_f1(model, test_dataset)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"F1: \", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-Zehz0PHIOR",
        "outputId": "d8e6164e-608b-44eb-d347-e0427935034a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.9629055799238891\n",
            "F1:  0.5358255451713395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results_file_path = oj(dir_path, \"auc_f1_224_no_malig_patch.txt\")\n",
        "print(results_file_path)\n",
        "with open(results_file_path, 'w') as f:\n",
        "    f.write('AUC: ' + str(auc) + \"\\n\")\n",
        "    f.write('F1: ' + str(f1) + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLtTSD1MHJUR",
        "outputId": "2aeab659-45f8-4fde-bbef-e0790dd3b6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/auc_f1_224_no_malig_patch.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload the model to skip retraining and test with patched images"
      ],
      "metadata": {
        "id": "xkybrGJ1HL7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_patch_path = '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224'\n",
        "not_cancer_path = '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/data/processed/no_cancer_224'"
      ],
      "metadata": {
        "id": "brmF0mLaHLJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_patch_dataset = CancerDataset(path=cancer_patch_path, is_cancer=1)\n",
        "not_cancer_dataset = CancerDataset(path=not_cancer_path, is_cancer=0)\n",
        "complete_patch_dataset = ConcatDataset((cancer_patch_dataset, not_cancer_dataset))\n",
        "\n",
        "num_total = len(complete_patch_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_test = num_total - num_train\n",
        "torch.manual_seed(0);\n",
        "print(\"num_train:\", num_train)\n",
        "print(\"num_test:\", num_test)\n",
        "\n",
        "train_patch_dataset, test_patch_dataset = torch.utils.data.random_split(complete_patch_dataset, [num_train, num_test])\n",
        "datasets = {'train' : train_patch_dataset, 'test':test_patch_dataset}\n",
        "dataset_sizes = {'train' : len(train_patch_dataset), 'test':len(test_patch_dataset)}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQoGG2D2HPIB",
        "outputId": "207e76e7-4b83-4245-90b4-e277d4608da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train: 9340\n",
            "num_test: 2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, test_idx = train_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, test_files"
      ],
      "metadata": {
        "id": "mq1baD3HHQwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Call the function and get the full file paths.\n",
        "train_patch_files, test_patch_files = extract_filenames(train_patch_dataset, test_patch_dataset)\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "metadata": {
        "id": "kqVkSeCeHR-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models they saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:\n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "metadata": {
        "id": "sOok88bsHS6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(model, dataset, filename):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for inputs, labels in dataset:\n",
        "        inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
        "        inputs = inputs.to(device)  # Move data to appropriate device\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Append predictions to the list\n",
        "        predictions.append(predicted.item())\n",
        "\n",
        "        # Check if labels are integers or tensors\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            true_labels.append(labels.item())\n",
        "        else:\n",
        "            true_labels.append(labels)  # Assume labels are integers\n",
        "\n",
        "    # Create a DataFrame to store predictions and true labels\n",
        "    df = pd.DataFrame({\n",
        "        'Prediction': predictions,\n",
        "        'True Label': true_labels\n",
        "    })\n",
        "\n",
        "    # Save DataFrame to CSV file\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Predictions saved to {filename}\")"
      ],
      "metadata": {
        "id": "mPdJNuK7HT7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = models.vgg16(pretrained=True)\n",
        "model = models.alexnet(pretrained=True)\n",
        "\n",
        "model.classifier[-1] = nn.Linear(4096, 2)  # Modify classifier\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240514015922.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# cancer_dataset and complete_dataset are already defined\n",
        "cancer_ratio = len(cancer_dataset) / len(complete_dataset)\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1 / cancer_ratio\n",
        "not_cancer_weight = 1 / not_cancer_ratio\n",
        "weights = torch.tensor([not_cancer_weight, cancer_weight], device=device, dtype=torch.float)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.00001\n",
        "        self.momentum = 0.9\n",
        "        self.seed = 42\n",
        "        self.regularizer_rate = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "num_epochs = args.epochs\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "params_to_update = model.classifier.parameters()\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)\n"
      ],
      "metadata": {
        "id": "-pELVFZOHU55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc, f1 = get_auc_f1(model, test_patch_dataset)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"F1: \", f1)"
      ],
      "metadata": {
        "id": "1wWzD0HwHWAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502d4f48-184c-4f54-eacc-ae3ad90ad38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.8902737234644129\n",
            "F1:  0.3521367521367521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions(model, test_patch_dataset, '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/alex_patch_predictions.csv')"
      ],
      "metadata": {
        "id": "cQc8Gxf3HW-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed42ae32-0507-4124-fa50-672ff0238fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/alex_patch_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and extract activations from last layer"
      ],
      "metadata": {
        "id": "54D7b0m8HZyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model\n",
        "#model = models.vgg16(pretrained=True)\n",
        "model = models.alexnet(pretrained=True)\n",
        "\n",
        "# Modify the classifier\n",
        "model.classifier[-1] = torch.nn.Linear(4096, 2)\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240514015922.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def preprocess_and_extract_activations(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path)\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Flatten the tensor before passing it to the linear layers\n",
        "    image_tensor = model.features(image_tensor)\n",
        "    image_tensor = model.avgpool(image_tensor)\n",
        "    image_tensor = torch.flatten(image_tensor, 1)  # Flatten the tensor\n",
        "\n",
        "    # Pass the tensor through the first 5 layers\n",
        "    for layer_idx, layer in enumerate(model.classifier[:5], start=1):\n",
        "        image_tensor = layer(image_tensor)\n",
        "\n",
        "    # Extract activations from the sixth linear layer\n",
        "    activations = model.classifier[5](image_tensor)\n",
        "    activations = activations.squeeze().cpu().detach().numpy()\n",
        "    #print(\"Activation size:\", activations.shape)\n",
        "\n",
        "    return activations\n",
        "\n",
        "# Function to recursively traverse folders and process images\n",
        "def process_images_in_folder(folder_path):\n",
        "    all_activations = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg')):\n",
        "                image_path = os.path.join(root, file)\n",
        "                activations = preprocess_and_extract_activations(image_path)\n",
        "                if activations is not None:\n",
        "                    all_activations.append(activations)\n",
        "    return all_activations\n",
        "\n",
        "# Folder path containing  images\n",
        "patch_malig_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "no_patch_malig_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224\"\n",
        "\n",
        "# Extract activations for all images in the folder\n",
        "wp_malig_all_activations = process_images_in_folder(patch_malig_folder_path)\n",
        "wo_malig_all_activations = process_images_in_folder(no_patch_malig_folder_path)"
      ],
      "metadata": {
        "id": "udNA8QR2HafJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ab8e90-0c12-4b01-825d-96b25d822995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stacks the activations stored in the list all_activations vertically; combines them into a single array\n",
        "np.vstack(all_activations).shape"
      ],
      "metadata": {
        "id": "vqCF1X99HdF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02f50ca-53be-4027-bcf2-a723e0715f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(631, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save activations for malignant wit patches as npy array (Sari likes npy, I like csv)\n",
        "np.save('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wp_activations.npy',np.vstack(wp_malig_all_activations))\n",
        "np.save('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wo_activations.npy',np.vstack(wo_malig_all_activations))"
      ],
      "metadata": {
        "id": "BpZWypD4HeOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "files= os.listdir(folder_path)\n",
        "for imgname in files:\n",
        "    shutil.copyfile('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/data/processed/cancer_224/'+imgname,'/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224/'+imgname)"
      ],
      "metadata": {
        "id": "-sW64F6PHfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`patch_no_patch = np.hstack([np.zeros((1,num_mal_test)),np.ones((1,num_mal_test))])` creates an array patch_no_patch by horizontally stacking two arrays:\n",
        "1. An array of zeros with shape (1, num_mal_test), representing labels for non-patched images (0).\n",
        "2. An array of ones with shape (1, num_mal_test), representing labels for patched images (1)."
      ],
      "metadata": {
        "id": "JqUsDGWg9OQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "files= os.listdir(folder_path)\n",
        "num_mal_test = len(files)\n",
        "patch_no_patch = np.hstack([np.zeros((1,num_mal_test)),np.ones((1,num_mal_test))])\n",
        "wp = np.load('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wp_activations.npy')\n",
        "wo = np.load('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wo_activations.npy')\n",
        "#vertically stacks the activations from both files (wo and wp), combining activations from both patched and non-patched images\n",
        "wop_activations = np.vstack([wo,wp])"
      ],
      "metadata": {
        "id": "BCky-4dxHgIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wop_activations.shape"
      ],
      "metadata": {
        "id": "fU1-tkXSHhHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e64c129-64f4-4887-d95b-ebe6ef65c88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1262, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_no_patch.shape"
      ],
      "metadata": {
        "id": "QCnZb-JZHil4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba12fc9-63d8-4e86-cf5f-de98ca7438d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1262)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenation of Arrays:\n",
        "\n",
        "two_arrays = np.concatenate((patch_no_patch.T, wop_activations), axis=1): This  concatenates two arrays along the columns (axis 1).\n",
        "\n",
        "patch_no_patch.T is the transpose of the patch_no_patch array created earlier. Transposing it makes its shape (num_mal_test, 1).\n",
        "\n",
        "wop_activations is likely a stacked array of activations from both patched and non-patched images. Its shape is (num_images, num_features).\n",
        "\n",
        "By concatenating these two arrays along axis 1, you get a new array two_arrays\n",
        "\n",
        "where the first column corresponds to the labels (patched vs. non-patched) and the remaining columns contain the activations. The resulting shape is (num_images, 1 + num_features).\n",
        "\n",
        "\n",
        "Correlation Calculation:\n",
        "\n",
        "corr = np.corrcoef(two_arrays.T): This line calculates the correlation coefficient matrix for the transposed two_arrays.\n",
        "\n",
        "The .corrcoef() function computes the correlation coefficients between columns of the input array.\n",
        "\n",
        "Transposing two_arrays ensures that the correlation is calculated between features (columns). The resulting corr matrix is symmetric and has dimensions (1 + num_features, 1 + num_features)."
      ],
      "metadata": {
        "id": "0vMmgu_F-q3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "two_arrays = np.concatenate((patch_no_patch.T, wop_activations), axis=1) # 900x568\n",
        "corr = np.corrcoef(two_arrays.T)"
      ],
      "metadata": {
        "id": "fNE5AlIUHj2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "activations_corr = np.abs(corr[0][1:]): This line extracts the correlation coefficients between the label and each activation feature. Since the correlation matrix corr has dimensions (1 + num_features, 1 + num_features), the correlation coefficient between the label and each feature is obtained from the first row (index 0) except for the first element (index 0), which represents the correlation of the label with itself. The np.abs() function is used to take the absolute values of the correlation coefficients to ensure all values are positive."
      ],
      "metadata": {
        "id": "BijmacB8_Kex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "activations_corr = np.abs(corr[0][1:])\n",
        "_ = plt.hist(activations_corr, bins='auto')"
      ],
      "metadata": {
        "id": "g2CF1Mg7Hl8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "144a83c7-f5c4-4955-caca-f0881970359d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations_corr.shape"
      ],
      "metadata": {
        "id": "umazsb02Hk8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e633bd6c-287c-43f3-ec45-60e13d30ecbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4096,)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.classifier)\n"
      ],
      "metadata": {
        "id": "Tlpm60ujHnXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5397ac37-25ae-4fb5-bb80-bcaadb13fbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder paths containing the original and patched images\n",
        "original_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224\"\n",
        "patched_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "\n",
        "# Extract activations for all images in the original and patched folders\n",
        "original_activations = process_images_in_folder(original_folder_path)\n",
        "patched_activations = process_images_in_folder(patched_folder_path)"
      ],
      "metadata": {
        "id": "OneBd8rp6r0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlations(original_activations, patched_activations):\n",
        "    # Convert activation lists to arrays\n",
        "    original_activations_array = np.array(original_activations)\n",
        "    patched_activations_array = np.array(patched_activations)\n",
        "\n",
        "    # Ensure activations arrays have the correct shape\n",
        "    original_activations_array = original_activations_array.reshape(len(original_activations_array), -1)\n",
        "    patched_activations_array = patched_activations_array.reshape(len(patched_activations_array), -1)\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = np.corrcoef(original_activations_array.T, patched_activations_array.T)\n",
        "\n",
        "    # Extract correlations for each neuron\n",
        "    neuron_correlations = correlations[:4096, 4096:]\n",
        "\n",
        "    return neuron_correlations\n",
        "\n",
        "# Calculate correlations for each neuron\n",
        "neuron_correlations = calculate_correlations(original_activations, patched_activations)\n",
        "\n",
        "# Print correlations for the first neuron as an example\n",
        "print(\"Correlations for the first neuron:\", neuron_correlations[0])"
      ],
      "metadata": {
        "id": "kiZPcod3HoPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea30b44c-21e9-4d90-b802-dcbd5efbb27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlations for the first neuron: [ 0.07240221 -0.00710556 -0.00525816 ... -0.00769213 -0.00429515\n",
            "  0.01825639]\n"
          ]
        }
      ]
    }
  ]
}