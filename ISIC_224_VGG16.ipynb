{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/Shortcut_Learning/blob/main/ISIC_224_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchextractor"
      ],
      "metadata": {
        "id": "F0Efql5w-XvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4930af6-a332-4f25-bea6-835e759a648c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchextractor\n",
            "  Downloading torchextractor-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchextractor) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchextractor) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torchextractor) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->torchextractor)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->torchextractor) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->torchextractor) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchextractor\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchextractor-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from os.path import join as oj\n",
        "from datetime import datetime\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from numpy.random import randint\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import copy\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "racKclkHP83h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and create paths for directories\n"
      ],
      "metadata": {
        "id": "apqJXlN2Qh7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RpChcjDPw0Q",
        "outputId": "71b81c59-190a-4642-f489-f818be1b8cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "dir_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224\"\n",
        "#dir_path = \"/content/drive/MyDrive/Projects/ISIC_224\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = oj(dir_path, \"models\", \"initial_classifier\")\n",
        "model_training_path = oj(model_path, \"training_224\")\n",
        "data_path = oj(dir_path, \"data\")\n",
        "\n",
        "not_cancer_path = oj(data_path, \"processed\", \"no_cancer_224\")\n",
        "cancer_path = oj(data_path, \"processed\", \"cancer_224\")"
      ],
      "metadata": {
        "id": "ik7Vaay-QmVZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments for training"
      ],
      "metadata": {
        "id": "a7Y1NXBlQp8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mean and Standard Deviation: These are normalization parameters used for preprocessing images. They are often used in computer vision tasks to scale pixel values to a common range.\n",
        "\n",
        "2. Device Configuration: It determines whether the code should run on CPU or GPU (cuda) based on the availability of CUDA support. CUDA is a parallel computing platform and application programming interface model created by NVIDIA. If CUDA is available, the code will run on the GPU; otherwise, it will use the CPU.\n",
        "\n",
        "3. Model Initialization: It loads a pre-trained VGG16 model from torchvision.models, modifies the last layer of the classifier to output 2 classes (it seems to be for some kind of classification task), moves the model to the specified device (CPU or GPU), and sets the parameters to update during training (in this case, only the parameters of the classifier)."
      ],
      "metadata": {
        "id": "K4PwrDvM_IMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The VGG16 model has the following structure:**\n",
        "1. Several convolutional layers followed by max-pooling layers.\n",
        "2. Three fully connected (dense) layers at the end, which constitute the classifier.\n",
        "Classifier Part of VGG16\n",
        "\n",
        "\n",
        "**The classifier part of the original VGG16 model is designed as follows:**\n",
        "\n",
        "1. First Fully Connected Layer: 4096 neurons, which take the flattened output from the last convolutional layer.\n",
        "2. Second Fully Connected Layer: 4096 neurons.\n",
        "3. Final Fully Connected Layer: The number of neurons corresponds to the number of output classes, which is 1000 for ImageNet."
      ],
      "metadata": {
        "id": "o-MN7A3UQeiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "mean = np.asarray([0.485, 0.456, 0.406])\n",
        "std = np.asarray([0.229, 0.224, 0.225])\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.00001\n",
        "        self.momentum = 0.9\n",
        "        self.seed = 42\n",
        "        self.regularizer_rate = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "num_epochs = args.epochs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "model = models.vgg16(pretrained=True)\n",
        "#changing nn.Linear(4096, 1000) to nn.Linear(4096, 2) as vgg was trained on imagenet with 1000 classes originally\n",
        "model.classifier[-1] = nn.Linear(4096, 2)\n",
        "model = model.to(device)\n",
        "params_to_update = model.classifier.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBAHXN3XQosJ",
        "outputId": "47a3bda1-45cd-4779-c18d-563ac1b20638"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 196MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clean up the image directories\n",
        "\n",
        "\n",
        "\n",
        "*   Remove empty images\n",
        "*   Remove duplicates which appear in a new folder but not the original.\n",
        "*   Ensure image sizes are all 224x224\n",
        "\n"
      ],
      "metadata": {
        "id": "90U5lSQfwkyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_empty_files(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = len(list_files)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        if os.path.getsize(oj(path, list_files[i])) < 100:\n",
        "            os.remove(oj(path, list_files[i]))\n",
        "            print(\"File \" + str(i) + \"deleted!\")\n",
        "'''\n",
        "def clean_up_duplicates(path1, path2):\n",
        "    newfiles = os.listdir(path1)\n",
        "    oldfiles = os.listdir(path2)\n",
        "    diff = [f for f in newfiles if f not in oldfiles]\n",
        "    for i in tqdm(diff):\n",
        "        os.remove(oj(path1, i))\n",
        "        print(\"File \" + str(i) + \"deleted!\")\n",
        "\n",
        "def check_img_sizes(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = len(list_files)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        im = Image.open(oj(path, list_files[i]))\n",
        "        if im.width != 224 or im.height != 224:\n",
        "            print(list_files[i])\n",
        "'''\n",
        "# clean_up_empty_files(cancer_path)\n",
        "# clean_up_empty_files(not_cancer_path)\n",
        "\n",
        "# newpath = oj(data_path, \"no_cancer_224_inpainted\")\n",
        "# oldpath = oj(data_path, \"processed\", \"no_cancer_224\")\n",
        "# clean_up_duplicates(newpath, oldpath)\n",
        "\n",
        "# check_img_sizes(not_cancer_path)\n"
      ],
      "metadata": {
        "id": "9Kz8kmvRw5xo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "023f3427-c057-4ad4-f6fb-813cbc853cda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef clean_up_duplicates(path1, path2):\\n    newfiles = os.listdir(path1)\\n    oldfiles = os.listdir(path2)\\n    diff = [f for f in newfiles if f not in oldfiles]\\n    for i in tqdm(diff):\\n        os.remove(oj(path1, i))\\n        print(\"File \" + str(i) + \"deleted!\")\\n\\ndef check_img_sizes(path):\\n    list_files= os.listdir(path)\\n    num_files = len(list_files)\\n    for i in tqdm(range(num_files)):\\n        im = Image.open(oj(path, list_files[i]))\\n        if im.width != 224 or im.height != 224:\\n            print(list_files[i])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch dataset class"
      ],
      "metadata": {
        "id": "yaVcxFBQxH14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CancerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: str = None, is_cancer: int = None, data_files = None, labels = None):\n",
        "        \"\"\"\n",
        "        Expects path and is_cancer both to be supplied if the relevant images all lie in the same directory and have the same class\n",
        "        or a list of full filepaths and list of all labels are both supplied using data_files and labels otherwise.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Initializes the CancerDataset.\n",
        "\n",
        "        If 'path' and 'is_cancer' are provided, it assumes that all images in the directory specified\n",
        "        by 'path' belong to the same class 'is_cancer'.\n",
        "\n",
        "        Alternatively, if 'data_files' and 'labels' are provided, it uses these lists directly\n",
        "        for file paths and corresponding labels.\n",
        "\n",
        "        Args:\n",
        "            path (str): Directory containing images, all belonging to the same class.\n",
        "            is_cancer (int): The class label (e.g., 1 for cancer, 0 for non-cancer) for all images in the directory.\n",
        "            data_files (list): List of full file paths to images.\n",
        "            labels (list): List of labels corresponding to 'data_files'.\n",
        "        \"\"\"\n",
        "        if path: # If a path is provided, list all files in the directory and assign the class label\n",
        "            self.path = path\n",
        "            self.data_files = os.listdir(self.path)\n",
        "            self.is_cancer = is_cancer\n",
        "\n",
        "        else: # Otherwise, use provided lists of data files and labels\n",
        "            self.path = ''\n",
        "            self.data_files = data_files\n",
        "            self.labels = labels\n",
        "            self.is_cancer = None\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        \"\"\"\n",
        "        Retrieves an image and its label at index 'i'.\n",
        "\n",
        "        Args:\n",
        "            i (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image tensor, label)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # Read in the image, convert to float between [0,1] and standardise , and convert to a PyTorch tensor\n",
        "        img = Image.open(oj(self.path, self.data_files[i]))\n",
        "        img_array = np.asarray(img)/255.0 # Convert image to float and scale to [0, 1]\n",
        "        img_array -= mean[None, None, :] # Subtract the mean for normalization\n",
        "        img_array /= std[None, None, :] # Divide by the standard deviation for normalization\n",
        "        img.close()\n",
        "        torch_img = torch.from_numpy(img_array.swapaxes(0,2).swapaxes(1,2)).float() # Convert the numpy array to a PyTorch tensor and rearrange the axes\n",
        "        # Determine the label: use the global class label if provided, otherwise extract the relevant label from the list of labels.\n",
        "        is_cancer = self.is_cancer if self.is_cancer is not None else self.labels[i]\n",
        "        return (torch_img, is_cancer)\n",
        "\n",
        "    def __len__(self): # Returns the total number of images in the dataset.\n",
        "        return len(self.data_files)"
      ],
      "metadata": {
        "id": "CBXypVPHxKw-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions for training\n",
        "\n",
        "Computing Gradients:\n",
        "\n",
        "*   `crit(model(im), target):` Computes the loss between the model's output and the target labels.\n",
        "*   `torch.autograd.grad(..., create_graph=True)`: Computes the gradients of the loss with respect to im.\n",
        "*   `.sum(dim=1):` Sums the gradients along the specified dimension.\n",
        "*   `torch.abs(...).sum()`: Computes the absolute value of the gradients and sums them up\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BxGJZUclxPuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_sum(im, target, model, crit, device='cuda'):\n",
        "    '''assume that eveything is already on cuda'''\n",
        "    im.requires_grad = True # Enable gradient computation for the input image\n",
        "    grad_params = torch.abs(torch.autograd.grad(crit(model(im), target), im,create_graph = True)[0].sum(dim=1)).sum()\n",
        "    return grad_params\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, resume_training=False):\n",
        "    since = time.time()\n",
        "    # train_loss_history = []\n",
        "    # train_acc_history = []\n",
        "    # train_cd_history= []\n",
        "\n",
        "\n",
        "    # Initialize best_loss, patience, and cur_patience to manage early stopping.\n",
        "    best_loss = 10.0\n",
        "    patience = 3 # Number of epochs to wait for improvement\n",
        "    cur_patience = 0 # Current patience counter\n",
        "\n",
        "    if len(os.listdir(model_training_path)) > 0 and resume_training:\n",
        "      # Check if there are saved model files and resume training if needed\n",
        "        model_list = [(f, os.path.getmtime(oj(model_training_path,f))) for f in os.listdir(model_training_path) if f.endswith('.pt')]\n",
        "        model_list.sort(key=lambda tup: tup[1], reverse=True)  # Sort models by modification time in place from most to least recent\n",
        "        model_name = model_list[0][0]\n",
        "        model.classifier.load_state_dict(torch.load(oj(model_training_path, model_name)))\n",
        "        print(\"Model loaded!\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        optimizer.step() # Update the model parameters\n",
        "        model.train()  # Set model to training mode\n",
        "        phase = 'train'\n",
        "        running_loss = 0.0\n",
        "        running_loss_cd = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data , moving inputs and labels to the specified device.\n",
        "        for i, (inputs, labels) in tqdm(enumerate(dataloaders[phase])):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                # need to do calc beforehand because we do need the gradients\n",
        "                if phase == 'train' and regularizer_rate !=0:\n",
        "                  # Apply gradient regularization if specified , compute additional loss from gradients and update the model.\n",
        "                    inputs.requires_grad = True\n",
        "                    add_loss = gradient_sum(inputs, labels, model, criterion)\n",
        "                    if add_loss!=0:\n",
        "                        (regularizer_rate*add_loss).backward()\n",
        "                        optimizer.step()\n",
        "                    #print(torch.cuda.memory_allocated()/(np.power(10,9)))\n",
        "                    optimizer.zero_grad()\n",
        "                    running_loss_cd += add_loss.item() * inputs.size(0)\n",
        "\n",
        "                    #inputs.require_grad = False\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if phase == 'train':\n",
        "                    (loss).backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_cd_loss = running_loss_cd / dataset_sizes[phase]\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f} CD Loss : {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc, epoch_cd_loss))\n",
        "\n",
        "        # train_loss_history.append(epoch_loss)\n",
        "        # train_cd_history.append(epoch_cd_loss)\n",
        "        # train_acc_history.append(epoch_acc.item())\n",
        "        torch.save(model.classifier.state_dict(), oj(model_training_path, datetime.now().strftime(\"%Y%m%d%H%M%S\") + \".pt\"))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60)\n",
        "    )\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    return model"
      ],
      "metadata": {
        "id": "AUBHiMpAxQuX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for evaluation\n",
        "\n",
        "❗❗❗ When setting a PyTorch model to evaluation mode (model.eval()), it switches off certain layers' behavior that are typically active during training. Here's what happens:\n",
        "\n",
        "Dropout Layers: During training, dropout layers randomly zero out some of the activations to prevent overfitting. However, during evaluation, we want the full strength of the model, so dropout layers are turned off, meaning they don't zero out any activations.\n",
        "\n",
        "Batch Normalization Layers: Batch normalization layers adjust the activations so that they have a mean of zero and a standard deviation of one, which can help in training. During evaluation, we often don't have batches of data to compute these statistics, so the batch normalization layers use the running statistics (computed during training) instead of batch statistics."
      ],
      "metadata": {
        "id": "QVDur5K-xk7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import auc,average_precision_score, roc_curve,roc_auc_score,precision_recall_curve, f1_score\n",
        "\n",
        "def get_output(model, dataset):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    model = model.eval()\n",
        "    y = []\n",
        "    y_hat = []\n",
        "    softmax= torch.nn.Softmax() #Creates a Softmax function to convert logits to probabilities.\n",
        "    with torch.no_grad() : # Disables gradient computation for the operations within this block, saving memory and computation.\n",
        "\n",
        "        # Iterate over the DataLoader, moves inputs to GPU, and appends true labels and predicted probabilities to their respective lists.\n",
        "        for inputs, labels in data_loader:\n",
        "            y_hat.append((labels).cpu().numpy())\n",
        "            y.append(torch.nn.Softmax(dim=1)( model(inputs.cuda()))[:,1].detach().cpu().numpy()) # take the probability for cancer\n",
        "    y_hat = np.concatenate( y_hat, axis=0 )\n",
        "    y = np.concatenate( y, axis=0 )\n",
        "    return y, y_hat # in the training set the values were switched\n",
        "\n",
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        # Load model weights from file\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        # Load weights into the model\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models we unfortunately saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        # Get true labels and predictions from the whole model\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        # Get true labels and predictions from the classifier part of the model\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:\n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "metadata": {
        "id": "fRG4cLfuxngv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Classifier Training"
      ],
      "metadata": {
        "id": "mpDo4dzRxqZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine datasets and split to train-test"
      ],
      "metadata": {
        "id": "kEEOZ8LXxvsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_dataset = CancerDataset(path=cancer_path, is_cancer=1)\n",
        "not_cancer_dataset = CancerDataset(path=not_cancer_path, is_cancer=0)\n",
        "complete_dataset = ConcatDataset((cancer_dataset, not_cancer_dataset))\n",
        "\n",
        "num_total = len(complete_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_test = num_total - num_train\n",
        "torch.manual_seed(0);\n",
        "print(\"num_train:\", num_train)\n",
        "print(\"num_test:\", num_test)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(complete_dataset, [num_train, num_test])\n",
        "datasets = {'train' : train_dataset, 'test':test_dataset}\n",
        "dataset_sizes = {'train' : len(train_dataset), 'test':len(test_dataset)}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPy29IJ_xyx2",
        "outputId": "0c36fd4f-f615-4c74-fbd9-bd1bbae70cb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train: 9340\n",
            "num_test: 2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the specific files in the training/test sets.\n"
      ],
      "metadata": {
        "id": "_8qVGPY_x1Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, test_idx = train_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, test_files"
      ],
      "metadata": {
        "id": "HQv-2S9kx4T2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and get the full file paths.\n",
        "train_files, test_files = extract_filenames(train_dataset, test_dataset)\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "metadata": {
        "id": "5w2Me7gXx7Km"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weights for training"
      ],
      "metadata": {
        "id": "YGwaiPUqx9KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the classes are unbalanced, we need to account for this in the loss function while training"
      ],
      "metadata": {
        "id": "UrJy_VbAx_q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_ratio = len(cancer_dataset)/len(complete_dataset)\n",
        "\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1/cancer_ratio\n",
        "not_cancer_weight = 1/ not_cancer_ratio\n",
        "weights = np.asarray([not_cancer_weight, cancer_weight])\n",
        "weights /= weights.sum()\n",
        "weights = torch.tensor(weights).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = weights.double().float())\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)"
      ],
      "metadata": {
        "id": "SWRPn6oVyCBW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save the model"
      ],
      "metadata": {
        "id": "fXhY2zi3yEfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs, resume_training=False)\n",
        "pid = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "torch.save(model.classifier.state_dict(),oj(dir_path, model_path, pid + \".pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbNn0nOayD7u",
        "outputId": "d914357a-d581-4603-8a66-af92722fc972"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "584it [24:35,  2.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3922 Acc: 0.8481 CD Loss : 0.0000\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:25,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2763 Acc: 0.8686 CD Loss : 0.0000\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2540 Acc: 0.8703 CD Loss : 0.0000\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2453 Acc: 0.8732 CD Loss : 0.0000\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2409 Acc: 0.8733 CD Loss : 0.0000\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2375 Acc: 0.8790 CD Loss : 0.0000\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2319 Acc: 0.8816 CD Loss : 0.0000\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2293 Acc: 0.8804 CD Loss : 0.0000\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2278 Acc: 0.8831 CD Loss : 0.0000\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "584it [02:24,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2274 Acc: 0.8851 CD Loss : 0.0000\n",
            "Training complete in 46m 32s\n",
            "Best val loss: 10.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc, f1 = get_auc_f1(model, test_dataset)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"F1: \", f1)\n"
      ],
      "metadata": {
        "id": "I9FRBsdkcRhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0998d1-1b59-467b-f5eb-36bf50588e9a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.9561924011554157\n",
            "F1:  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_file_path = oj(dir_path, \"auc_f1_224_no_malig_patch.txt\")\n",
        "print(results_file_path)\n",
        "with open(results_file_path, 'w') as f:\n",
        "    f.write('AUC: ' + str(auc) + \"\\n\")\n",
        "    f.write('F1: ' + str(f1) + \"\\n\")"
      ],
      "metadata": {
        "id": "HbIm8MQZcWuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312b55b4-4594-4050-9a02-77563b60a68b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/auc_f1_224_no_malig_patch.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload the model to skip retraining and test with patched images"
      ],
      "metadata": {
        "id": "s-sL5LmyZ14p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_patch_path = '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224'\n",
        "not_cancer_path = '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/data/processed/no_cancer_224'"
      ],
      "metadata": {
        "id": "hLmXe-BKd7d6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_patch_dataset = CancerDataset(path=cancer_patch_path, is_cancer=1)\n",
        "not_cancer_dataset = CancerDataset(path=not_cancer_path, is_cancer=0)\n",
        "complete_patch_dataset = ConcatDataset((cancer_patch_dataset, not_cancer_dataset))\n",
        "\n",
        "num_total = len(complete_patch_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_test = num_total - num_train\n",
        "torch.manual_seed(0);\n",
        "print(\"num_train:\", num_train)\n",
        "print(\"num_test:\", num_test)\n",
        "\n",
        "train_patch_dataset, test_patch_dataset = torch.utils.data.random_split(complete_patch_dataset, [num_train, num_test])\n",
        "datasets = {'train' : train_patch_dataset, 'test':test_patch_dataset}\n",
        "dataset_sizes = {'train' : len(train_patch_dataset), 'test':len(test_patch_dataset)}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiYtVQKQcCiQ",
        "outputId": "77b7e6da-cde7-4d89-e4f5-2615736e658f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train: 9340\n",
            "num_test: 2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, test_idx = train_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, test_files"
      ],
      "metadata": {
        "id": "rzfQCxdPcIen"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to remember why..probably a useless cell"
      ],
      "metadata": {
        "id": "8plFnOPItoqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and get the full file paths.\n",
        "train_patch_files, test_patch_files = extract_filenames(train_patch_dataset, test_patch_dataset)\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "metadata": {
        "id": "ypnVoR1weu3K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models they saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:\n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "metadata": {
        "id": "ED7P9eHcknCA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(model, dataset, filename):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for inputs, labels in dataset:\n",
        "        inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
        "        inputs = inputs.to(device)  # Move data to appropriate device\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Append predictions to the list\n",
        "        predictions.append(predicted.item())\n",
        "\n",
        "        # Check if labels are integers or tensors\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            true_labels.append(labels.item())\n",
        "        else:\n",
        "            true_labels.append(labels)  # Assume labels are integers\n",
        "\n",
        "    # Create a DataFrame to store predictions and true labels\n",
        "    df = pd.DataFrame({\n",
        "        'Prediction': predictions,\n",
        "        'True Label': true_labels\n",
        "    })\n",
        "\n",
        "    # Save DataFrame to CSV file\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Predictions saved to {filename}\")"
      ],
      "metadata": {
        "id": "YtXdQ9JFhX_G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier[-1] = nn.Linear(4096, 2)  # Modify classifier\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240519185334.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# cancer_dataset and complete_dataset are already defined\n",
        "cancer_ratio = len(cancer_dataset) / len(complete_dataset)\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1 / cancer_ratio\n",
        "not_cancer_weight = 1 / not_cancer_ratio\n",
        "weights = torch.tensor([not_cancer_weight, cancer_weight], device=device, dtype=torch.float)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.00001\n",
        "        self.momentum = 0.9\n",
        "        self.seed = 42\n",
        "        self.regularizer_rate = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "num_epochs = args.epochs\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "params_to_update = model.classifier.parameters()\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)\n"
      ],
      "metadata": {
        "id": "_NYeroADbgBy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc, f1 = get_auc_f1(model, test_patch_dataset)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"F1: \", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc0o9QBgyI0e",
        "outputId": "04172923-d758-4bab-8aee-e51c6afd810d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.9566642722868366\n",
            "F1:  0.5233644859813084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions(model, test_patch_dataset, '/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/patch_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEyz8zK4m4fN",
        "outputId": "2da4ede8-cf89-4cad-df65-97ea14b93194"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/patch_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and extract activations from last layer"
      ],
      "metadata": {
        "id": "8BWlccI3VmOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Modify the classifier\n",
        "model.classifier[-1] = torch.nn.Linear(4096, 2)\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240515120403.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def preprocess_and_extract_activations(image_path):\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        image = Image.open(image_path)\n",
        "        # unsqueeze(0) adds an extra dimension to simulate a batch of size 1\n",
        "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Flatten the tensor before passing it to the linear layers\n",
        "        # Pass the preprocessed image through the convolutional and pooling layers of VGG16. This part of the model captures various hierarchical features of the image.\n",
        "        image_tensor = model.features(image_tensor)\n",
        "        # This layer reduces the spatial dimensions of the feature maps, summarizing the information in each feature map.\n",
        "        image_tensor = model.avgpool(image_tensor)\n",
        "        #Convert the 2D feature maps into a 1D vector. This is necessary because the fully connected layers expect 1D input.\n",
        "        image_tensor = torch.flatten(image_tensor, 1)  # Flatten the tensor\n",
        "\n",
        "        # Sequentially pass the tensor through the first five layers of the classifier. Each layer transforms the input using learned weights and biases, and typically applies a non-linear activation function.\n",
        "        for layer_idx, layer in enumerate(model.classifier[:5], start=1):\n",
        "            image_tensor = layer(image_tensor)\n",
        "\n",
        "        # Extract activations from the sixth linear layer of the classifier\n",
        "        activations = model.classifier[5](image_tensor)\n",
        "        #.squeeze() removes any dimensions of size 1., .detach() ensures no gradients are tracked, which is important for saving memory during inference and convert the tensor to numpy array for simplicity\n",
        "        activations = activations.squeeze().cpu().detach().numpy()\n",
        "        #print(\"Activation size:\", activations.shape)\n",
        "        return activations\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to recursively traverse folders and process images\n",
        "def process_images_in_folder(folder_path):\n",
        "    all_activations = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg')):\n",
        "                image_path = os.path.join(root, file)\n",
        "                activations = preprocess_and_extract_activations(image_path)\n",
        "                if activations is not None:\n",
        "                    all_activations.append(activations)\n",
        "    return all_activations\n",
        "\n",
        "# Folder path containing  images\n",
        "patch_malig_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "no_patch_malig_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224\"\n",
        "\n",
        "# Extract activations for all images in the folder\n",
        "wp_malig_all_activations = process_images_in_folder(patch_malig_folder_path)\n",
        "wo_malig_all_activations = process_images_in_folder(no_patch_malig_folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRcHWdJ9TCT8",
        "outputId": "aea030f6-b834-4c59-8671-d4a08156c165"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each row corresponds to the activations of one image.\n",
        "np.vstack(wp_malig_all_activations).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYuRXhFSlO_E",
        "outputId": "a36062d3-4330-40de-f143-5d85756bbba5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(631, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.vstack(wo_malig_all_activations).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oae6JVbHmRv-",
        "outputId": "2ec2b0fb-1489-4d5a-db03-e822477bdf9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(631, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save activations for malignant wit patches as npy array (Sari likes npy, I like csv)\n",
        "np.save('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wp_activations.npy',np.vstack(wp_malig_all_activations))\n",
        "np.save('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wo_activations.npy',np.vstack(wo_malig_all_activations))"
      ],
      "metadata": {
        "id": "1wckbu5FbsZr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "files= os.listdir(folder_path)\n",
        "# Creates a dataset of malignant images without patches for further analysis\n",
        "for imgname in files:\n",
        "    shutil.copyfile('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/data/processed/cancer_224/'+imgname,'/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224/'+imgname)"
      ],
      "metadata": {
        "id": "480gt9JTgl4L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`np.zeros((1, num_mal_test)):` This creates an array of zeros with shape `(1, num_mal_test).` Each zero represents the absence of a patch.\n",
        "\n",
        "`np.ones((1, num_mal_test)):` This creates an array of ones with shape `(1, num_mal_test)`. Each one represents the presence of a patch.\n",
        "\n",
        "`np.hstack([...])`: This horizontally stacks the arrays provided as arguments.\n",
        "So, it concatenates the array of zeros and the array of ones horizontally, resulting in a binary indicator array where the first row represents the absence (0) or presence (1) of patches for each file.\n",
        "\n",
        "Let's say the first two images have patches, and the last image does not have a patch.\n",
        "Each column represents one image, where:\n",
        "0 indicates no patch.\n",
        "1 indicates presence of a patch.\n",
        "\n",
        "\\\\text{patch_no_patch} = \\begin{bmatrix}\n",
        "0 & 1 & 0 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "Let's say the activations for the three images are as follows:\n",
        "\\text{wop_activations} = \\begin{bmatrix}\n",
        "0.1 & 0.3 & 0.2 \\\\\n",
        "0.5 & 0.2 & 0.6 \\\\\n",
        "0.7 & 0.4 & 0.9 \\\\\n",
        "0.2 & 0.1 & 0.3 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "Each column represents one image, and each row represents one activation value.\n",
        "\n",
        "Putting it all together, the combined matrix for correlation analysis (two_arrays) would look like this:\n",
        "\n",
        "\n",
        "Apologies for the confusion. Let's represent the data in a clearer format.\n",
        "\n",
        "Suppose we have 3 malignant images, and each image has 4 activations. Here's how the data would look like in a matrix form:\n",
        "\n",
        "Binary Indicator Array (patch_no_patch):\n",
        "\n",
        "Each column represents one image, where:\n",
        "0 indicates no patch.\n",
        "1 indicates presence of a patch.\n",
        "\\\\text{patch_no_patch} = \\begin{bmatrix}\n",
        "0 & 1 & 0 \\\\\n",
        "\\end{bmatrix}\n",
        "Activations Array (wop_activations):\n",
        "\n",
        "Let's say the activations for the three images are as follows:\n",
        "\\text{wop_activations} = \\begin{bmatrix}\n",
        "0.1 & 0.3 & 0.2 \\\\\n",
        "0.5 & 0.2 & 0.6 \\\\\n",
        "0.7 & 0.4 & 0.9 \\\\\n",
        "0.2 & 0.1 & 0.3 \\\\\n",
        "\\end{bmatrix}\n",
        "Each column represents one image, and each row represents one activation value.\n",
        "\n",
        "Putting it all together, the combined matrix for correlation analysis (two_arrays) would look like this:\n",
        "\n",
        "\\text{two_arrays} = \\begin{bmatrix}\n",
        "0 & 1 & 0 \\\\\n",
        "0.1 & 0.5 & 0.7 \\\\\n",
        "0.3 & 0.2 & 0.4 \\\\\n",
        "0.2 & 0.6 & 0.9 \\\\\n",
        "0.2 & 0.1 & 0.3 \\\\\n",
        "\\end{bmatrix}\n",
        "Each row corresponds to one feature (neuron) or the presence/absence of patches, and each column corresponds to one image."
      ],
      "metadata": {
        "id": "LNLWginQy6JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block creates a binary indicator array patch_no_patch to label whether each set of activations corresponds to an image with a patch (1) or without a patch (0)."
      ],
      "metadata": {
        "id": "5f5zhCPIqrx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "files= os.listdir(folder_path)\n",
        "num_mal_test = len(files)\n",
        "patch_no_patch = np.hstack([np.zeros((1,num_mal_test)),np.ones((1,num_mal_test))])\n",
        "wp = np.load('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wp_activations.npy')\n",
        "wo = np.load('/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/mal_test_wo_activations.npy')\n",
        "wop_activations = np.vstack([wo,wp])"
      ],
      "metadata": {
        "id": "GSKFLNwzkHFJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wop_activations.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLU5HHwtlqQY",
        "outputId": "c54a9866-0155-4ad7-9be1-0b3bbbaded2e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1262, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_arrays = np.concatenate((patch_no_patch.T, wop_activations), axis=1) # 900x568\n",
        "corr = np.corrcoef(two_arrays.T)"
      ],
      "metadata": {
        "id": "ef-tZX0Jlz_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906c6a38-9058-416f-bcac-f59996db4e78"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_no_patch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCun3nAhlsPj",
        "outputId": "da75eaf4-bb67-476d-efda-ea5eeeca8ed0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1262)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "activations_corr = np.abs(corr[0][1:])\n",
        "_ = plt.hist(activations_corr, bins='auto')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "a3Kuj1Y0l7wX",
        "outputId": "ceb9e135-8c5a-42ab-babe-304559a72aa6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations_corr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8XztnE8mcu1",
        "outputId": "bd0643b4-ba90-4ad8-b2f9-494b884f40d8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4096,)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(activations_corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7BtghV2cQJ_",
        "outputId": "bf40c1c9-fdaa-493b-caad-bf2a50d399b7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.03559531 0.00085221 0.00096072 ... 0.03201354 0.02456037 0.05800861]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.classifier)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjyXUMDIIgSU",
        "outputId": "ea2f3fc6-7de9-4361-8d89-26950bd8611b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def calculate_correlations(original_activations, patched_activations):\n",
        "    # Convert activation lists to arrays\n",
        "    original_activations_array = np.array([act[0] for act in original_activations])\n",
        "    patched_activations_array = np.array([act[0] for act in patched_activations])\n",
        "\n",
        "    # Ensure activations arrays have the correct shape\n",
        "    original_activations_array = original_activations_array.reshape(len(original_activations_array), -1)\n",
        "    patched_activations_array = patched_activations_array.reshape(len(patched_activations_array), -1)\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = np.corrcoef(original_activations_array.T, patched_activations_array.T)\n",
        "\n",
        "    # Extract correlations for each neuron\n",
        "    neuron_correlations = correlations[:4096, 4096:]\n",
        "\n",
        "    return neuron_correlations\n",
        "\n",
        "\n",
        "# Extract activations for original and patched MAL images\n",
        "#original_activations = preprocess_and_extract_activations(test_dataset)\n",
        "#patched_activations = preprocess_and_extract_activations(test_patch_dataset)\n",
        "\n",
        "# Define the folder paths containing the original and patched images\n",
        "original_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224\"\n",
        "patched_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "\n",
        "# Extract activations for all images in the original and patched folders\n",
        "original_activations = preprocess_and_extract_activations(original_folder_path)\n",
        "patched_activations = preprocess_and_extract_activations(patched_folder_path)\n",
        "\n",
        "# Calculate correlations for each neuron\n",
        "neuron_correlations = calculate_correlations(original_activations, patched_activations)\n",
        "\n",
        "# Print correlations for the first neuron as an example\n",
        "print(\"Correlations for the first neuron:\", neuron_correlations[0])\n",
        "'''"
      ],
      "metadata": {
        "id": "e6voBGuw5dB3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "57add5b0-dc44-4b87-9583-764678e8cf1f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef calculate_correlations(original_activations, patched_activations):\\n    # Convert activation lists to arrays\\n    original_activations_array = np.array([act[0] for act in original_activations])\\n    patched_activations_array = np.array([act[0] for act in patched_activations])\\n\\n    # Ensure activations arrays have the correct shape\\n    original_activations_array = original_activations_array.reshape(len(original_activations_array), -1)\\n    patched_activations_array = patched_activations_array.reshape(len(patched_activations_array), -1)\\n\\n    # Calculate correlations\\n    correlations = np.corrcoef(original_activations_array.T, patched_activations_array.T)\\n\\n    # Extract correlations for each neuron\\n    neuron_correlations = correlations[:4096, 4096:]\\n\\n    return neuron_correlations\\n\\n\\n# Extract activations for original and patched MAL images\\n#original_activations = preprocess_and_extract_activations(test_dataset)\\n#patched_activations = preprocess_and_extract_activations(test_patch_dataset)\\n\\n# Define the folder paths containing the original and patched images\\noriginal_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_nopatch_224\"\\npatched_folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\\n\\n# Extract activations for all images in the original and patched folders\\noriginal_activations = preprocess_and_extract_activations(original_folder_path)\\npatched_activations = preprocess_and_extract_activations(patched_folder_path)\\n\\n# Calculate correlations for each neuron\\nneuron_correlations = calculate_correlations(original_activations, patched_activations)\\n\\n# Print correlations for the first neuron as an example\\nprint(\"Correlations for the first neuron:\", neuron_correlations[0])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stophere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "66IjbaeyBkXF",
        "outputId": "f95e79e8-6ce6-4bf4-8ab5-ac35748094ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stophere' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-88d56cf18612>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstophere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stophere' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archive"
      ],
      "metadata": {
        "id": "gUi_kjJCVvm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG19 model\n",
        "#model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Print the model architecture\n",
        "#print(model)"
      ],
      "metadata": {
        "id": "EruGIokTsGg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def extract_activations(model, dataset):\n",
        "    activations = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataset:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model[0](inputs)  # Pass through the convolutional layers only\n",
        "            outputs = torch.flatten(outputs, 1)  # Flatten the output tensor\n",
        "            outputs = model[2](outputs)  # Pass through the linear layers\n",
        "            activations.append(outputs.cpu().numpy())  # Append activation\n",
        "            print(\"Activation size:\", outputs.shape)  # Print activation size\n",
        "    return activations\n",
        "'''"
      ],
      "metadata": {
        "id": "KNbsdnaV-YUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Function to extract activations from images\n",
        "def extract_activations(model, dataset):\n",
        "    activations = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataset:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            activations.append(outputs.squeeze().cpu().numpy())\n",
        "            print(\"Activation size:\", outputs.shape)  # Append activation\n",
        "    return activations\n",
        "'''"
      ],
      "metadata": {
        "id": "O7Ak_Jzl2Hsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Assuming 'patches' is a vector indicating whether patches were present or not for each image (length of 2N)\n",
        "patches = np.array([0]*len(test_dataset) + [1]*len(test_patch_dataset))\n",
        "\n",
        "print('patches', patches.shape)\n",
        "\n",
        "# Assuming 'model' is the pre-trained model and 'test_dataset' and 'test_patch_dataset' are the original and patched image datasets\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Move the model to the same device as the input data\n",
        "model.to(device)\n",
        "\n",
        "# Assuming 'test_dataset' and 'test_patch_dataset' are concatenated together\n",
        "original_activations = extract_activations(model, test_dataset)\n",
        "patched_activations = extract_activations(model, test_patch_dataset)\n",
        "\n",
        "# Split activations into two arrays based on patches\n",
        "original_activations = np.array(original_activations[:len(test_dataset)])\n",
        "patched_activations = np.array(patched_activations[len(test_dataset):])\n",
        "\n",
        "print(\"Original activations shape:\", original_activations.shape)\n",
        "print(\"Patched activations shape:\", patched_activations.shape)\n",
        "\n",
        "\n",
        "\n",
        "#first_5_entries = original_activations[:5]\n",
        "#print(first_5_entries)\n",
        "'''"
      ],
      "metadata": {
        "id": "KJLycFRlzhEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(original_activations)"
      ],
      "metadata": {
        "id": "er4Qj3DHE4bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "variant 2"
      ],
      "metadata": {
        "id": "4zlVtOWq-RMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Load pre-trained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Define the layer index you want to access\n",
        "layer_index = 3\n",
        "\n",
        "# Access the desired layer\n",
        "target_layer = model.classifier[layer_index]\n",
        "\n",
        "print(target_layer)\n",
        "'''"
      ],
      "metadata": {
        "id": "ggKYAW4XHVSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Modify the classifier\n",
        "model.classifier[-1] = torch.nn.Linear(4096, 2)\n",
        "\n",
        "# Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240415115229.pt\"\n",
        "model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Function to preprocess an image and extract activations\n",
        "def preprocess_and_extract_activations(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path)\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Flatten the tensor before passing it to the linear layers\n",
        "    image_tensor = model.features(image_tensor)\n",
        "    image_tensor = model.avgpool(image_tensor)\n",
        "    image_tensor = torch.flatten(image_tensor, 1)  # Flatten the tensor\n",
        "    print(\"Flattened tensor shape:\", image_tensor.shape)\n",
        "    # Extract activations from the first to the fourth linear layer\n",
        "    activations = []\n",
        "    for layer_idx, layer in enumerate(model.classifier[:6], start=1):  # Slicing to access layers 0 to 5\n",
        "        image_tensor = layer(image_tensor)\n",
        "        activations.append(image_tensor.squeeze().cpu().detach().numpy())\n",
        "        print(f\"Activation {layer_idx} size:\", activations[-1].shape)\n",
        "\n",
        "        '''\n",
        "    for layer_idx, layer in enumerate(model.classifier[:6], start=1):  # Slicing to access layers 0 to 5\n",
        "        image_tensor = layer(image_tensor)\n",
        "        activations.append(image_tensor.squeeze().cpu().detach().numpy())\n",
        "        print(f\"Activation {layer_idx} size:\", activations[-1].shape)  # Print size of current activation\n",
        "        '''\n",
        "\n",
        "    return activations\n",
        "\n",
        "# Function to recursively traverse folders and process images\n",
        "def process_images_in_folder(folder_path):\n",
        "    all_activations = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg')):\n",
        "                image_path = os.path.join(root, file)\n",
        "                activations = preprocess_and_extract_activations(image_path)\n",
        "                if activations is not None:\n",
        "                    all_activations.append(activations)\n",
        "    return all_activations\n",
        "\n",
        "# Folder path containing the images\n",
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "\n",
        "# Extract activations for all images in the folder\n",
        "all_activations = process_images_in_folder(folder_path)\n",
        "\n",
        "# Now you can use all_activations for further analysis, such as classification or visualization\n"
      ],
      "metadata": {
        "id": "6nrs9Zl1-QFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "results_file_path = oj(dir_path, \"auc_f1_224_malig_patch.txt\")\n",
        "print(results_file_path)\n",
        "with open(results_file_path, 'w') as f:\n",
        "    f.write('AUC: ' + str(auc) + \"\\n\")\n",
        "    f.write('F1: ' + str(f1) + \"\\n\")\n",
        "'''"
      ],
      "metadata": {
        "id": "vQe4R_-ayKlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Flattened tensor shape:\", image_tensor.shape)\n"
      ],
      "metadata": {
        "id": "K0vRKwNlMCyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "# Modify the last linear layer to match the output size of the loaded state dictionary\n",
        "model.classifier[-1] = torch.nn.Linear(4096, 2) # Load the saved parameters into the model\n",
        "saved_model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/models/initial_classifier/20240415115229.pt\"\n",
        "#model.classifier.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Define preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Function to preprocess an image and extract activations\n",
        "# Function to preprocess an image and extract activations\n",
        "def preprocess_and_extract_activations(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path)\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Flatten the tensor before passing it to the linear layers\n",
        "    image_tensor = model.features(image_tensor)\n",
        "    image_tensor = model.avgpool(image_tensor)\n",
        "    image_tensor = torch.flatten(image_tensor, 1)  # Flatten the tensor\n",
        "\n",
        "    # Extract activations from the last linear layer before the classification layer\n",
        "    activations = model.classifier[-3](image_tensor)\n",
        "\n",
        "    return activations.squeeze().cpu().detach().numpy()\n",
        "\n",
        "# Function to recursively traverse folders and process images\n",
        "def process_images_in_folder(folder_path):\n",
        "    all_activations = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg')):\n",
        "                image_path = os.path.join(root, file)\n",
        "                activations = preprocess_and_extract_activations(image_path)\n",
        "                if activations is not None:\n",
        "                    all_activations.append(activations)\n",
        "    return all_activations\n",
        "\n",
        "# Folder path containing the images\n",
        "folder_path = \"/content/drive/MyDrive/Masterthesis/Datasets/ISIC_224/malignant_patched_224\"\n",
        "\n",
        "# Extract activations for all images in the folder\n",
        "all_activations = process_images_in_folder(folder_path)\n",
        "\n",
        "# Now you can use all_activations for further analysis, such as classification or visualization\n",
        "'''"
      ],
      "metadata": {
        "id": "kGNRsA7YMp-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}