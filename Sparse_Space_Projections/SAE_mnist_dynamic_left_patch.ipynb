{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUKe/HpjtofamoksEzpFkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/Shortcut_Learning/blob/main/SAE_mnist_dynamic_left_patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1WEVoosewjfMjD8UuH2meIMx-nFahH1Uj#scrollTo=gqAJrN4UcEQr"
      ],
      "metadata": {
        "id": "dKeVpYldQMg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFxmb0BQHmjc",
        "outputId": "c7e6ac39-fccd-4a36-9442-55a15db12435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "izV_UoBcHtrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom AlexNet model based on your structure\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, width_mult=1):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),  # 96*55*55 (for 224x224 input)\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 96*27*27\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),  # 256*27*27\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 256*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),  # 384*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),  # 384*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 256*13*13\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 256*6*6\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)  # Modify to 2 if you are classifying 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = x.view(-1, 256 * 1 * 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GPwXOTy-QKrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets a random seed for reproducibility, loads and preprocesses images, and initializes an AlexNet model for extracting neural activations at specified layers. It defines functions to load images from directories, preprocess them, and extract model activations at various stages of the network. For each model and layer, activations are flattened, aligned, and averaged across multiple models to provide consistent, comparable representations. The computed activations are then saved to a specified Google Drive path for later analysis. In essence, this script facilitates the extraction and aggregation of layer-wise neural activations across different models and datasets, enabling consistent and reproducible analysis of model behavior on image data."
      ],
      "metadata": {
        "id": "EVZ5ls7Y4_GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "snippet 1"
      ],
      "metadata": {
        "id": "Mu42-82OEER7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "def set_seed(seed=1):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Call set_seed at the beginning of the script to ensure reproducibility\n",
        "set_seed(1)\n",
        "\n",
        "\n",
        "# Step 1: Preprocess images, extract or load activations\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        print(f\"Initializing dataset with {len(image_paths)} images\")\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        print(f\"Loading image: {image_path}\")\n",
        "        image = Image.open(image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def load_model(model_path):\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model = AlexNet()\n",
        "    #model.classifier[-1] = torch.nn.Linear(4096, 2)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully\")\n",
        "    return model\n",
        "\n",
        "# Preprocessing function\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((64, 64)),  # Match the resize shape in MnistDataset\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Match the normalization values in MnistDataset\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_and_extract_activations(model, dataloader, layer):\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensor in dataloader:\n",
        "            image_tensor = image_tensor.to(device)\n",
        "\n",
        "            # Extract activations according to your custom AlexNet model's structure\n",
        "            if layer == 0:  # After Layer 1\n",
        "                tensor = model.layer1(image_tensor)\n",
        "            elif layer == 1:  # After Layer 2\n",
        "                tensor = model.layer2(model.layer1(image_tensor))\n",
        "            elif layer == 2:  # After Layer 3\n",
        "                tensor = model.layer3(model.layer2(model.layer1(image_tensor)))\n",
        "            elif layer == 3:  # After Layer 4\n",
        "                tensor = model.layer4(model.layer3(model.layer2(model.layer1(image_tensor))))\n",
        "            elif layer == 4:  # After Layer 5\n",
        "                tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            elif layer == 5:  # After FC1\n",
        "                tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "                tensor = tensor.view(-1, 256 * 1 * 1)  # Reshape to match the input to FC1\n",
        "                tensor = model.fc1(tensor)\n",
        "            elif layer == 6:  # After FC2\n",
        "                tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "                tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "                tensor = model.fc2(model.fc1(tensor))\n",
        "            elif layer == 7:  # After FC3 (Output)\n",
        "                tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "                tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "                tensor = model.fc3(model.fc2(model.fc1(tensor)))\n",
        "\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "\n",
        "            # Clear memory\n",
        "            del tensor\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    print(f\"Extracted activations for {len(activations)} images\")\n",
        "    return activations\n",
        "\n",
        "\n",
        "def process_images_in_folder(model, folder_path, layer, batch_size=1):  # Set batch_size=1 to avoid batch aggregation\n",
        "    all_layer_activations = []\n",
        "\n",
        "    image_paths = [os.path.join(root, file)\n",
        "                   for root, dirs, files in os.walk(folder_path)\n",
        "                   for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    dataset = ImageDataset(image_paths, transform=preprocess)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    activations = preprocess_and_extract_activations(model, dataloader, layer)\n",
        "    all_layer_activations.extend(activations)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return all_layer_activations\n",
        "\n",
        "def flatten_and_align_activations(activations_list):\n",
        "    print(\"Flattening and aligning activations\")\n",
        "    flat_activations = [act.flatten() for act in activations_list]\n",
        "    max_length = max(len(act) for act in flat_activations)\n",
        "\n",
        "    aligned_activations = []\n",
        "    for activation in flat_activations:\n",
        "        if len(activation) < max_length:\n",
        "            padded_activation = np.pad(activation, (0, max_length - len(activation)), 'constant')\n",
        "        else:\n",
        "            padded_activation = activation[:max_length]\n",
        "        aligned_activations.append(padded_activation)\n",
        "    print(f\"Aligned activations to shape: {np.vstack(aligned_activations).shape}\")\n",
        "    return np.vstack(aligned_activations)\n",
        "\n",
        "def save_activations(activations, folder_name, filename):\n",
        "    drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/{folder_name}/{filename}.npy'\n",
        "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "    print(f\"Saving activations to {drive_path}\")\n",
        "    np.save(drive_path, activations)\n",
        "\n",
        "def load_activations(folder_name, filename):\n",
        "    drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/{folder_name}/{filename}.npy'\n",
        "    print(f\"Loading activations from {drive_path}\")\n",
        "    return np.load(drive_path, allow_pickle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Check if activations are already present and load them instead of saving them\n",
        "'''\n",
        "def compute_activations_for_layers(model_paths, folder_paths, layers, activations_file_prefix):\n",
        "    # Initialize the dictionary to store activations\n",
        "    all_layer_activations = {layer: {key: [] for key in folder_paths.keys()} for layer in layers}\n",
        "\n",
        "    # Loop through folder paths (e.g., 'wb_patch', 'wb_no_patch', etc.)\n",
        "    for folder_name, folder_path in folder_paths.items():\n",
        "        print(f\"Processing folder: {folder_name}\")\n",
        "\n",
        "        for layer in layers:\n",
        "            # Construct the file path for the saved activations\n",
        "            activations_filename = f'{activations_file_prefix}_{folder_name}'\n",
        "            drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/layer_{layer}/{activations_filename}.npy'\n",
        "\n",
        "            # Check if the activations file already exists\n",
        "            if os.path.exists(drive_path):\n",
        "                print(f\"Activations for layer {layer} and folder {folder_name} found. Loading from file.\")\n",
        "                averaged_activations = load_activations(f'layer_{layer}', activations_filename)\n",
        "            else:\n",
        "                print(f\"Activations for layer {layer} and folder {folder_name} not found. Extracting...\")\n",
        "\n",
        "                combined_activations = None  # To store the sum of activations across models\n",
        "\n",
        "                # Process activations for each model\n",
        "                for model_idx, model_path in enumerate(model_paths):\n",
        "                    print(f\"Processing model {model_idx + 1}/{len(model_paths)}\")\n",
        "                    model = load_model(model_path)\n",
        "\n",
        "                    # Compute activations for the folder\n",
        "                    activations = process_images_in_folder(model, folder_path, layer)\n",
        "                    activations = flatten_and_align_activations(activations)\n",
        "\n",
        "                    # Accumulate activations across models\n",
        "                    if combined_activations is None:\n",
        "                        combined_activations = np.array(activations, dtype=np.float32)\n",
        "                    else:\n",
        "                        combined_activations += np.array(activations, dtype=np.float32)\n",
        "\n",
        "                    # Free resources\n",
        "                    del model\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                # Average activations across models\n",
        "                averaged_activations = combined_activations / len(model_paths)\n",
        "\n",
        "                # Save averaged activations\n",
        "                save_activations(averaged_activations, f'layer_{layer}', activations_filename)\n",
        "\n",
        "            # Store in the dictionary\n",
        "            all_layer_activations[layer][folder_name].append(averaged_activations)\n",
        "\n",
        "    return all_layer_activations\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Modify the compute_activations_for_layers function to average activations across models\n",
        "def compute_activations_for_layers(model_paths, folder_paths, layers, activations_file_prefix):\n",
        "    # Initialize the dictionary with all keys in folder_paths\n",
        "    all_layer_activations = {layer: {key: [] for key in folder_paths.keys()} for layer in layers}\n",
        "\n",
        "    # Loop through folder paths (e.g., 'wb_patch', 'wb_no_patch', etc.)\n",
        "    for folder_name, folder_path in folder_paths.items():\n",
        "        print(f\"Processing folder {folder_name}\")\n",
        "\n",
        "        for layer in layers:\n",
        "            combined_activations = None  # To store the sum of activations across models\n",
        "\n",
        "            # Process activations for each model\n",
        "            for model_idx, model_path in enumerate(model_paths):\n",
        "                print(f\"Processing model {model_idx + 1}/{len(model_paths)}\")\n",
        "                model = load_model(model_path)\n",
        "\n",
        "                # Compute activations for the folder\n",
        "                activations = process_images_in_folder(model, folder_path, layer)\n",
        "                activations = flatten_and_align_activations(activations)\n",
        "\n",
        "                # Accumulate activations across models\n",
        "                if combined_activations is None:\n",
        "                    combined_activations = np.array(activations, dtype=np.float32)\n",
        "                else:\n",
        "                    combined_activations += np.array(activations, dtype=np.float32)\n",
        "\n",
        "                # Free resources\n",
        "                del model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            # Average activations across models\n",
        "            averaged_activations = combined_activations / len(model_paths)\n",
        "            all_layer_activations[layer][folder_name].append(averaged_activations)\n",
        "\n",
        "            # Save averaged activations\n",
        "            save_activations(averaged_activations, f'layer_{layer}', f'{activations_file_prefix}_{folder_name}')\n",
        "\n",
        "    return all_layer_activations\n",
        "\n",
        "# Step 2: Extract Training Activations\n",
        "\n",
        "model_paths = [\n",
        "    \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\",\n",
        "    \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_11train.pt\",\n",
        "    \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_111train.pt\"\n",
        "]\n",
        "\n",
        "layers_of_interest = [5, 6]\n",
        "\n",
        "# Paths to training data\n",
        "#folder_paths = {\n",
        "    #'two_no_patch': '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/train/class_2',\n",
        "    #'zero_no_patch': '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/train/class_0_half',\n",
        "    #'zero_patch': '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/train/class_0_half_second'\n",
        "#}\n",
        "\n",
        "# Extract and save averaged training activations across three models\n",
        "#train_activations = compute_activations_for_layers(model_paths, folder_paths, layers_of_interest, 'train')\n",
        "\n"
      ],
      "metadata": {
        "id": "BipeZnpqHu78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a framework for loading activations, building a sparse autoencoder, and training the autoencoder with early stopping and loss tracking for neural network representations."
      ],
      "metadata": {
        "id": "iq7jHykE5N2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "snippet 2"
      ],
      "metadata": {
        "id": "e74uhJ9uEGJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Step 1: Load the activations from Google Drive\n",
        "def load_saved_activations(layer, subset, activations_file_prefix):\n",
        "    # Define the path where activations were saved on Google Drive\n",
        "    drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/layer_{layer}/{activations_file_prefix}_{subset}.npy'\n",
        "    print(f\"Loading activations from {drive_path}\")\n",
        "\n",
        "    # Load the saved .npy file from the path\n",
        "    if os.path.exists(drive_path):\n",
        "        activations = np.load(drive_path, allow_pickle=True)\n",
        "        print(f\"Loaded activations for layer {layer}, subset {subset}. Shape: {activations.shape}\")\n",
        "        return activations\n",
        "    else:\n",
        "        print(f\"Activations file {drive_path} does not exist.\")\n",
        "        return None\n",
        "\n",
        "# Step 2: Define Sparse Autoencoder with KL-divergence\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, in_dims, h_dims, sparsity_lambda=1e-3, sparsity_target=0.10, xavier_norm_init=True):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "        self.in_dims = in_dims  # Input dimension (number of neurons in the input layer)\n",
        "        self.h_dims = h_dims  # Hidden dimension (number of neurons in the hidden layer)\n",
        "        self.sparsity_lambda = sparsity_lambda  # Weight for the sparsity penalty term\n",
        "        self.sparsity_target = sparsity_target  # Target sparsity (desired average activation)\n",
        "\n",
        "        # Encoder: Projects input to the hidden (sparse) space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_dims, self.h_dims),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        if xavier_norm_init:\n",
        "            nn.init.xavier_uniform_(self.encoder[0].weight)  # Xavier initialization\n",
        "\n",
        "        # Decoder: Reconstructs the input from the hidden (sparse) representation\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.h_dims, self.in_dims),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        if xavier_norm_init:\n",
        "            nn.init.xavier_uniform_(self.decoder[0].weight)\n",
        "\n",
        "    # Forward pass through the encoder and decoder\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)  # Pass input through encoder\n",
        "        decoded = self.decoder(encoded)  # Pass encoded (sparse) representation through decoder\n",
        "        return encoded, decoded\n",
        "\n",
        "    # KL-divergence sparsity penalty calculation\n",
        "    def sparsity_penalty(self, encoded):\n",
        "        rho_hat = torch.mean(encoded, dim=0)  # Compute the average activation for each hidden neuron\n",
        "        rho = torch.ones_like(rho_hat) * self.sparsity_target  # Target sparsity value\n",
        "        epsilon = 1e-8  # Small value to avoid log(0)\n",
        "        kl_divergence = F.kl_div((rho_hat + epsilon).log(), rho + epsilon, reduction='batchmean')  # KL-divergence\n",
        "        return self.sparsity_lambda * kl_divergence  # Return the sparsity penalty, weighted by lambda\n",
        "\n",
        "    # Loss function combining MSE (reconstruction error) and sparsity penalty\n",
        "    def loss_function(self, decoded, original, encoded):\n",
        "        mse_loss = F.mse_loss(decoded, original)  # Mean Squared Error for reconstruction\n",
        "        sparsity_loss = self.sparsity_penalty(encoded)  # Sparsity penalty for hidden layer activations\n",
        "        return mse_loss + sparsity_loss  # Total loss is MSE + sparsity penalty\n",
        "\n",
        "\n",
        "# Early stopping mechanism to prevent overfitting\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=30, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta  # Minimum change to qualify as an improvement\n",
        "        self.best_loss = None  # Best validation loss observed so far\n",
        "        self.counter = 0  # Counter to keep track of how many epochs since the last improvement\n",
        "\n",
        "    # Check if training should be stopped based on validation loss\n",
        "    def check(self, loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = loss  # Set the initial best loss\n",
        "            return False\n",
        "\n",
        "        # If the loss has improved significantly\n",
        "        if loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = loss  # Update best loss\n",
        "            self.counter = 0  # Reset counter\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1  # Increment counter if no improvement\n",
        "            if self.counter >= self.patience:\n",
        "                print(\"Early stopping triggered.\")  # Stop training if patience is exceeded\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "# Training Function with Loss Tracking and Plotting\n",
        "def train_autoencoder(autoencoder, data, num_epochs=500, batch_size=128, learning_rate=1e-5, validation_split=0.2, clip_gradients=True, max_grad_norm=0.5):\n",
        "    print(f\"Training autoencoder with input dim {data.shape[1]} and encoding dim {autoencoder.h_dims}\")\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    num_train = int((1 - validation_split) * len(data))  # Compute the number of training samples\n",
        "    train_data = data[:num_train]  # Training data\n",
        "    val_data = data[num_train:]  # Validation data\n",
        "\n",
        "    # Create PyTorch datasets and dataloaders for training and validation\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float())\n",
        "    val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(val_data).float())\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Adam optimizer with learning rate and weight decay for regularization\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
        "\n",
        "    # Initialize early stopping\n",
        "    early_stopping = EarlyStopping(patience=50, min_delta=0.001)\n",
        "\n",
        "    # Lists to store training and validation loss values\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Start training loop\n",
        "    autoencoder.train()  # Set the autoencoder in training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0  # Initialize training loss for the current epoch\n",
        "\n",
        "        # Iterate over batches in the training set\n",
        "        for x_batch, in train_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            encoded, decoded = autoencoder(x_batch)  # Forward pass through autoencoder\n",
        "            loss = autoencoder.loss_function(decoded, x_batch, encoded)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the error\n",
        "\n",
        "            # Apply gradient clipping if enabled\n",
        "            if clip_gradients:\n",
        "                torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), max_grad_norm)\n",
        "\n",
        "            optimizer.step()  # Update the weights using the optimizer\n",
        "            total_train_loss += loss.item()  # Accumulate training loss for this batch\n",
        "\n",
        "        # Validation step after each epoch\n",
        "        total_val_loss = 0  # Initialize validation loss\n",
        "        autoencoder.eval()  # Set the autoencoder in evaluation mode\n",
        "        with torch.no_grad():  # No gradient calculation in validation mode\n",
        "            for x_batch, in val_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                encoded, decoded = autoencoder(x_batch)\n",
        "                loss = autoencoder.loss_function(decoded, x_batch, encoded)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        # Compute the average training and validation loss for this epoch\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Store the loss values for plotting later\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Print progress for the current epoch\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')\n",
        "\n",
        "        # Check early stopping condition based on validation loss\n",
        "        if early_stopping.check(avg_val_loss):\n",
        "            break\n",
        "\n",
        "    print(\"Autoencoder training completed\")\n",
        "\n",
        "    # Plot the training and validation loss over epochs\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h5bqAiSaHwMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is responsible for loading preprocessed activations, training sparse autoencoders for specified layers, and saving the trained SAE models for later use."
      ],
      "metadata": {
        "id": "ekR9stBb5WDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "snippet 3"
      ],
      "metadata": {
        "id": "xlyiWRMKEH7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths and Layers Definition\n",
        "save_sae_dir = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/Autoencoders/'  # Directory to save the trained autoencoder\n",
        "os.makedirs(save_sae_dir, exist_ok=True)\n",
        "\n",
        "#layers_of_interest = [5, 6]  # Define which layers' activations to train autoencoders on\n",
        "layers_of_interest = [6]\n",
        "# Step 3: Load the preprocessed activations from drive and train the autoencoders\n",
        "for layer in layers_of_interest:\n",
        "    print(f'\\nTraining autoencoder for layer {layer}')\n",
        "\n",
        "    activations_list = []\n",
        "    for subset in ['two_no_patch', 'zero_no_patch', 'zero_patch']:\n",
        "        # Load activations for each subset and layer\n",
        "        activations = load_saved_activations(layer, subset, 'train')\n",
        "        if activations is not None:\n",
        "            activations_list.append(activations)\n",
        "\n",
        "    # Check if activations were loaded correctly\n",
        "    if len(activations_list) == 0:\n",
        "        print(f\"No activations found for layer {layer}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Concatenate all activations for this layer into a single array\n",
        "    combined_activations = np.vstack(activations_list)\n",
        "    print(f\"Combined activations shape for layer {layer}: {combined_activations.shape}\")\n",
        "\n",
        "    # Print the number of combined activations (i.e., number of samples)\n",
        "    num_combined_activations = combined_activations.shape[0]\n",
        "    print(f\"Number of combined activations (samples): {num_combined_activations}\")\n",
        "\n",
        "    # Set the encoding dimension (hidden layer size) as --almost-- 2 times the number of combined activations\n",
        "    encoding_dim = min(8192, 8192)  # Cap the encoding dim at 8192 also use it for later experimentations on oter layers\n",
        "    print(f\"Setting encoding dimension (h_dim) as: {encoding_dim}\")\n",
        "\n",
        "    # Input dimension is the number of neurons in the original input layer\n",
        "    input_dim = combined_activations.shape[1]\n",
        "\n",
        "    # Initialize the autoencoder with the specified input and hidden dimensions\n",
        "    autoencoder = SparseAutoencoder(input_dim, encoding_dim).to(device)\n",
        "\n",
        "    # Train the autoencoder on the combined activations\n",
        "    print(f\"Training autoencoder for layer {layer}\")\n",
        "    autoencoder = train_autoencoder(\n",
        "        autoencoder,\n",
        "        combined_activations,  # unnormalized activations\n",
        "        num_epochs=500,\n",
        "        learning_rate=1e-5\n",
        "    )\n",
        "\n",
        "    # Save the trained autoencoder\n",
        "    save_path = os.path.join(save_sae_dir, f'autoencoder_layer_{layer}.pth')\n",
        "    torch.save(autoencoder.state_dict(), save_path)\n",
        "    print(f\"Autoencoder for layer {layer} saved at {save_path}\")\n",
        "\n",
        "    # Clear memory after saving\n",
        "    del combined_activations, autoencoder\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nAll autoencoders trained and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gmYi78ByHxs0",
        "outputId": "b60401f6-319c-4b64-fabf-3ddb458f6710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training autoencoder for layer 6\n",
            "Loading activations from /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/layer_6/train_two_no_patch.npy\n",
            "Loaded activations for layer 6, subset two_no_patch. Shape: (5958, 4096)\n",
            "Loading activations from /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/layer_6/train_zero_no_patch.npy\n",
            "Loaded activations for layer 6, subset zero_no_patch. Shape: (5923, 4096)\n",
            "Loading activations from /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/average/layer_6/train_zero_patch.npy\n",
            "Loaded activations for layer 6, subset zero_patch. Shape: (2500, 4096)\n",
            "Combined activations shape for layer 6: (14381, 4096)\n",
            "Number of combined activations (samples): 14381\n",
            "Setting encoding dimension (h_dim) as: 8192\n",
            "Training autoencoder for layer 6\n",
            "Training autoencoder with input dim 4096 and encoding dim 8192\n",
            "Epoch 1/500, Train Loss: 2.5721774948967826, Val Loss: 2.2065611455751504\n",
            "Epoch 2/500, Train Loss: 2.1931030962202285, Val Loss: 2.1663742635561074\n",
            "Epoch 3/500, Train Loss: 2.178900210062663, Val Loss: 2.1564296898634536\n",
            "Epoch 4/500, Train Loss: 2.1678650538126627, Val Loss: 2.1389627767645796\n",
            "Epoch 5/500, Train Loss: 2.155114001697964, Val Loss: 2.130753387575564\n",
            "Epoch 6/500, Train Loss: 2.1469039652082653, Val Loss: 2.1170090903406558\n",
            "Epoch 7/500, Train Loss: 2.1355709155400593, Val Loss: 2.1124665322511094\n",
            "Epoch 8/500, Train Loss: 2.12875407271915, Val Loss: 2.10060657625613\n",
            "Epoch 9/500, Train Loss: 2.1137229681015013, Val Loss: 2.0893635697986768\n",
            "Epoch 10/500, Train Loss: 2.1028961261113484, Val Loss: 2.075767480808756\n",
            "Epoch 11/500, Train Loss: 2.090291659037272, Val Loss: 2.0656936894292417\n",
            "Epoch 12/500, Train Loss: 2.083040270540449, Val Loss: 2.061713192773902\n",
            "Epoch 13/500, Train Loss: 2.0797834171189202, Val Loss: 2.0591312284054966\n",
            "Epoch 14/500, Train Loss: 2.0741225361824034, Val Loss: 2.045538093732751\n",
            "Epoch 15/500, Train Loss: 2.0567951255374486, Val Loss: 2.0330656818721606\n",
            "Epoch 16/500, Train Loss: 2.051226586765713, Val Loss: 2.026674721551978\n",
            "Epoch 17/500, Train Loss: 2.0452811241149904, Val Loss: 2.0229782177054365\n",
            "Epoch 18/500, Train Loss: 2.043675434589386, Val Loss: 2.0219163324521934\n",
            "Epoch 19/500, Train Loss: 2.0361160622702705, Val Loss: 2.0134446050809776\n",
            "Epoch 20/500, Train Loss: 2.0326111965709264, Val Loss: 2.009948248448579\n",
            "Epoch 21/500, Train Loss: 2.0290458493762547, Val Loss: 2.0070314925649893\n",
            "Epoch 22/500, Train Loss: 2.026853530936771, Val Loss: 2.0058632778084795\n",
            "Epoch 23/500, Train Loss: 2.0205838680267334, Val Loss: 1.9899035122083581\n",
            "Epoch 24/500, Train Loss: 2.004728284147051, Val Loss: 1.9818882320238196\n",
            "Epoch 25/500, Train Loss: 2.0027937054634095, Val Loss: 1.9796476571456245\n",
            "Epoch 26/500, Train Loss: 1.9980370614263747, Val Loss: 1.975192489831344\n",
            "Epoch 27/500, Train Loss: 1.995133313867781, Val Loss: 1.9741366635198179\n",
            "Epoch 28/500, Train Loss: 1.9928935050964356, Val Loss: 1.9716183361799822\n",
            "Epoch 29/500, Train Loss: 1.991711805926429, Val Loss: 1.969836898471998\n",
            "Epoch 30/500, Train Loss: 1.9905204931894938, Val Loss: 1.9697496061739714\n",
            "Epoch 31/500, Train Loss: 1.9900978750652738, Val Loss: 1.969232439994812\n",
            "Epoch 32/500, Train Loss: 1.9900736331939697, Val Loss: 1.969193950943325\n",
            "Epoch 33/500, Train Loss: 1.989054372575548, Val Loss: 1.9685135768807454\n",
            "Epoch 34/500, Train Loss: 1.9865572717454698, Val Loss: 1.965485920076785\n",
            "Epoch 35/500, Train Loss: 1.9853781355751885, Val Loss: 1.9644687538561614\n",
            "Epoch 36/500, Train Loss: 1.9840797914399042, Val Loss: 1.9626416175261787\n",
            "Epoch 37/500, Train Loss: 1.9810319794548883, Val Loss: 1.96179155163143\n",
            "Epoch 38/500, Train Loss: 1.9794440454906888, Val Loss: 1.9612674920455269\n",
            "Epoch 39/500, Train Loss: 1.9787927150726319, Val Loss: 1.9609717908112898\n",
            "Epoch 40/500, Train Loss: 1.978493262661828, Val Loss: 1.9602007399434629\n",
            "Epoch 41/500, Train Loss: 1.9766947017775642, Val Loss: 1.9580654890640923\n",
            "Epoch 42/500, Train Loss: 1.9752620750003391, Val Loss: 1.9577218190483425\n",
            "Epoch 43/500, Train Loss: 1.9740860329733954, Val Loss: 1.9560203448585842\n",
            "Epoch 44/500, Train Loss: 1.9698904580540126, Val Loss: 1.9452975988388062\n",
            "Epoch 45/500, Train Loss: 1.9583250575595432, Val Loss: 1.9373817962148916\n",
            "Epoch 46/500, Train Loss: 1.953278330961863, Val Loss: 1.9369918211646702\n",
            "Epoch 47/500, Train Loss: 1.9534187767240736, Val Loss: 1.9363680300505266\n",
            "Epoch 48/500, Train Loss: 1.9529378533363342, Val Loss: 1.936163860818614\n",
            "Epoch 49/500, Train Loss: 1.9527822163369921, Val Loss: 1.9362458353457244\n",
            "Epoch 50/500, Train Loss: 1.9480931136343214, Val Loss: 1.9216447964958523\n",
            "Epoch 51/500, Train Loss: 1.9382967578040229, Val Loss: 1.9166328181391177\n",
            "Epoch 52/500, Train Loss: 1.9352288603782655, Val Loss: 1.9162276257639346\n",
            "Epoch 53/500, Train Loss: 1.9346304204728868, Val Loss: 1.915343745895054\n",
            "Epoch 54/500, Train Loss: 1.9327332350942823, Val Loss: 1.912574229033097\n",
            "Epoch 55/500, Train Loss: 1.9289502170350816, Val Loss: 1.9087385716645613\n",
            "Epoch 56/500, Train Loss: 1.9279782401190864, Val Loss: 1.9093051267706829\n",
            "Epoch 57/500, Train Loss: 1.9276314987076653, Val Loss: 1.9086039843766585\n",
            "Epoch 58/500, Train Loss: 1.92546829117669, Val Loss: 1.9040006658305293\n",
            "Epoch 59/500, Train Loss: 1.9213800893889532, Val Loss: 1.8996184338694033\n",
            "Epoch 60/500, Train Loss: 1.9173052615589565, Val Loss: 1.8986498583918032\n",
            "Epoch 61/500, Train Loss: 1.9150721814897325, Val Loss: 1.8952843261801677\n",
            "Epoch 62/500, Train Loss: 1.9123332540194193, Val Loss: 1.8951730054357778\n",
            "Epoch 63/500, Train Loss: 1.9121766474511888, Val Loss: 1.8955401493155437\n",
            "Epoch 64/500, Train Loss: 1.9116290741496615, Val Loss: 1.8943969944249028\n",
            "Epoch 65/500, Train Loss: 1.9107612318462797, Val Loss: 1.8934990996899812\n",
            "Epoch 66/500, Train Loss: 1.906770439942678, Val Loss: 1.8882034135901409\n",
            "Epoch 67/500, Train Loss: 1.9037997881571451, Val Loss: 1.8885539718296216\n",
            "Epoch 68/500, Train Loss: 1.9034389164712695, Val Loss: 1.8870836392692898\n",
            "Epoch 69/500, Train Loss: 1.902598508199056, Val Loss: 1.8859105680299841\n",
            "Epoch 70/500, Train Loss: 1.9001492884423998, Val Loss: 1.8837592549945996\n",
            "Epoch 71/500, Train Loss: 1.8998730354838902, Val Loss: 1.8847128308337668\n",
            "Epoch 72/500, Train Loss: 1.8986813108126321, Val Loss: 1.881678767826246\n",
            "Epoch 73/500, Train Loss: 1.8978615827030605, Val Loss: 1.88104596345321\n",
            "Epoch 74/500, Train Loss: 1.897244155406952, Val Loss: 1.8798009882802549\n",
            "Epoch 75/500, Train Loss: 1.8953632844818964, Val Loss: 1.8779323412024456\n",
            "Epoch 76/500, Train Loss: 1.8924913671281602, Val Loss: 1.8732301981552788\n",
            "Epoch 77/500, Train Loss: 1.8899154239230687, Val Loss: 1.873429738956949\n",
            "Epoch 78/500, Train Loss: 1.8898774226506552, Val Loss: 1.8700070692145305\n",
            "Epoch 79/500, Train Loss: 1.886381557252672, Val Loss: 1.8684535648511804\n",
            "Epoch 80/500, Train Loss: 1.8862349324756198, Val Loss: 1.8690102981484455\n",
            "Epoch 81/500, Train Loss: 1.886264595720503, Val Loss: 1.8683751614197441\n",
            "Epoch 82/500, Train Loss: 1.8862302369541593, Val Loss: 1.8685108941534292\n",
            "Epoch 83/500, Train Loss: 1.8860211862458123, Val Loss: 1.8663071860437808\n",
            "Epoch 84/500, Train Loss: 1.8832648356755575, Val Loss: 1.865210807841757\n",
            "Epoch 85/500, Train Loss: 1.882771541012658, Val Loss: 1.8651612841564675\n",
            "Epoch 86/500, Train Loss: 1.882627801100413, Val Loss: 1.8647917146268098\n",
            "Epoch 87/500, Train Loss: 1.8815397050645617, Val Loss: 1.8625488592230754\n",
            "Epoch 88/500, Train Loss: 1.8800886617766486, Val Loss: 1.8625124278275862\n",
            "Epoch 89/500, Train Loss: 1.8780872172779508, Val Loss: 1.8590711666190105\n",
            "Epoch 90/500, Train Loss: 1.8763276855150859, Val Loss: 1.8580492061117422\n",
            "Epoch 91/500, Train Loss: 1.8757011546028985, Val Loss: 1.8578466643457827\n",
            "Epoch 92/500, Train Loss: 1.8732692850960626, Val Loss: 1.8549632818802544\n",
            "Epoch 93/500, Train Loss: 1.8724114484257168, Val Loss: 1.853245698887369\n",
            "Epoch 94/500, Train Loss: 1.8704994002978006, Val Loss: 1.8516884783039922\n",
            "Epoch 95/500, Train Loss: 1.8695853352546692, Val Loss: 1.8513295650482178\n",
            "Epoch 96/500, Train Loss: 1.8674147579405043, Val Loss: 1.8475122607272605\n",
            "Epoch 97/500, Train Loss: 1.8659304870499505, Val Loss: 1.846735601839812\n",
            "Epoch 98/500, Train Loss: 1.8658718175358242, Val Loss: 1.8468746827996296\n",
            "Epoch 99/500, Train Loss: 1.8647236029307048, Val Loss: 1.8450320233469424\n",
            "Epoch 100/500, Train Loss: 1.8630681819385952, Val Loss: 1.8420442550078682\n",
            "Epoch 101/500, Train Loss: 1.859980985853407, Val Loss: 1.8423377586447673\n",
            "Epoch 102/500, Train Loss: 1.8500680261188083, Val Loss: 1.8062808513641357\n",
            "Epoch 103/500, Train Loss: 1.8129130959510804, Val Loss: 1.7884354461794314\n",
            "Epoch 104/500, Train Loss: 1.8105618198712667, Val Loss: 1.7894641057304714\n",
            "Epoch 105/500, Train Loss: 1.8116688595877752, Val Loss: 1.7917414955470874\n",
            "Epoch 106/500, Train Loss: 1.813009994559818, Val Loss: 1.7897837524828704\n",
            "Epoch 107/500, Train Loss: 1.810752981238895, Val Loss: 1.7884192155755085\n",
            "Epoch 108/500, Train Loss: 1.810180089208815, Val Loss: 1.787236537622369\n",
            "Epoch 109/500, Train Loss: 1.8091979304949442, Val Loss: 1.7877433844234631\n",
            "Epoch 110/500, Train Loss: 1.8087545143233406, Val Loss: 1.786198255808457\n",
            "Epoch 111/500, Train Loss: 1.8087254299057856, Val Loss: 1.7867423166399417\n",
            "Epoch 112/500, Train Loss: 1.808548539214664, Val Loss: 1.7863049947697183\n",
            "Epoch 113/500, Train Loss: 1.8084238992797004, Val Loss: 1.7862105680548626\n",
            "Epoch 114/500, Train Loss: 1.8085828728146023, Val Loss: 1.7863995329193447\n",
            "Epoch 115/500, Train Loss: 1.808414265844557, Val Loss: 1.7867869605188784\n",
            "Epoch 116/500, Train Loss: 1.8085280484623378, Val Loss: 1.786184365334718\n",
            "Epoch 117/500, Train Loss: 1.8085436595810784, Val Loss: 1.7870562335719233\n",
            "Epoch 118/500, Train Loss: 1.8084663033485413, Val Loss: 1.7870260114255159\n",
            "Epoch 119/500, Train Loss: 1.8082748982641432, Val Loss: 1.7865597916686016\n",
            "Epoch 120/500, Train Loss: 1.8083449575636121, Val Loss: 1.786120945992677\n",
            "Epoch 121/500, Train Loss: 1.8084779726134406, Val Loss: 1.785903326843096\n",
            "Epoch 122/500, Train Loss: 1.8085242629051208, Val Loss: 1.786726422931837\n",
            "Epoch 123/500, Train Loss: 1.8083297000990974, Val Loss: 1.786200401575669\n",
            "Epoch 124/500, Train Loss: 1.808325809902615, Val Loss: 1.7869027904842212\n",
            "Epoch 125/500, Train Loss: 1.8083302325672574, Val Loss: 1.7866290082102236\n",
            "Epoch 126/500, Train Loss: 1.8081109391318426, Val Loss: 1.7865835661473481\n",
            "Epoch 127/500, Train Loss: 1.8082076403829785, Val Loss: 1.7865370304688164\n",
            "Epoch 128/500, Train Loss: 1.8081112265586854, Val Loss: 1.7861787910046785\n",
            "Epoch 129/500, Train Loss: 1.8082223706775242, Val Loss: 1.7863766369612322\n",
            "Epoch 130/500, Train Loss: 1.8082242674297757, Val Loss: 1.7862214368322622\n",
            "Epoch 131/500, Train Loss: 1.8081010977427165, Val Loss: 1.786105682020602\n",
            "Epoch 132/500, Train Loss: 1.8078808877203199, Val Loss: 1.786379819330962\n",
            "Epoch 133/500, Train Loss: 1.8079363756709628, Val Loss: 1.7866917045220085\n",
            "Epoch 134/500, Train Loss: 1.8078889913029141, Val Loss: 1.7863418226656707\n",
            "Epoch 135/500, Train Loss: 1.8077586200502185, Val Loss: 1.7861891648043757\n",
            "Epoch 136/500, Train Loss: 1.807874365647634, Val Loss: 1.786812398744666\n",
            "Epoch 137/500, Train Loss: 1.8079049626986186, Val Loss: 1.7860126547191455\n",
            "Epoch 138/500, Train Loss: 1.807795724603865, Val Loss: 1.785891486250836\n",
            "Epoch 139/500, Train Loss: 1.80797534916136, Val Loss: 1.786144601262134\n",
            "Epoch 140/500, Train Loss: 1.807897569073571, Val Loss: 1.7860611703084863\n",
            "Epoch 141/500, Train Loss: 1.807646205690172, Val Loss: 1.7860513381336047\n",
            "Epoch 142/500, Train Loss: 1.8077644944190978, Val Loss: 1.7857920190562373\n",
            "Epoch 143/500, Train Loss: 1.8078247560395135, Val Loss: 1.786167455756146\n",
            "Epoch 144/500, Train Loss: 1.8077310111787583, Val Loss: 1.7858286759127742\n",
            "Epoch 145/500, Train Loss: 1.807708936267429, Val Loss: 1.785956877729167\n",
            "Epoch 146/500, Train Loss: 1.8078018969959684, Val Loss: 1.7861037798549817\n",
            "Epoch 147/500, Train Loss: 1.8077016009224787, Val Loss: 1.7860678874928018\n",
            "Epoch 148/500, Train Loss: 1.8076231771045261, Val Loss: 1.785541042037632\n",
            "Epoch 149/500, Train Loss: 1.807763601673974, Val Loss: 1.786082563192948\n",
            "Epoch 150/500, Train Loss: 1.807658064365387, Val Loss: 1.7862501040748928\n",
            "Epoch 151/500, Train Loss: 1.8078486363093058, Val Loss: 1.7854662796725398\n",
            "Epoch 152/500, Train Loss: 1.8076751669247946, Val Loss: 1.7864353553108547\n",
            "Epoch 153/500, Train Loss: 1.8076347668965658, Val Loss: 1.7865230700244075\n",
            "Epoch 154/500, Train Loss: 1.8076430241266885, Val Loss: 1.785486884739088\n",
            "Epoch 155/500, Train Loss: 1.8076542735099792, Val Loss: 1.7856342792510986\n",
            "Epoch 156/500, Train Loss: 1.8074805339177449, Val Loss: 1.7860148341759392\n",
            "Epoch 157/500, Train Loss: 1.8076283322440254, Val Loss: 1.7860424985056338\n",
            "Epoch 158/500, Train Loss: 1.8076283031039768, Val Loss: 1.7854448349579521\n",
            "Epoch 159/500, Train Loss: 1.8075518661075167, Val Loss: 1.7859400536703027\n",
            "Epoch 160/500, Train Loss: 1.8075981365309821, Val Loss: 1.7853576644607212\n",
            "Early stopping triggered.\n",
            "Autoencoder training completed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder for layer 6 saved at /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/Autoencoders/autoencoder_layer_6_0.pth\n",
            "\n",
            "All autoencoders trained and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stophere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "7UtFtdfyY66O",
        "outputId": "4250e2d1-92a3-479f-ef5e-42df9482782c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stophere' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-88d56cf18612>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstophere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stophere' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a pipeline to load sparse autoencoders for specific layers of a model, project test activations into a sparse space, and analyze correlations between projected activations with and without patches."
      ],
      "metadata": {
        "id": "dembWydf5fZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "snippet 4"
      ],
      "metadata": {
        "id": "nTpqKNn6EJR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Autoencoders\n",
        "def load_autoencoder(layer, encoding_dim, device):\n",
        "    save_sae_dir = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/Autoencoders/'\n",
        "    save_path = os.path.join(save_sae_dir, f'autoencoder_layer_{layer}.pth')\n",
        "\n",
        "    # Set the input dimension based on the layer\n",
        "    layer_input_dims = {\n",
        "        0: 290400,\n",
        "        1: 186624,\n",
        "        2: 64896,\n",
        "        3: 64896,\n",
        "        4: 9216,\n",
        "        5: 4096,\n",
        "        6: 4096\n",
        "    }\n",
        "\n",
        "    if layer not in layer_input_dims:\n",
        "        print(f\"Input dimension for layer {layer} not found. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    input_dim = layer_input_dims[layer]  # Use the correct input dimension for the layer\n",
        "    print(f\"Loading autoencoder for layer {layer} from {save_path} with input dimension {input_dim}\")\n",
        "\n",
        "    autoencoder = SparseAutoencoder(input_dim, encoding_dim).to(device)\n",
        "\n",
        "    try:\n",
        "        autoencoder.load_state_dict(torch.load(save_path))\n",
        "        autoencoder.eval()\n",
        "        print(f\"Autoencoder for layer {layer} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading autoencoder for layer {layer}: {e}\")\n",
        "        return None\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "# Function to project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(f\"Projecting activations. Shape before projection: {activations.shape}\")\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            # Use activations directly without normalization\n",
        "            projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "        print(f\"Projection successful. Shape after projection: {projected.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during projection: {e}\")\n",
        "        return None\n",
        "\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Extract activations from a model for a folder (with or without patches)\n",
        "def process_images_in_folder(model, folder_path, layer, batch_size=1):\n",
        "    all_layer_activations = []\n",
        "    image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(folder_path) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    dataset = ImageDataset(image_paths, transform=preprocess)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    activations = preprocess_and_extract_activations(model, dataloader, layer)\n",
        "    all_layer_activations.extend(activations)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return flatten_and_align_activations(all_layer_activations)\n",
        "\n",
        "# Extract and project activations into sparse space (now across multiple models)\n",
        "def extract_and_project_test_activations(model_paths, patch_folder, no_patch_folder, layers, device):\n",
        "    projected_test_activations = {'two_patch': {}, 'two_no_patch': {}}\n",
        "\n",
        "    for layer in layers:\n",
        "        print(f'\\n--- Processing layer {layer} ---')\n",
        "\n",
        "        # Step 1: Accumulate activations across models for both patch and no-patch conditions\n",
        "        combined_activations_with_patch = None\n",
        "        combined_activations_without_patch = None\n",
        "\n",
        "        # Process each model's activations\n",
        "        for model_idx, model_path in enumerate(model_paths):\n",
        "            model = load_model(model_path)\n",
        "\n",
        "            # Extract activations with patch\n",
        "            try:\n",
        "                activations_with_patch = process_images_in_folder(model, patch_folder, layer)\n",
        "                if combined_activations_with_patch is None:\n",
        "                    combined_activations_with_patch = np.array(activations_with_patch, dtype=np.float32)\n",
        "                else:\n",
        "                    combined_activations_with_patch += np.array(activations_with_patch, dtype=np.float32)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing 'two_patch' activations for layer {layer}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Extract activations without patch\n",
        "            try:\n",
        "                activations_without_patch = process_images_in_folder(model, no_patch_folder, layer)\n",
        "                if combined_activations_without_patch is None:\n",
        "                    combined_activations_without_patch = np.array(activations_without_patch, dtype=np.float32)\n",
        "                else:\n",
        "                    combined_activations_without_patch += np.array(activations_without_patch, dtype=np.float32)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing 'two_no_patch' activations for layer {layer}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Free resources\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Average activations across models\n",
        "        combined_activations_with_patch /= len(model_paths)\n",
        "        combined_activations_without_patch /= len(model_paths)\n",
        "\n",
        "        # Step 2: Project the averaged activations into the sparse space using autoencoder\n",
        "        autoencoder = load_autoencoder(layer, encoding_dim=8192, device=device)\n",
        "        if autoencoder is None:\n",
        "            print(f\"Skipping layer {layer} due to autoencoder loading error.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            projected_test_activations['two_patch'][layer] = project_activations(autoencoder, combined_activations_with_patch, device)\n",
        "            projected_test_activations['two_no_patch'][layer] = project_activations(autoencoder, combined_activations_without_patch, device)\n",
        "        except Exception as e:\n",
        "            print(f\"Error projecting activations for layer {layer}: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Projection completed for layer {layer}.\")\n",
        "        del autoencoder\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return projected_test_activations\n",
        "\n",
        "# Step 3: Perform Correlation Analysis\n",
        "def plot_histogram_for_layer(layer, layer_name, activations_with_patch, activations_without_patch, color):\n",
        "    x = np.linspace(start=-1, stop=1, num=100)\n",
        "    xaxis = x + (x[1] - x[0]) / 2\n",
        "    xaxis = xaxis[:-1]\n",
        "\n",
        "    patch_no_patch = np.hstack([np.zeros(len(activations_without_patch)), np.ones(len(activations_with_patch))])\n",
        "    combined_activations = np.vstack((activations_without_patch, activations_with_patch))\n",
        "\n",
        "    A = []\n",
        "    for ii in range(combined_activations.shape[1]):\n",
        "        A.append(pearsonr(patch_no_patch, combined_activations[:, ii])[0])\n",
        "\n",
        "    B = np.histogram(A, bins=x)\n",
        "\n",
        "    bin_means = B[0]\n",
        "    bin_stds = np.std(bin_means)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(xaxis, bin_means, '-', color=color, label=layer_name)\n",
        "    ax.fill_between(xaxis, bin_means - bin_stds, bin_means + bin_stds, color=color, alpha=0.2)\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.title(f'Histogram of Activations Correlation for {layer_name}')\n",
        "    plt.xlabel('Correlation')\n",
        "    plt.ylabel('Number of Neurons')\n",
        "    plt.show()\n",
        "\n",
        "# Perform correlation analysis for each layer\n",
        "def perform_correlation_analysis(projected_test_activations, layers_of_interest, layer_names, colors):\n",
        "    for idx, layer in enumerate(layers_of_interest):\n",
        "        plot_histogram_for_layer(\n",
        "            layer,\n",
        "            layer_names[layer],  # Use layer_names dictionary to get the name of the layer\n",
        "            projected_test_activations['two_patch'][layer],\n",
        "            projected_test_activations['two_no_patch'][layer],\n",
        "            colors[idx]\n",
        "        )\n",
        "\n",
        "# Combined plot for all layers\n",
        "def plot_combined_histogram(layer_names, colors, projected_test_activations, layers_of_interest):\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    x = np.linspace(start=-1, stop=1, num=100)\n",
        "    xaxis = x + (x[1] - x[0]) / 2\n",
        "    xaxis = xaxis[:-1]\n",
        "\n",
        "    all_layer_bin_means = []\n",
        "\n",
        "    for idx, layer in enumerate(layers_of_interest):\n",
        "        activations_with_patch = projected_test_activations['two_patch'][layer]\n",
        "        activations_without_patch = projected_test_activations['two_no_patch'][layer]\n",
        "\n",
        "        patch_no_patch = np.hstack([np.zeros(len(activations_without_patch)), np.ones(len(activations_with_patch))])\n",
        "        combined_activations = np.vstack((activations_without_patch, activations_with_patch))\n",
        "\n",
        "        A = []\n",
        "        for ii in range(combined_activations.shape[1]):\n",
        "            A.append(pearsonr(patch_no_patch, combined_activations[:, ii])[0])\n",
        "\n",
        "        B = np.histogram(A, bins=x)\n",
        "\n",
        "        bin_means = B[0] / np.sum(B[0]) if np.sum(B[0]) != 0 else np.zeros_like(B[0])  # Normalize the bin counts\n",
        "        bin_stds = np.std(bin_means)\n",
        "\n",
        "        all_layer_bin_means.append(bin_means)\n",
        "\n",
        "        ax.plot(xaxis, bin_means, '-', color=colors[idx], label=layer_names[layer])\n",
        "        ax.fill_between(xaxis, bin_means - bin_stds, bin_means + bin_stds, color=colors[idx], alpha=0.2)\n",
        "\n",
        "    # Compute average and standard deviation of the normalized histograms across layers\n",
        "    all_layer_bin_means = np.array(all_layer_bin_means)\n",
        "    avg_bin_means = np.mean(all_layer_bin_means, axis=0)\n",
        "    std_bin_means = np.std(all_layer_bin_means, axis=0)\n",
        "\n",
        "    ax.plot(xaxis, avg_bin_means, '-', color='black', label='Average')\n",
        "    ax.fill_between(xaxis, avg_bin_means - std_bin_means, avg_bin_means + std_bin_means, color='black', alpha=0.2)\n",
        "\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.title('Normalized Histogram of Activations Correlation for All Layers in Sparse Space')\n",
        "    plt.xlabel('Correlation')\n",
        "    plt.ylabel('Normalized Number of Neurons')\n",
        "    plt.show()\n",
        "\n",
        "# Main Execution\n",
        "\n",
        "# Paths to test folders\n",
        "patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/test/class_2'\n",
        "no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "# Load the models (three AlexNet models)\n",
        "model_paths = [\n",
        "    \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\",\n",
        "    \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_11train.pt\",\n",
        "    \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_111train.pt\"\n",
        "]\n",
        "\n",
        "# Define layers of interest\n",
        "layers_of_interest = [5,6]\n",
        "layer_names = {\n",
        "    0: \"Conv1 (model.features[0])\",\n",
        "    1: \"Conv2 (model.features[3])\",\n",
        "    2: \"Conv3 (model.features[6])\",\n",
        "    3: \"Conv4 (model.features[8])\",\n",
        "    4: \"Conv5 (model.features[10])\",\n",
        "    5: \"FC1 (model.classifier[1])\",\n",
        "    6: \"FC2 (model.classifier[4])\"\n",
        "}\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
        "\n",
        "# Extract and project test activations averaged across models\n",
        "projected_test_activations = extract_and_project_test_activations(model_paths, patch_folder, no_patch_folder, layers_of_interest, device)\n",
        "\n",
        "# Perform correlation analysis\n",
        "perform_correlation_analysis(projected_test_activations, layers_of_interest, layer_names, colors)\n",
        "\n",
        "# Combined histogram plot\n",
        "plot_combined_histogram(layer_names, colors, projected_test_activations, layers_of_interest)\n"
      ],
      "metadata": {
        "id": "puaCbyWjH12G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Perform two-sample t-tests comparing neuron activations\n",
        "def perform_two_sample_t_tests(projected_activations):\n",
        "    t_test_results = {}\n",
        "    for layer, data in projected_activations['two_patch'].items():\n",
        "        wp_activations = np.array(projected_activations['two_patch'][layer])  # With patch\n",
        "        wo_activations = np.array(projected_activations['two_no_patch'][layer])  # Without patch\n",
        "\n",
        "        # Perform two-sample t-test for each neuron\n",
        "        ttest_result = ttest_ind(wp_activations, wo_activations, axis=0)\n",
        "\n",
        "        # Bonferroni Correction: Calculate the number of comparisons (neurons)\n",
        "        num_comparisons = wp_activations.shape[1]  # Number of neurons\n",
        "        adjusted_p_values = np.minimum(ttest_result.pvalue * num_comparisons, 1.0)  # Adjust p-values\n",
        "\n",
        "        t_test_results[layer] = {\n",
        "            'ttest': ttest_result,\n",
        "            'raw_pvalue': ttest_result.pvalue,  # Store raw p-values\n",
        "            'adjusted_pvalue': adjusted_p_values,\n",
        "            'wp_means': np.mean(wp_activations, axis=0),\n",
        "            'wo_means': np.mean(wo_activations, axis=0)\n",
        "        }\n",
        "    return t_test_results\n",
        "\n",
        "# Print t-test results with percentages of neurons below or equal to specific p-values\n",
        "def print_t_test_results(t_test_results):\n",
        "    for layer, results in t_test_results.items():\n",
        "        wp_means = results['wp_means']\n",
        "        wo_means = results['wo_means']\n",
        "        raw_pvalue = results['raw_pvalue']\n",
        "        adjusted_pvalue = results['adjusted_pvalue']\n",
        "\n",
        "        # Calculate the percentage of neurons below or equal to specific p-value thresholds\n",
        "        below_or_equal_0_05_raw = np.mean(raw_pvalue <= 0.05) * 100\n",
        "        below_or_equal_0_02_raw = np.mean(raw_pvalue <= 0.02) * 100\n",
        "        below_or_equal_0_05_adj = np.mean(adjusted_pvalue <= 0.05) * 100\n",
        "        below_or_equal_0_02_adj = np.mean(adjusted_pvalue <= 0.02) * 100\n",
        "\n",
        "        print(f\"Layer {layer}:\")\n",
        "        print(\"  WP Activations:\")\n",
        "        print(f\"    Mean: {wp_means.mean():.4f}\")\n",
        "        print(\"  WO Activations:\")\n",
        "        print(f\"    Mean: {wo_means.mean():.4f}\")\n",
        "        print(\"  T-Test (before Bonferroni correction):\")\n",
        "        print(f\"    Percentage of neurons with raw p-value <= 0.05: {below_or_equal_0_05_raw:.2f}%\")\n",
        "        print(f\"    Percentage of neurons with raw p-value <= 0.02: {below_or_equal_0_02_raw:.2f}%\")\n",
        "        print(\"  T-Test (after Bonferroni correction):\")\n",
        "        print(f\"    Percentage of neurons with adjusted p-value <= 0.05: {below_or_equal_0_05_adj:.2f}%\")\n",
        "        print(f\"    Percentage of neurons with adjusted p-value <= 0.02: {below_or_equal_0_02_adj:.2f}%\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Main Execution to perform t-tests on the projected activations\n",
        "t_test_results = perform_two_sample_t_tests(projected_test_activations)\n",
        "print_t_test_results(t_test_results)\n"
      ],
      "metadata": {
        "id": "aLqasXElH8rE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
