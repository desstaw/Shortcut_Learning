{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhSRoQg5mSiTmjw9uBv5Qb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/Shortcut_Learning/blob/main/Muting_Sparse_Neurons_SAE_dyn_left_patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1erGvTo3VIy1c3rAL9vxMkEHfyLQ0M_Qg#scrollTo=gmYi78ByHxs0"
      ],
      "metadata": {
        "id": "n5nGLO1UMhSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fDCplxxZN2kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10de19f6-0704-4c19-9084-1fca4124b8a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "srMj8i6IHBQd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import gc\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "import seaborn as sns\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom AlexNet model based from the older notebook\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, width_mult=1):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = x.view(-1, 256 * 1 * 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load AlexNet Model\n",
        "def load_model(model_path):\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model = AlexNet()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully\")\n",
        "    return model\n",
        "\n",
        "# Define Image Dataset and Preprocessing\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Preprocessing function\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LGzIGFXwN9oG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Sparse Autoencoder from older notebook\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, in_dims, h_dims):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(in_dims, h_dims), nn.ReLU())\n",
        "        self.decoder = nn.Sequential(nn.Linear(h_dims, in_dims), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "\n",
        "# Load the pre-trained autoencoder for layer 6 (fc2) (from snippet 4)\n",
        "def load_autoencoder(device):\n",
        "    save_sae_dir = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/Autoencoders/autoencoder_layer_6.pth'\n",
        "    input_dims = 4096\n",
        "    encoding_dim = 8192\n",
        "    autoencoder = SparseAutoencoder(input_dims, encoding_dim).to(device)\n",
        "    autoencoder.load_state_dict(torch.load(save_sae_dir))\n",
        "    autoencoder.eval()\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "w3BYZWteOAaG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oaO9u71__jOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from scipy.stats import ttest_ind\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up device for model computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure base directory paths are created\n",
        "base_dir = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist\"\n",
        "activation_dir = os.path.join(base_dir, \"activations\")\n",
        "output_base_dir = os.path.join(base_dir, \"outputs\")\n",
        "Path(output_base_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define paths for pre-saved activations\n",
        "def get_activation_path(folder_name, filename):\n",
        "    return os.path.join(activation_dir, folder_name, f\"{filename}.npy\")\n",
        "\n",
        "# Extract activations for fc2 (layer 6)\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting Alexnet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensor in dataloader:\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "            print(f\"Processed {len(activations)} images\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return np.vstack(activations)\n",
        "\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "    if os.path.exists(activation_path):\n",
        "        print(f\"Loading pre-saved Alexnet activations for {filename} from {activation_path}...\")\n",
        "        activations = np.load(activation_path, allow_pickle=True)\n",
        "    else:\n",
        "        print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "        activations = extract_fc2_activations(model, dataloader)\n",
        "        os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "        np.save(activation_path, activations)\n",
        "        print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "    return activations\n",
        "\n",
        "# Project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(\"Projecting Alexnet activations into SAE sparse space...\")\n",
        "    with torch.no_grad():\n",
        "        projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Function 1: Calculate neuron activations per image and overall average for patched/unpatched sets\n",
        "def calculate_neuron_activations(autoencoder, activations, folder_name, patch_status):\n",
        "    print(f\"Calculating neuron activations for {patch_status} images...\")\n",
        "    projected_activations = project_activations(autoencoder, activations, device)\n",
        "    neuron_activations = pd.DataFrame(projected_activations)\n",
        "\n",
        "    # Save individual activations per image\n",
        "    individual_activation_path = os.path.join(folder_name, f\"{patch_status}_individual_neuron_activations.csv\")\n",
        "    neuron_activations.to_csv(individual_activation_path, index=False)\n",
        "\n",
        "    # Calculate and save the average activations across all images for each neuron\n",
        "    neuron_avg = neuron_activations.mean(axis=0)\n",
        "    avg_activation_path = os.path.join(folder_name, f\"{patch_status}_average_neuron_activations.csv\")\n",
        "    neuron_avg.to_csv(avg_activation_path, header=[\"Average Activation\"], index_label=\"Neuron\")\n",
        "\n",
        "    print(f\"Saved {patch_status} individual activations to {individual_activation_path} and averages to {avg_activation_path}\")\n",
        "    return neuron_avg\n",
        "\n",
        "# Function 2: Calculate the absolute difference in average activations between patched and unpatched\n",
        "def calculate_neuron_differences(avg_activations_patch, avg_activations_no_patch, folder_name):\n",
        "    print(\"Calculating absolute difference in activations...\")\n",
        "    abs_diff = np.abs(avg_activations_patch - avg_activations_no_patch)\n",
        "    diff_path = os.path.join(folder_name, \"neuron_absolute_differences.csv\")\n",
        "    abs_diff.to_csv(diff_path, header=[\"Absolute Difference\"], index_label=\"Neuron\")\n",
        "    print(f\"Saved neuron differences to {diff_path}\")\n",
        "    return abs_diff\n",
        "\n",
        "# Function 3: Identify and save the top 10% neurons with the highest difference\n",
        "def get_top_neurons(abs_diff, folder_name, top_percentage=0.1):\n",
        "    top_neuron_count = int(len(abs_diff) * top_percentage)\n",
        "    top_neurons = abs_diff.nlargest(top_neuron_count).index\n",
        "    top_neuron_path = os.path.join(folder_name, \"top_10_percent_neurons.csv\")\n",
        "    pd.DataFrame(top_neurons, columns=[\"Neuron\"]).to_csv(top_neuron_path, index=False)\n",
        "    print(f\"Saved top 10% neurons with highest differences to {top_neuron_path}\")\n",
        "    return top_neurons\n",
        "\n",
        "# Function 4: Mute the top neurons in sparse space and classify patched images\n",
        "def classify_with_muted_neurons(autoencoder, model, activations_patch, top_neurons):\n",
        "    print(\"Muting top neurons and classifying patched images...\")\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_patch[:, top_neurons] = 0  # Mute selected neurons\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Pass through AlexNet softmax for classification\n",
        "    predictions = []\n",
        "    for activation in decoded_patch:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Function 5: Classify unmuted sparse activations for non-patched images\n",
        "def classify_without_muting(autoencoder, model, activations_no_patch):\n",
        "    print(\"Classifying non-patched images without muting neurons...\")\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "    decoded_no_patch = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Classify with AlexNet softmax\n",
        "    predictions = []\n",
        "    for activation in decoded_no_patch:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Function 6: Save decoded neuron activations per image\n",
        "def save_decoded_activations(decoded_activations, patch_status, folder_name):\n",
        "    decoded_path = os.path.join(folder_name, f\"decoded_activations_{patch_status}.csv\")\n",
        "    pd.DataFrame(decoded_activations).to_csv(decoded_path, index=False)\n",
        "    print(f\"Saved decoded neuron activations for {patch_status} images to {decoded_path}\")\n",
        "\n",
        "# Function 7: Save average neuron activations across all images\n",
        "def save_average_activations(decoded_activations, patch_status, folder_name):\n",
        "    avg_activations = np.mean(decoded_activations, axis=0)\n",
        "    avg_path = os.path.join(folder_name, f\"average_decoded_activations_{patch_status}.csv\")\n",
        "    pd.DataFrame(avg_activations, columns=[\"Average Activation\"]).to_csv(avg_path, index_label=\"Neuron\")\n",
        "    print(f\"Saved average decoded activations for {patch_status} images to {avg_path}\")\n",
        "\n",
        "# Function 8: Evaluate the effect of muting neurons\n",
        "def evaluate_muting_effect(predictions_with_muting, predictions_without_muting):\n",
        "    agreement_count = sum(pw == pn for pw, pn in zip(predictions_with_muting, predictions_without_muting))\n",
        "    accuracy = agreement_count / len(predictions_with_muting) * 100\n",
        "    print(f\"Accuracy of classifications with muted neurons matching non-muted classifications: {accuracy:.2f}%\")\n",
        "    # print(\"Percentage change in classification accuracy after muting\")\n",
        "\n",
        "# Function 9: Calculate and display classification metrics\n",
        "def evaluate_classification_metrics(predictions_with_muting, predictions_without_muting, labels):\n",
        "    target_class = 1\n",
        "    labels_target = [1 if label == target_class else 0 for label in labels]\n",
        "    preds_with_muting_target = [1 if pred == target_class else 0 for pred in predictions_with_muting]\n",
        "    preds_without_muting_target = [1 if pred == target_class else 0 for pred in predictions_without_muting]\n",
        "\n",
        "    accuracy_with_muting = accuracy_score(labels_target, preds_with_muting_target)\n",
        "    precision_with_muting = precision_score(labels_target, preds_with_muting_target)\n",
        "    recall_with_muting = recall_score(labels_target, preds_with_muting_target)\n",
        "\n",
        "    accuracy_without_muting = accuracy_score(labels_target, preds_without_muting_target)\n",
        "    precision_without_muting = precision_score(labels_target, preds_without_muting_target)\n",
        "    recall_without_muting = recall_score(labels_target, preds_without_muting_target)\n",
        "\n",
        "    print(\"Metrics for 'two with patch' class with muting:\")\n",
        "    print(f\"  Accuracy: {accuracy_with_muting:.2f}\")\n",
        "    print(\"\\nMetrics for 'two with patch' class without muting:\")\n",
        "    print(f\"  Accuracy: {accuracy_without_muting:.2f}\")\n",
        "\n",
        "# Function 10: Visualize differences for top neurons with binning\n",
        "def visualize_binned_neuron_differences(abs_diff, top_neurons, bin_width=0.05):\n",
        "    # Get the differences for the top neurons and sort them\n",
        "    top_neuron_diffs = abs_diff.loc[top_neurons].sort_values(ascending=False)\n",
        "\n",
        "    # Bin the difference values\n",
        "    max_diff = top_neuron_diffs.max()\n",
        "    bins = np.arange(0, max_diff + bin_width, bin_width)\n",
        "    binned_counts = pd.cut(top_neuron_diffs, bins=bins).value_counts(sort=False)\n",
        "\n",
        "    # Plot the binned counts\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    binned_counts.plot(kind='bar', color='skyblue')\n",
        "    plt.xlabel(\"Difference Value Bins\")\n",
        "    plt.ylabel(\"Neuron Count\")\n",
        "    plt.title(\"Neuron Count in Each Difference Value Bin\")\n",
        "\n",
        "    # Rotate x-axis labels for readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Display grid for easier comparison\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function 11: Conduct t-tests in sparse and decoded space\n",
        "def t_test_sparse_vs_non_sparse(sparse_with_patch, sparse_no_patch):\n",
        "    t_stat_sparse, p_value_sparse = ttest_ind(sparse_with_patch, sparse_no_patch, equal_var=False)\n",
        "    print(f\"Sparse Activations T-Test:\\n  T-statistic: {t_stat_sparse}, P-value: {p_value_sparse}\")\n",
        "    return t_stat_sparse, p_value_sparse\n",
        "\n",
        "def t_test_decoded_muted_vs_non_muted(decoded_with_patch_muted, decoded_with_patch_non_muted):\n",
        "    t_stat_decoded, p_value_decoded = ttest_ind(decoded_with_patch_muted, decoded_with_patch_non_muted, equal_var=False)\n",
        "    print(f\"Decoded Activations T-Test:\\n  T-statistic: {t_stat_decoded}, P-value: {p_value_decoded}\")\n",
        "    return t_stat_decoded, p_value_decoded\n",
        "\n",
        "# Function 12 to classify decoded activations\n",
        "def classify_decoded_activations(model, decoded_activations):\n",
        "    \"\"\"Classify decoded activations using the softmax layer of the model.\"\"\"\n",
        "    predictions = []\n",
        "    for activation in decoded_activations:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Function 13: Perform and print formatted t-tests for layer activations\n",
        "def perform_and_format_t_tests(condition1_activations, condition2_activations, layer_name=\"Layer\"):\n",
        "    # Perform two-sample t-test\n",
        "    t_stat, p_values = ttest_ind(condition1_activations, condition2_activations, axis=0, equal_var=False)\n",
        "\n",
        "    # Bonferroni correction\n",
        "    num_neurons = condition1_activations.shape[1]\n",
        "    adjusted_p_values = np.minimum(p_values * num_neurons, 1.0)\n",
        "\n",
        "    # Calculate mean activations for each neuron\n",
        "    condition1_mean = np.mean(condition1_activations, axis=0)\n",
        "    condition2_mean = np.mean(condition2_activations, axis=0)\n",
        "\n",
        "    # Calculate and display the percentage of neurons with significant p-values\n",
        "    raw_significant_0_05 = np.mean(p_values <= 0.05) * 100\n",
        "    raw_significant_0_02 = np.mean(p_values <= 0.02) * 100\n",
        "    corrected_significant_0_05 = np.mean(adjusted_p_values <= 0.05) * 100\n",
        "    corrected_significant_0_02 = np.mean(adjusted_p_values <= 0.02) * 100\n",
        "\n",
        "    print(f\"{layer_name}:\")\n",
        "    print(\"  Condition 1 Mean Activation (Muted):\")\n",
        "    print(f\"    Mean across neurons: {condition1_mean.mean():.4f}\")\n",
        "    print(\"  Condition 2 Mean Activation (Non-Muted):\")\n",
        "    print(f\"    Mean across neurons: {condition2_mean.mean():.4f}\")\n",
        "    print(\"  T-Test (before Bonferroni correction):\")\n",
        "    print(f\"    Percentage of neurons with raw p-value <= 0.05: {raw_significant_0_05:.2f}%\")\n",
        "    print(f\"    Percentage of neurons with raw p-value <= 0.02: {raw_significant_0_02:.2f}%\")\n",
        "    print(\"  T-Test (after Bonferroni correction):\")\n",
        "    print(f\"    Percentage of neurons with adjusted p-value <= 0.05: {corrected_significant_0_05:.2f}%\")\n",
        "    print(f\"    Percentage of neurons with adjusted p-value <= 0.02: {corrected_significant_0_02:.2f}%\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "zQQ5POZ2_xzI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Paths and initialization\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\"\n",
        "    patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/test/class_2'\n",
        "    no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "    # Load model and autoencoder\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Prepare dataloaders\n",
        "    patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    no_patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(no_patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    patch_dataset = ImageDataset(patch_image_paths, transform=preprocess)\n",
        "    no_patch_dataset = ImageDataset(no_patch_image_paths, transform=preprocess)\n",
        "\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "    no_patch_loader = DataLoader(no_patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Load or extract fc2 activations\n",
        "    activations_patch = load_or_extract_fc2_activations(model, patch_loader, 'test_patch', 'fc2_activations_patch')\n",
        "    activations_no_patch = load_or_extract_fc2_activations(model, no_patch_loader, 'test_no_patch', 'fc2_activations_no_patch')\n",
        "\n",
        "    # Directory for saving results\n",
        "    sparse_output_dir = os.path.join(output_base_dir, \"fc2_sparse_outputs\")\n",
        "    Path(sparse_output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Calculate activations and save outputs\n",
        "    avg_activations_patch = calculate_neuron_activations(autoencoder, activations_patch, sparse_output_dir, \"patch\")\n",
        "    avg_activations_no_patch = calculate_neuron_activations(autoencoder, activations_no_patch, sparse_output_dir, \"no_patch\")\n",
        "\n",
        "    # Calculate differences and get top neurons\n",
        "    abs_diff = calculate_neuron_differences(avg_activations_patch, avg_activations_no_patch, sparse_output_dir)\n",
        "    top_neurons = get_top_neurons(abs_diff, sparse_output_dir)\n",
        "\n",
        "    # Visualize top neuron differences\n",
        "    visualize_binned_neuron_differences(abs_diff, top_neurons, bin_width=0.05)\n",
        "\n",
        "\n",
        "    # Classify with and without muting, then evaluate\n",
        "    predictions_with_muting = classify_with_muted_neurons(autoencoder, model, activations_patch, top_neurons)\n",
        "    predictions_without_muting = classify_without_muting(autoencoder, model, activations_no_patch)\n",
        "    evaluate_muting_effect(predictions_with_muting, predictions_without_muting)\n",
        "\n",
        "    # Evaluate classification metrics\n",
        "    labels = [1] * len(predictions_with_muting)  # All images are class 2\n",
        "    evaluate_classification_metrics(predictions_with_muting, predictions_without_muting, labels)\n",
        "\n",
        "    # Project into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    # Ensure we have decoded versions if needed for t-tests on decoded activations\n",
        "    decoded_with_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_with_patch_non_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Perform t-tests\n",
        "    #t_stat_sparse, p_value_sparse = t_test_sparse_vs_non_sparse(projected_patch, projected_no_patch)\n",
        "    #t_stat_decoded, p_value_decoded = t_test_decoded_muted_vs_non_muted(decoded_with_patch_muted, decoded_with_patch_non_muted)\n",
        "\n",
        "\n",
        "    # Perform t-tests\n",
        "    #t_stat_sparse, p_value_sparse = t_test_sparse_vs_non_sparse(projected_patch, projected_no_patch)\n",
        "    #t_stat_decoded, p_value_decoded = t_test_decoded_muted_vs_non_muted(decoded_with_patch_muted, decoded_with_patch_non_muted)\n",
        "\n",
        "    # Decode both conditions for comparisons in decoded space\n",
        "    decoded_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_patch_non_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Perform t-tests on sparse activations\n",
        "    print(\"Performing T-Tests on Sparse Activations:\")\n",
        "    perform_and_format_t_tests(projected_patch, projected_no_patch, layer_name=\"Sparse Activations\")\n",
        "\n",
        "    # Perform t-tests on decoded activations\n",
        "    print(\"Performing T-Tests on Decoded Activations (Muted vs Non-Muted):\")\n",
        "    perform_and_format_t_tests(decoded_with_patch_muted, decoded_with_patch_non_muted, layer_name=\"Decoded Activations\")\n",
        "\n",
        "    # Classify decoded activations for images with no patch in sparse space\n",
        "    print(\"\\nClassifying decoded activations for 'two with no patch' after projecting into sparse space and decoding...\")\n",
        "    predictions_no_patch_decoded = classify_decoded_activations(model, decoded_patch_non_muted)\n",
        "\n",
        "    # Calculate and print accuracy for 'two with no patch' decoded activations\n",
        "    labels_no_patch = [1] * len(predictions_no_patch_decoded)  # All images are labeled as class 2\n",
        "    accuracy_no_patch_decoded = accuracy_score(labels_no_patch, predictions_no_patch_decoded)\n",
        "    print(f\"Accuracy of 'two with no patch' decoded activations after sparse projection: {accuracy_no_patch_decoded:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ldNEN2kn_8UZ",
        "outputId": "7ff3b89f-9d08-45b0-8bb7-04455146fcda"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-f02631ea6dac>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-ad49f019404b>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(save_sae_dir))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-saved Alexnet activations for fc2_activations_patch from /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/fc2_activations_patch.npy...\n",
            "Loading pre-saved Alexnet activations for fc2_activations_no_patch from /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_no_patch/fc2_activations_no_patch.npy...\n",
            "Calculating neuron activations for patch images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Saved patch individual activations to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/patch_individual_neuron_activations.csv and averages to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/patch_average_neuron_activations.csv\n",
            "Calculating neuron activations for no_patch images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Saved no_patch individual activations to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/no_patch_individual_neuron_activations.csv and averages to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/no_patch_average_neuron_activations.csv\n",
            "Calculating absolute difference in activations...\n",
            "Saved neuron differences to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/neuron_absolute_differences.csv\n",
            "Saved top 10% neurons with highest differences to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/top_10_percent_neurons.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muting top neurons and classifying patched images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Classifying non-patched images without muting neurons...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Accuracy of classifications with muted neurons matching non-muted classifications: 27.91%\n",
            "Metrics for 'two with patch' class with muting:\n",
            "  Accuracy: 0.19\n",
            "\n",
            "Metrics for 'two with patch' class without muting:\n",
            "  Accuracy: 0.85\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Performing T-Tests on Sparse Activations:\n",
            "Sparse Activations:\n",
            "  Condition 1 Mean Activation (Muted):\n",
            "    Mean across neurons: 0.6564\n",
            "  Condition 2 Mean Activation (Non-Muted):\n",
            "    Mean across neurons: 0.6785\n",
            "  T-Test (before Bonferroni correction):\n",
            "    Percentage of neurons with raw p-value <= 0.05: 78.88%\n",
            "    Percentage of neurons with raw p-value <= 0.02: 75.26%\n",
            "  T-Test (after Bonferroni correction):\n",
            "    Percentage of neurons with adjusted p-value <= 0.05: 49.22%\n",
            "    Percentage of neurons with adjusted p-value <= 0.02: 48.11%\n",
            "--------------------------------------------------\n",
            "Performing T-Tests on Decoded Activations (Muted vs Non-Muted):\n",
            "Decoded Activations:\n",
            "  Condition 1 Mean Activation (Muted):\n",
            "    Mean across neurons: 0.7909\n",
            "  Condition 2 Mean Activation (Non-Muted):\n",
            "    Mean across neurons: 0.8176\n",
            "  T-Test (before Bonferroni correction):\n",
            "    Percentage of neurons with raw p-value <= 0.05: 43.29%\n",
            "    Percentage of neurons with raw p-value <= 0.02: 42.33%\n",
            "  T-Test (after Bonferroni correction):\n",
            "    Percentage of neurons with adjusted p-value <= 0.05: 35.89%\n",
            "    Percentage of neurons with adjusted p-value <= 0.02: 35.16%\n",
            "--------------------------------------------------\n",
            "\n",
            "Classifying decoded activations for 'two with no patch' after projecting into sparse space and decoding...\n",
            "Accuracy of 'two with no patch' decoded activations after sparse projection: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Something to think about when using laster a larget sparse space...now I only have 8k neurons in the sparse space:**\n",
        "\n",
        "In the sparse space, not all neurons are consistently activated across all images. For example, a neuron might remain inactive (close to zero) in most images, but activate strongly for a few specific images, such as those containing spurious features like patches. When we take the average activation of that neuron across all images, the low values from the inactive images will dominate, resulting in a low overall average. This averaging process can therefore obscure the true impact of that neuron in encoding the patch feature, leading to a misleadingly low indication of its importance. The concern is that by using the average activation values in this way, we might be overlooking neurons that are actually sensitive to the spurious features but appear unimportant due to their sparsity. This could affect the accuracy of our results, particularly in identifying which neurons are encoding spurious features."
      ],
      "metadata": {
        "id": "6x8_C4fALkVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stophere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ho5W3seN_haQ",
        "outputId": "beab9d48-42db-4570-fd43-b4f32a6b2810"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stophere' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-88d56cf18612>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstophere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stophere' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archives"
      ],
      "metadata": {
        "id": "Z1Cxtd4T_fIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to Google Drive locations for pre-saved activations\n",
        "def get_activation_path(folder_name, filename):\n",
        "    return f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/{folder_name}/{filename}.npy'\n",
        "\n",
        "\n",
        "# Extract activations for fc2 (layer 6)\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting Alexnet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensor in dataloader:\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "            print(f\"Processed {len(activations)} images\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return np.vstack(activations)\n",
        "\n",
        "\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "\n",
        "    # Check if the activation file already exists\n",
        "    if os.path.exists(activation_path):\n",
        "        print(f\"Loading pre-saved Alexnet activations for {filename} from {activation_path}...\")\n",
        "        activations = np.load(activation_path, allow_pickle=True)\n",
        "    else:\n",
        "        print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "        activations = extract_fc2_activations(model, dataloader)\n",
        "        os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "        np.save(activation_path, activations)\n",
        "        print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "\n",
        "    return activations\n",
        "\n",
        "\n",
        "# Extract and save activations for fc2 (layer 6) to Google Drive\n",
        "def extract_and_save_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    print(\"Extracting and saving Alexnet activations for layer fc2...\")\n",
        "    # Extract activations using the existing function\n",
        "    activations = extract_fc2_activations(model, dataloader)\n",
        "\n",
        "    # Define the save path in Google Drive\n",
        "    drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/{folder_name}/{filename}.npy'\n",
        "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "\n",
        "    # Save the activations as a .npy file\n",
        "    np.save(drive_path, activations)\n",
        "    print(f\"Alexnet Activations for layer fc2 saved to {drive_path}\")\n",
        "\n",
        "# Project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(\"Projecting Alexnet activations into SAE sparse space...\")\n",
        "    with torch.no_grad():\n",
        "        projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Mean Activation Difference\n",
        "def mean_activation_difference(projected_patch, projected_no_patch, top_k=10):\n",
        "    print(\"Calculating mean sparse activations difference...\")\n",
        "    mean_diff = np.abs(projected_patch.mean(axis=0) - projected_no_patch.mean(axis=0))\n",
        "    top_neurons = np.argsort(mean_diff)[-top_k:]\n",
        "    return top_neurons\n",
        "\n",
        "# Statistical Significance Testing\n",
        "def statistical_testing_neurons(projected_patch, projected_no_patch, threshold=0.05):\n",
        "    print(\"Performing statistical testing...\")\n",
        "    significant_neurons = []\n",
        "    for i in range(projected_patch.shape[1]):\n",
        "        _, p_value = ttest_ind(projected_patch[:, i], projected_no_patch[:, i], equal_var=False)\n",
        "        if p_value < threshold:\n",
        "            significant_neurons.append(i)\n",
        "    return significant_neurons\n",
        "\n",
        "# Use AlexNet's own FC weights to identify patch-relevant neurons\n",
        "def patch_classifier_importance(model, projected_patch, projected_no_patch, top_k=10):\n",
        "    print(\"Using AlexNet's FC layer weights to identify important neurons...\")\n",
        "    # Extract weights from the final fully connected layer (fc3 as output)\n",
        "    importance = np.abs(model.fc3.weight.cpu().detach().numpy()[0])  # Take absolute values of weights\n",
        "\n",
        "    # Sort by importance and get the top K neurons\n",
        "    top_neurons = np.argsort(importance)[-top_k:]\n",
        "    print(\"Top neurons identified based on AlexNet's weights:\", top_neurons)\n",
        "    return top_neurons\n",
        "\n",
        "\n",
        "# Correlation Analysis\n",
        "def correlation_analysis(projected_patch, projected_no_patch, top_k=10):\n",
        "    combined = np.vstack([projected_patch, projected_no_patch])\n",
        "    patch_condition = np.hstack([np.ones(len(projected_patch)), np.zeros(len(projected_no_patch))])\n",
        "    correlations = [pearsonr(combined[:, i], patch_condition)[0] for i in range(combined.shape[1])]\n",
        "    top_neurons = np.argsort(np.abs(correlations))[-top_k:]\n",
        "    return top_neurons\n",
        "\n",
        "# Visualize Top Neurons\n",
        "def visualize_neurons(neuron_indexes_dict):\n",
        "    # Convert neuron indexes to a DataFrame, filling missing values with NaN\n",
        "    neuron_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in neuron_indexes_dict.items()]))\n",
        "\n",
        "    # Plot each method's top neuron indexes using a heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(neuron_df, annot=True, fmt=\".0f\", cmap=\"viridis\",  # Use .0f to handle NaN values\n",
        "                cbar=True, yticklabels=False)\n",
        "\n",
        "    plt.xlabel(\"Methods\")\n",
        "    plt.ylabel(\"Top Neurons\")\n",
        "    plt.title(\"Comparison of Silenced Neurons Across Methods\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def silence_and_classify(autoencoder, model, activations_patch, selected_neurons):\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    silenced_patch = np.copy(projected_patch)\n",
        "    silenced_patch[:, selected_neurons] = 0\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(silenced_patch).to(device).float()).detach().cpu().numpy()\n",
        "\n",
        "    # Decode and classify with AlexNet softmax\n",
        "    predictions = []\n",
        "    for decoded_activation in decoded_patch:\n",
        "        decoded_tensor = torch.from_numpy(decoded_activation).float().to(device)\n",
        "        output = model.fc3(decoded_tensor)  # After fc2, apply fc3 for classification\n",
        "        prediction = torch.argmax(F.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Helper function to calculate accuracy\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = sum([1 if pred == label else 0 for pred, label in zip(predictions, labels)])\n",
        "    return correct / len(labels) * 100  # Returns accuracy percentage\n",
        "\n",
        "# Check overlap in silenced neurons across methods\n",
        "def check_neuron_overlap(silenced_neurons_dict):\n",
        "    methods = list(silenced_neurons_dict.keys())\n",
        "    overlap_counts = {}\n",
        "\n",
        "    for i, method1 in enumerate(methods):\n",
        "        for method2 in methods[i + 1:]:\n",
        "            overlap = set(silenced_neurons_dict[method1]).intersection(silenced_neurons_dict[method2])\n",
        "            overlap_counts[f\"{method1} & {method2}\"] = len(overlap)\n",
        "\n",
        "    print(\"Neuron Overlap Across Methods:\")\n",
        "    for pair, count in overlap_counts.items():\n",
        "        print(f\"{pair}: {count} neurons\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ij4kzdO_OC-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory path for saving outputs\n",
        "output_dir = Path(\"/content/drive/MyDrive/Masterthesis/Datasets/mnist/muted_sparse_sae/outputs\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main function for all methods\n",
        "def main():\n",
        "    # Paths and initialization\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\"\n",
        "    patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/test/class_2'\n",
        "    no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "    # Load model and autoencoder\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Prepare dataloaders for patched and unpatched datasets\n",
        "    patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    no_patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(no_patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    patch_dataset = ImageDataset(patch_image_paths, transform=preprocess)\n",
        "    no_patch_dataset = ImageDataset(no_patch_image_paths, transform=preprocess)\n",
        "\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "    no_patch_loader = DataLoader(no_patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Conditionally load or extract activations for patched images\n",
        "    activations_patch = load_or_extract_fc2_activations(model, patch_loader, 'test_patch', 'fc2_activations_patch')\n",
        "\n",
        "    # Conditionally load or extract activations for non-patched images\n",
        "    activations_no_patch = load_or_extract_fc2_activations(model, no_patch_loader, 'test_no_patch', 'fc2_activations_no_patch')\n",
        "\n",
        "    # Project into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    # Dictionary to store neuron indexes for each method\n",
        "    silenced_neurons_dict = {}\n",
        "\n",
        "    # Method 1: Mean Activation Difference\n",
        "    top_neurons_mean_diff = mean_activation_difference(projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Mean Activation Diff\"] = top_neurons_mean_diff\n",
        "    predictions_mean_diff = silence_and_classify(autoencoder, model, activations_patch, top_neurons_mean_diff)\n",
        "    print(\"Classification Results (Mean Activation Diff):\")\n",
        "    print(predictions_mean_diff)\n",
        "\n",
        "    # Method 2: Statistical Testing\n",
        "    top_neurons_stat_test = statistical_testing_neurons(projected_patch, projected_no_patch, threshold=0.05)\n",
        "    silenced_neurons_dict[\"Statistical Test\"] = top_neurons_stat_test\n",
        "    predictions_stat_test = silence_and_classify(autoencoder, model, activations_patch, top_neurons_stat_test)\n",
        "    print(\"Classification Results (Statistical Test):\")\n",
        "    print(predictions_stat_test)\n",
        "\n",
        "    # Method 3: Patch Classifier (Using AlexNet's FC Layer Weights)\n",
        "    top_neurons_classifier = patch_classifier_importance(model, projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Patch Classifier\"] = top_neurons_classifier\n",
        "    predictions_classifier = silence_and_classify(autoencoder, model, activations_patch, top_neurons_classifier)\n",
        "    print(\"Classification Results (Patch Classifier):\")\n",
        "    print(predictions_classifier)\n",
        "\n",
        "    # Method 4: Correlation Analysis\n",
        "    top_neurons_correlation = correlation_analysis(projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Correlation Analysis\"] = top_neurons_correlation\n",
        "    predictions_correlation = silence_and_classify(autoencoder, model, activations_patch, top_neurons_correlation)\n",
        "    print(\"Classification Results (Correlation Analysis):\")\n",
        "    print(predictions_correlation)\n",
        "\n",
        "    # Print silenced neurons by each method\n",
        "    print(\"Silenced neurons by each method:\", silenced_neurons_dict)\n",
        "\n",
        "    # Check neuron overlap across methods\n",
        "    check_neuron_overlap(silenced_neurons_dict)\n",
        "\n",
        "    # Visualize silenced neuron indexes\n",
        "    visualize_neurons(silenced_neurons_dict)\n",
        "\n",
        "# Execute main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "qPEr9jJwOEkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}