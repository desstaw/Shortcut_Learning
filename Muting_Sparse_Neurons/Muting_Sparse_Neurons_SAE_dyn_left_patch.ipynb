{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP14vTrNBTMrNtYaiyxdsgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/Shortcut_Learning/blob/main/Muting_Sparse_Neurons_SAE_dyn_left_patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1erGvTo3VIy1c3rAL9vxMkEHfyLQ0M_Qg#scrollTo=gmYi78ByHxs0"
      ],
      "metadata": {
        "id": "n5nGLO1UMhSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fDCplxxZN2kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597c210a-f9a2-42a7-dbad-0990482a6ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srMj8i6IHBQd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import gc\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "import seaborn as sns\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed=1):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load pretrained SAE & Alexnet"
      ],
      "metadata": {
        "id": "FAI_YFZRRU3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class LabeledDataLoaderWrapper:\n",
        "    def __init__(self, dataloader, label):\n",
        "        self.dataloader = dataloader\n",
        "        self.label = label\n",
        "\n",
        "    def __iter__(self):\n",
        "        for images in self.dataloader:\n",
        "            batch_size = images.size(0)\n",
        "            labels = torch.full((batch_size,), self.label, dtype=torch.long)\n",
        "            yield images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "K1tNdLnKlD5v",
        "outputId": "4d25a55c-02ec-470c-c39c-b5a835b26039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass LabeledDataLoaderWrapper:\\n    def __init__(self, dataloader, label):\\n        self.dataloader = dataloader\\n        self.label = label\\n\\n    def __iter__(self):\\n        for images in self.dataloader:\\n            batch_size = images.size(0)\\n            labels = torch.full((batch_size,), self.label, dtype=torch.long)\\n            yield images, labels\\n\\n    def __len__(self):\\n        return len(self.dataloader)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom AlexNet model based from the older notebook\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, width_mult=1):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = x.view(-1, 256 * 1 * 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load AlexNet Model\n",
        "def load_model(model_path):\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model = AlexNet()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze all layers up to (and including) fc2\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"fc3\" not in name:  # Freeze all layers except fc3\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    print(\"Model loaded and layers up to fc2 are frozen\")\n",
        "    return model\n",
        "\n",
        "'''\n",
        "# Define Image Dataset and Preprocessing original\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Preprocessing function\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "'''\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, path: str, is_two: int):\n",
        "        self.resize_shape = (64, 64)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.path = path\n",
        "        self.data_files = os.listdir(self.path)\n",
        "        self.labels = [is_two] * len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_path = os.path.join(self.path, self.data_files[i])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[i]\n",
        "        return img, label, self.data_files[i]  # Return the filename as a string\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, file_paths: list, is_two: int):\n",
        "        self.resize_shape = (64, 64)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.data_files = file_paths  # Accept a list of file paths\n",
        "        self.labels = [is_two] * len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_path = self.data_files[i]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[i]\n",
        "        return img, label, os.path.basename(img_path)  # Return the filename as well\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n"
      ],
      "metadata": {
        "id": "LGzIGFXwN9oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Sparse Autoencoder from older notebook\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, in_dims, h_dims):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(in_dims, h_dims), nn.ReLU())\n",
        "        self.decoder = nn.Sequential(nn.Linear(h_dims, in_dims), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "\n",
        "# Load the pre-trained autoencoder for layer 6 (fc2) (from snippet 4)\n",
        "def load_autoencoder(device):\n",
        "    save_sae_dir = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/dynamic_left_patch/Autoencoders/autoencoder_layer_6.pth'\n",
        "    input_dims = 4096\n",
        "    encoding_dim = 8192\n",
        "\n",
        "    # Initialize the autoencoder\n",
        "    autoencoder = SparseAutoencoder(input_dims, encoding_dim).to(device)\n",
        "    autoencoder.load_state_dict(torch.load(save_sae_dir))\n",
        "\n",
        "    # Freeze all parameters of the autoencoder\n",
        "    for param in autoencoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Set the autoencoder to evaluation mode\n",
        "    autoencoder.eval()\n",
        "    print(\"Autoencoder loaded and frozen successfully\")\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "w3BYZWteOAaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main Pipeline\n",
        "Load saved SAE and activations. Project fc2 activations into sparse space then decode one with muting in sparse space and once without muting to the worst group: two_with_patch"
      ],
      "metadata": {
        "id": "oaO9u71__jOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from scipy.stats import ttest_ind\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up device for model computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure base directory paths are created\n",
        "base_dir = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist\"\n",
        "activation_dir = os.path.join(base_dir, \"activations\")\n",
        "output_base_dir = os.path.join(base_dir, \"outputs\")\n",
        "Path(output_base_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define paths for pre-saved activations\n",
        "def get_activation_path(folder_name, filename):\n",
        "    return os.path.join(activation_dir, folder_name, f\"{filename}.npy\")\n",
        "'''\n",
        "# Extract activations for fc2 (layer 6)\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting Alexnet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensor in dataloader:\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "            print(f\"Processed {len(activations)} images\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return np.vstack(activations)\n",
        "'''\n",
        "\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting AlexNet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            # Unpack the tuple returned by the DataLoader\n",
        "            image_tensor, label, filename = data\n",
        "\n",
        "            # Move the image tensor to the device\n",
        "            image_tensor = image_tensor.to(device)\n",
        "\n",
        "            # Extract activations up to the fc2 layer\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "\n",
        "            # Convert the tensor to numpy and append to the list\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "\n",
        "            # Clear CUDA cache if necessary\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    # Convert the list of numpy arrays to a single numpy array\n",
        "    return np.vstack(activations)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "    if os.path.exists(activation_path):\n",
        "        print(f\"Loading pre-saved Alexnet activations for {filename} from {activation_path}...\")\n",
        "        activations = np.load(activation_path, allow_pickle=True)\n",
        "    else:\n",
        "        print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "        activations = extract_fc2_activations(model, dataloader)\n",
        "        os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "        np.save(activation_path, activations)\n",
        "        print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "    return activations\n",
        "\n",
        "# Project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(\"Projecting Alexnet activations into SAE sparse space...\")\n",
        "    with torch.no_grad():\n",
        "        projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Function 1: Calculate neuron activations per image and overall average for patched/unpatched sets\n",
        "def calculate_neuron_activations(autoencoder, activations, folder_name, patch_status):\n",
        "    print(f\"Calculating neuron activations for {patch_status} images...\")\n",
        "    projected_activations = project_activations(autoencoder, activations, device)\n",
        "    neuron_activations = pd.DataFrame(projected_activations)\n",
        "\n",
        "    # Save individual activations per image\n",
        "    individual_activation_path = os.path.join(folder_name, f\"{patch_status}_individual_neuron_activations.csv\")\n",
        "    neuron_activations.to_csv(individual_activation_path, index=False)\n",
        "\n",
        "    # Calculate and save the average activations across all images for each neuron\n",
        "    neuron_avg = neuron_activations.mean(axis=0)\n",
        "    avg_activation_path = os.path.join(folder_name, f\"{patch_status}_average_neuron_activations.csv\")\n",
        "    neuron_avg.to_csv(avg_activation_path, header=[\"Average Activation\"], index_label=\"Neuron\")\n",
        "\n",
        "    print(f\"Saved {patch_status} individual activations to {individual_activation_path} and averages to {avg_activation_path}\")\n",
        "    return neuron_avg\n",
        "\n",
        "# Function 2: Calculate the absolute difference in average activations between patched and unpatched\n",
        "def calculate_neuron_differences(avg_activations_patch, avg_activations_no_patch, folder_name):\n",
        "    print(\"Calculating absolute difference in activations...\")\n",
        "    abs_diff = np.abs(avg_activations_patch - avg_activations_no_patch)\n",
        "    diff_path = os.path.join(folder_name, \"neuron_absolute_differences.csv\")\n",
        "    abs_diff.to_csv(diff_path, header=[\"Absolute Difference\"], index_label=\"Neuron\")\n",
        "    print(f\"Saved neuron differences to {diff_path}\")\n",
        "    return abs_diff\n",
        "\n",
        "# Function 3: Identify and save the top 10% neurons with the highest difference\n",
        "def get_top_neurons(abs_diff, folder_name, top_percentage=0.1):\n",
        "    top_neuron_count = int(len(abs_diff) * top_percentage)\n",
        "    top_neurons = abs_diff.nlargest(top_neuron_count).index\n",
        "    top_neuron_path = os.path.join(folder_name, \"top_10_percent_neurons.csv\")\n",
        "    pd.DataFrame(top_neurons, columns=[\"Neuron\"]).to_csv(top_neuron_path, index=False)\n",
        "    print(f\"Saved top 10% neurons with highest differences to {top_neuron_path}\")\n",
        "    return top_neurons\n",
        "\n",
        "# Function 4: Mute the top neurons in sparse space and classify patched images\n",
        "\n",
        "def classify_with_muted_neurons(autoencoder, model, activations_patch, top_neurons):\n",
        "    print(\"Muting top neurons and classifying patched images...\")\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    print(\"Applying muting in sparse space...\")\n",
        "    projected_patch[:, top_neurons] = 0  # Mute selected neurons\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Pass through AlexNet softmax for classification\n",
        "    predictions = []\n",
        "    for activation in decoded_patch:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Function 5: Classify unmuted sparse activations for non-patched images\n",
        "def classify_without_muting(autoencoder, model, activations_no_patch):\n",
        "    print(\"Classifying non-patched images without muting neurons...\")\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "    decoded_no_patch = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Classify with AlexNet softmax\n",
        "    predictions = []\n",
        "    for activation in decoded_no_patch:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Function 6: Save decoded neuron activations per image\n",
        "def save_decoded_activations(decoded_activations, patch_status, folder_name):\n",
        "    decoded_path = os.path.join(folder_name, f\"decoded_activations_{patch_status}.csv\")\n",
        "    pd.DataFrame(decoded_activations).to_csv(decoded_path, index=False)\n",
        "    print(f\"Saved decoded neuron activations for {patch_status} images to {decoded_path}\")\n",
        "\n",
        "# Function 7: Save average neuron activations across all images\n",
        "def save_average_activations(decoded_activations, patch_status, folder_name):\n",
        "    avg_activations = np.mean(decoded_activations, axis=0)\n",
        "    avg_path = os.path.join(folder_name, f\"average_decoded_activations_{patch_status}.csv\")\n",
        "    pd.DataFrame(avg_activations, columns=[\"Average Activation\"]).to_csv(avg_path, index_label=\"Neuron\")\n",
        "    print(f\"Saved average decoded activations for {patch_status} images to {avg_path}\")\n",
        "\n",
        "# Function 8: Evaluate the effect of muting neurons\n",
        "def evaluate_muting_effect(predictions_with_muting, predictions_without_muting):\n",
        "    agreement_count = sum(pw == pn for pw, pn in zip(predictions_with_muting, predictions_without_muting))\n",
        "    accuracy = agreement_count / len(predictions_with_muting) * 100\n",
        "    print(f\"Accuracy of classifications with muted neurons matching non-muted classifications: {accuracy:.2f}%\")\n",
        "    # print(\"Percentage change in classification accuracy after muting\")\n",
        "\n",
        "# Function 9: Calculate and display classification metrics\n",
        "def evaluate_classification_metrics(predictions_with_muting, predictions_without_muting, labels):\n",
        "    target_class = 1\n",
        "    labels_target = [1 if label == target_class else 0 for label in labels]\n",
        "    preds_with_muting_target = [1 if pred == target_class else 0 for pred in predictions_with_muting]\n",
        "    preds_without_muting_target = [1 if pred == target_class else 0 for pred in predictions_without_muting]\n",
        "\n",
        "    accuracy_with_muting = accuracy_score(labels_target, preds_with_muting_target)\n",
        "    precision_with_muting = precision_score(labels_target, preds_with_muting_target)\n",
        "    recall_with_muting = recall_score(labels_target, preds_with_muting_target)\n",
        "\n",
        "    accuracy_without_muting = accuracy_score(labels_target, preds_without_muting_target)\n",
        "    precision_without_muting = precision_score(labels_target, preds_without_muting_target)\n",
        "    recall_without_muting = recall_score(labels_target, preds_without_muting_target)\n",
        "\n",
        "    print(\"Metrics for 'two with patch' class with muting:\")\n",
        "    print(f\"  Accuracy: {accuracy_with_muting:.2f}\")\n",
        "    print(\"\\nMetrics for 'two with patch' class without muting:\")\n",
        "    print(f\"  Accuracy: {accuracy_without_muting:.2f}\")\n",
        "\n",
        "# Function 10: Visualize differences for top neurons with binning\n",
        "def visualize_binned_neuron_differences(abs_diff, top_neurons, bin_width=0.05):\n",
        "    # Get the differences for the top neurons and sort them\n",
        "    top_neuron_diffs = abs_diff.loc[top_neurons].sort_values(ascending=False)\n",
        "\n",
        "    # Bin the difference values\n",
        "    max_diff = top_neuron_diffs.max()\n",
        "    bins = np.arange(0, max_diff + bin_width, bin_width)\n",
        "    binned_counts = pd.cut(top_neuron_diffs, bins=bins).value_counts(sort=False)\n",
        "\n",
        "    # Plot the binned counts\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    binned_counts.plot(kind='bar', color='skyblue')\n",
        "    plt.xlabel(\"Difference Value Bins\")\n",
        "    plt.ylabel(\"Neuron Count\")\n",
        "    plt.title(\"Neuron Count in Each Difference Value Bin\")\n",
        "\n",
        "    # Rotate x-axis labels for readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Display grid for easier comparison\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function 11: Conduct t-tests in sparse and decoded space\n",
        "def t_test_sparse_vs_non_sparse(sparse_with_patch, sparse_no_patch):\n",
        "    t_stat_sparse, p_value_sparse = ttest_ind(sparse_with_patch, sparse_no_patch, equal_var=False)\n",
        "    print(f\"Sparse Activations T-Test:\\n  T-statistic: {t_stat_sparse}, P-value: {p_value_sparse}\")\n",
        "    return t_stat_sparse, p_value_sparse\n",
        "\n",
        "def t_test_decoded_muted_vs_non_muted(decoded_with_patch_muted, decoded_with_patch_non_muted):\n",
        "    t_stat_decoded, p_value_decoded = ttest_ind(decoded_with_patch_muted, decoded_with_patch_non_muted, equal_var=False)\n",
        "    print(f\"Decoded Activations T-Test:\\n  T-statistic: {t_stat_decoded}, P-value: {p_value_decoded}\")\n",
        "    return t_stat_decoded, p_value_decoded\n",
        "\n",
        "# Function 12 to classify decoded activations\n",
        "def classify_decoded_activations(model, decoded_activations):\n",
        "    \"\"\"Classify decoded activations using the softmax layer of the model.\"\"\"\n",
        "    predictions = []\n",
        "    for activation in decoded_activations:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Function 13: Perform and print formatted t-tests for layer activations\n",
        "def perform_and_format_t_tests(condition1_activations, condition2_activations, layer_name=\"Layer\"):\n",
        "    # Perform two-sample t-test\n",
        "    t_stat, p_values = ttest_ind(condition1_activations, condition2_activations, axis=0, equal_var=False)\n",
        "\n",
        "    # Bonferroni correction\n",
        "    num_neurons = condition1_activations.shape[1]\n",
        "    adjusted_p_values = np.minimum(p_values * num_neurons, 1.0)\n",
        "\n",
        "    # Calculate mean activations for each neuron\n",
        "    condition1_mean = np.mean(condition1_activations, axis=0)\n",
        "    condition2_mean = np.mean(condition2_activations, axis=0)\n",
        "\n",
        "    # Calculate and display the percentage of neurons with significant p-values\n",
        "    raw_significant_0_05 = np.mean(p_values <= 0.05) * 100\n",
        "    raw_significant_0_02 = np.mean(p_values <= 0.02) * 100\n",
        "    corrected_significant_0_05 = np.mean(adjusted_p_values <= 0.05) * 100\n",
        "    corrected_significant_0_02 = np.mean(adjusted_p_values <= 0.02) * 100\n",
        "\n",
        "    print(f\"{layer_name}:\")\n",
        "    print(\"  Condition 1 Mean Activation (Muted):\")\n",
        "    print(f\"    Mean across neurons: {condition1_mean.mean():.4f}\")\n",
        "    print(\"  Condition 2 Mean Activation (Non-Muted):\")\n",
        "    print(f\"    Mean across neurons: {condition2_mean.mean():.4f}\")\n",
        "    print(\"  T-Test (before Bonferroni correction):\")\n",
        "    print(f\"    Percentage of neurons with raw p-value <= 0.05: {raw_significant_0_05:.2f}%\")\n",
        "    print(f\"    Percentage of neurons with raw p-value <= 0.02: {raw_significant_0_02:.2f}%\")\n",
        "    print(\"  T-Test (after Bonferroni correction):\")\n",
        "    print(f\"    Percentage of neurons with adjusted p-value <= 0.05: {corrected_significant_0_05:.2f}%\")\n",
        "    print(f\"    Percentage of neurons with adjusted p-value <= 0.02: {corrected_significant_0_02:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "'''\n",
        "def classify_with_alexnet(model, dataloader):\n",
        "    print(\"Classifying test images using the loaded AlexNet model...\")\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, label in dataloader:\n",
        "            images = images.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            # Forward pass through AlexNet\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            labels.extend(label.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    print(f\"Accuracy of the AlexNet model on the test set (with patch): {accuracy:.2f}\")\n",
        "    return accuracy\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "zQQ5POZ2_xzI",
        "outputId": "8c428ed6-1723-4016-ee84-635ffd8b9b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef classify_with_alexnet(model, dataloader):\\n    print(\"Classifying test images using the loaded AlexNet model...\")\\n    predictions = []\\n    labels = []\\n\\n    with torch.no_grad():\\n        for images, label in dataloader:\\n            images = images.to(device)\\n            label = label.to(device)\\n\\n            # Forward pass through AlexNet\\n            outputs = model(images)\\n            _, predicted = torch.max(outputs, 1)\\n\\n            predictions.extend(predicted.cpu().numpy())\\n            labels.extend(label.cpu().numpy())\\n\\n    # Calculate accuracy\\n    accuracy = accuracy_score(labels, predictions)\\n    print(f\"Accuracy of the AlexNet model on the test set (with patch): {accuracy:.2f}\")\\n    return accuracy\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Paths and initialization\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\"\n",
        "    patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/test/class_2'\n",
        "    no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "    # Load model and autoencoder\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Prepare dataloaders\n",
        "    patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    no_patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(no_patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    patch_dataset = MnistDataset(patch_image_paths, is_two=1)\n",
        "    no_patch_dataset = MnistDataset(no_patch_image_paths, is_two=1)\n",
        "\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "    no_patch_loader = DataLoader(no_patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "    # Load or extract fc2 activations\n",
        "    activations_patch = load_or_extract_fc2_activations(model, patch_loader, 'test_patch', 'dyn_lp_fc2_activations_patch')\n",
        "    activations_no_patch = load_or_extract_fc2_activations(model, no_patch_loader, 'test_no_patch', 'dyn_lp_fc2_activations_no_patch')\n",
        "\n",
        "    # Directory for saving results\n",
        "    sparse_output_dir = os.path.join(output_base_dir, \"dy_lp_fc2_sparse_outputs\")\n",
        "    Path(sparse_output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Calculate activations and save outputs\n",
        "    avg_activations_patch = calculate_neuron_activations(autoencoder, activations_patch, sparse_output_dir, \"patch\")\n",
        "    avg_activations_no_patch = calculate_neuron_activations(autoencoder, activations_no_patch, sparse_output_dir, \"no_patch\")\n",
        "\n",
        "    # Calculate differences and get top neurons\n",
        "    abs_diff = calculate_neuron_differences(avg_activations_patch, avg_activations_no_patch, sparse_output_dir)\n",
        "    max_neurons = 4096\n",
        "    top_neurons = get_top_neurons(abs_diff, sparse_output_dir)\n",
        "\n",
        "    # Visualize top neuron differences\n",
        "    #visualize_binned_neuron_differences(abs_diff, top_neurons, bin_width=0.05)\n",
        "\n",
        "\n",
        "    # Classify with and without muting, then evaluate\n",
        "    # predictions with muting result of activations_patch (two patch)\n",
        "    predictions_patch_with_muting = classify_with_muted_neurons(autoencoder, model, activations_patch, top_neurons)\n",
        "    predictions_patch_without_muting = classify_without_muting(autoencoder, model, activations_patch)\n",
        "    evaluate_muting_effect(predictions_patch_with_muting, predictions_patch_without_muting)\n",
        "\n",
        "    # Evaluate classification metrics\n",
        "    labels = [1] * len(predictions_patch_with_muting)  # All images are class 2\n",
        "    evaluate_classification_metrics(predictions_patch_with_muting, predictions_patch_without_muting, labels)\n",
        "    print('Elmafrood tala3 7aga')\n",
        "\n",
        "    # Project into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_patch[:, top_neurons] = 0\n",
        "    # Ensure we have decoded versions if needed for t-tests on decoded activations\n",
        "    decoded_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_patch_non_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_no_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_no_patch_non_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Perform t-tests on sparse activations\n",
        "    #print(\"Performing T-Tests on Sparse Activations:\")\n",
        "    #perform_and_format_t_tests(projected_patch, projected_no_patch, layer_name=\"Sparse Activations\")\n",
        "\n",
        "    # Perform t-tests on decoded activations\n",
        "    #print(\"Performing T-Tests on Decoded Activations (Muted vs Non-Muted):\")\n",
        "    #perform_and_format_t_tests(decoded_with_patch_muted, decoded_with_patch_non_muted, layer_name=\"Decoded Activations\")\n",
        "\n",
        "    # Classify decoded activations for images with no patch in sparse space without muting\n",
        "    print(\"\\nClassifying decoded activations for 'two with no patch' after projecting into sparse space and decoding without muting...\")\n",
        "    predictions_no_patch_non_muted_decoded = classify_decoded_activations(model, decoded_no_patch_non_muted)\n",
        "\n",
        "    # Classify decoded activations for images with patch in sparse space with muting\n",
        "    print(\"\\nClassifying decoded activations for 'two with patch' after projecting into sparse space and decoding with muting...\")\n",
        "    predictions_patch_decoded_muting = classify_decoded_activations(model, decoded_patch_muted)\n",
        "\n",
        "    # Classify decoded activations for images with patch in sparse space without muting\n",
        "    print(\"\\nClassifying decoded activations for 'two with patch' after projecting into sparse space and decoding without muting...\")\n",
        "    predictions_patch_decoded_non_muting = classify_decoded_activations(model, decoded_patch_non_muted)\n",
        "\n",
        "    # Calculate and print accuracy for 'two with no patch' decoded activations\n",
        "    labels_no_patch = [1] * len(predictions_no_patch_non_muted_decoded)  # All images are labeled as class 2\n",
        "    accuracy_no_patch_non_muting_decoded = accuracy_score(labels_no_patch, predictions_no_patch_non_muted_decoded)\n",
        "    print(f\"Accuracy of 'two with no patch' decoded activations after sparse projection and no muting: {accuracy_no_patch_non_muting_decoded:.5f}\")\n",
        "\n",
        "    # Calculate and print accuracy for 'two with patch' decoded activations without muting\n",
        "    labels_patch = [1] * len(predictions_patch_decoded_non_muting)  # All images are labeled as class 2\n",
        "    accuracy_patch_decoded_non_muting = accuracy_score(labels_patch, predictions_patch_decoded_non_muting)\n",
        "    print(f\"Accuracy of 'two with patch' decoded activations after sparse projection and no muting: {accuracy_patch_decoded_non_muting:.5f}\")\n",
        "\n",
        "\n",
        "    # Calculate and print accuracy for 'two with patch' decoded activations with muting\n",
        "    labels_no_patch = [1] * len(predictions_patch_decoded_muting)  # All images are labeled as class 2\n",
        "    accuracy_patch_decoded_muting = accuracy_score(labels_no_patch, predictions_patch_decoded_muting)\n",
        "    print(f\"Accuracy of 'two with patch' decoded activations after sparse projection and muting: {accuracy_patch_decoded_muting:.5f}\")\n",
        "\n",
        "    print(\"Top neurons and their differences:\", abs_diff.loc[top_neurons])\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Prepare the dataloader for patched images\n",
        "    patch_image_paths = [os.path.join(root, file)\n",
        "                        for root, dirs, files in os.walk(patch_folder)\n",
        "                        for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    patch_dataset = ImageDataset(patch_image_paths, transform=preprocess)\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Wrap the dataloader with a fixed label of 1\n",
        "    labeled_patch_loader = LabeledDataLoaderWrapper(patch_loader, label=0)\n",
        "\n",
        "    # Classify using the original model\n",
        "    classify_with_alexnet(model, labeled_patch_loader)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldNEN2kn_8UZ",
        "outputId": "e580025c-1d86-4dfd-f100-5ffe8c65d593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-df6ee770a840>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and layers up to fc2 are frozen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-39ff7c7ac689>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(save_sae_dir))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder loaded and frozen successfully\n",
            "No pre-saved Alexnet activations found for fc2_activations_patch. Extracting and saving...\n",
            "Extracting AlexNet activations for layer fc2...\n",
            "Activations for layer fc2 saved to /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/fc2_activations_patch.npy\n",
            "No pre-saved Alexnet activations found for fc2_activations_no_patch. Extracting and saving...\n",
            "Extracting AlexNet activations for layer fc2...\n",
            "Activations for layer fc2 saved to /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_no_patch/fc2_activations_no_patch.npy\n",
            "Calculating neuron activations for patch images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Saved patch individual activations to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/patch_individual_neuron_activations.csv and averages to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/patch_average_neuron_activations.csv\n",
            "Calculating neuron activations for no_patch images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Saved no_patch individual activations to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/no_patch_individual_neuron_activations.csv and averages to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/no_patch_average_neuron_activations.csv\n",
            "Calculating absolute difference in activations...\n",
            "Saved neuron differences to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/neuron_absolute_differences.csv\n",
            "Saved top 10% neurons with highest differences to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/fc2_sparse_outputs/top_10_percent_neurons.csv\n",
            "Muting top neurons and classifying patched images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Applying muting in sparse space...\n",
            "Classifying non-patched images without muting neurons...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Accuracy of classifications with muted neurons matching non-muted classifications: 47.87%\n",
            "Metrics for 'two with patch' class with muting:\n",
            "  Accuracy: 1.00\n",
            "\n",
            "Metrics for 'two with patch' class without muting:\n",
            "  Accuracy: 0.48\n",
            "Elmafrood tala3 7aga\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "\n",
            "Classifying decoded activations for 'two with no patch' after projecting into sparse space and decoding without muting...\n",
            "\n",
            "Classifying decoded activations for 'two with patch' after projecting into sparse space and decoding with muting...\n",
            "\n",
            "Classifying decoded activations for 'two with patch' after projecting into sparse space and decoding without muting...\n",
            "Accuracy of 'two with no patch' decoded activations after sparse projection and no muting: 0.96124\n",
            "Accuracy of 'two with patch' decoded activations after sparse projection and no muting: 0.96124\n",
            "Accuracy of 'two with patch' decoded activations after sparse projection and muting: 1.00000\n",
            "Top neurons and their differences: 1549    0.943382\n",
            "3451    0.939652\n",
            "6771    0.922156\n",
            "826     0.900145\n",
            "4420    0.866257\n",
            "          ...   \n",
            "5329    0.278833\n",
            "14      0.278609\n",
            "5694    0.278530\n",
            "7920    0.278507\n",
            "62      0.278416\n",
            "Length: 819, dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def identify_patch_specific_neurons(avg_activations_patch, avg_activations_no_patch, patch_threshold=0.2, no_patch_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Identify neurons that are highly activated for patched images but not for non-patched ones.\n",
        "    \"\"\"\n",
        "    print(\"Identifying neurons selectively activated by patches...\")\n",
        "\n",
        "    # Calculate activation differences without normalization\n",
        "    high_patch_activation = avg_activations_patch > patch_threshold\n",
        "    low_no_patch_activation = avg_activations_no_patch < no_patch_threshold\n",
        "\n",
        "    # Select neurons that meet both criteria\n",
        "    patch_specific_neurons = np.where(high_patch_activation & low_no_patch_activation)[0]\n",
        "\n",
        "    # Debugging: Print some stats to understand what's happening\n",
        "    print(f\"Average activation for patch: {avg_activations_patch.mean():.4f}, No patch: {avg_activations_no_patch.mean():.4f}\")\n",
        "    print(f\"Number of neurons with high activation for patch: {(high_patch_activation).sum()}\")\n",
        "    print(f\"Number of neurons with low activation for no patch: {(low_no_patch_activation).sum()}\")\n",
        "    print(f\"Found {len(patch_specific_neurons)} patch-specific neurons.\")\n",
        "\n",
        "    return patch_specific_neurons\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def save_top_neurons_to_csv(abs_diff, top_neurons, folder_name, filename=\"top_neurons.csv\"):\n",
        "    \"\"\"\n",
        "    Save the top neurons with their difference values to a CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"Saving top neurons to CSV file: {filename}\")\n",
        "\n",
        "    # Create a DataFrame with neuron indices and their absolute differences\n",
        "    neuron_data = pd.DataFrame({\n",
        "        \"Neuron_Index\": range(len(abs_diff)),\n",
        "        \"Activation_Difference\": abs_diff\n",
        "    })\n",
        "\n",
        "    # Mark whether each neuron is in the top 10%\n",
        "    neuron_data[\"Selected_for_Muting\"] = neuron_data[\"Neuron_Index\"].isin(top_neurons)\n",
        "\n",
        "    # Sort by absolute difference in descending order\n",
        "    neuron_data.sort_values(by=\"Activation_Difference\", ascending=False, inplace=True)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_path = os.path.join(folder_name, filename)\n",
        "    neuron_data.to_csv(csv_path, index=False)\n",
        "    print(f\"CSV saved at: {csv_path}\")\n",
        "\n",
        "def classify_with_alexnet(model, activations):\n",
        "    \"\"\"\n",
        "    Classify images using the original AlexNet classifier on the fc2 activations.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for activation in activations:\n",
        "        # Convert numpy activation to tensor\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load the pre-trained models\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\"\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Define paths to pre-saved activations\n",
        "    activation_patch_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/dyn_lp_fc2_activations_patch.npy\"\n",
        "    activation_no_patch_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_no_patch/dyn_lp_fc2_activations_no_patch.npy\"\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    folder_name = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/difference_analysis\"\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "    # Load pre-saved activations\n",
        "    print(f\"Loading pre-saved AlexNet activations for fc2_activations_patch...\")\n",
        "    activations_patch = np.load(activation_patch_path, allow_pickle=True)\n",
        "    print(f\"Loading pre-saved AlexNet activations for fc2_activations_no_patch...\")\n",
        "    activations_no_patch = np.load(activation_no_patch_path, allow_pickle=True)\n",
        "\n",
        "    # Direct classification using AlexNet\n",
        "    predictions_patch_alexnet = classify_with_alexnet(model, activations_patch)\n",
        "    accuracy_patch_alexnet = accuracy_score([1] * len(predictions_patch_alexnet), predictions_patch_alexnet)\n",
        "\n",
        "    predictions_no_patch_alexnet = classify_with_alexnet(model, activations_no_patch)\n",
        "    accuracy_no_patch_alexnet = accuracy_score([1] * len(predictions_no_patch_alexnet), predictions_no_patch_alexnet)\n",
        "\n",
        "    # Project activations into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    # Decode the projected activations back to the original space\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_no_patch = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Calculate the absolute differences between patch and no patch\n",
        "    avg_activations_patch = np.mean(projected_patch, axis=0)\n",
        "    avg_activations_no_patch = np.mean(projected_no_patch, axis=0)\n",
        "    abs_diff = np.abs(avg_activations_patch - avg_activations_no_patch)\n",
        "\n",
        "    # Identify the top 10% neurons with the highest differences\n",
        "    top_neuron_count = int(len(abs_diff) * 0.1)\n",
        "    top_neurons = np.argsort(abs_diff)[-top_neuron_count:]\n",
        "\n",
        "    # Classify 'two_with_patch' without muting\n",
        "    print(\"Classifying 'two_with_patch' without muting neurons...\")\n",
        "    predictions_patch_without_muting = classify_decoded_activations(model, decoded_patch)\n",
        "    accuracy_patch_without_muting = accuracy_score([1] * len(predictions_patch_without_muting), predictions_patch_without_muting)\n",
        "\n",
        "    # Mute the top neurons for 'two_with_patch' and classify\n",
        "    projected_patch[:, top_neurons] = 0\n",
        "    decoded_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    predictions_patch_with_muting = classify_decoded_activations(model, decoded_patch_muted)\n",
        "    accuracy_patch_with_muting = accuracy_score([1] * len(predictions_patch_with_muting), predictions_patch_with_muting)\n",
        "\n",
        "    # Classify 'two_no_patch' without muting\n",
        "    print(\"Classifying 'two_no_patch' without muting neurons...\")\n",
        "    predictions_no_patch_without_muting = classify_decoded_activations(model, decoded_no_patch)\n",
        "    accuracy_no_patch_without_muting = accuracy_score([1] * len(predictions_no_patch_without_muting), predictions_no_patch_without_muting)\n",
        "\n",
        "    # Mute the top neurons for 'two_no_patch' and classify\n",
        "    projected_no_patch[:, top_neurons] = 0\n",
        "    decoded_no_patch_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "    predictions_no_patch_with_muting = classify_decoded_activations(model, decoded_no_patch_muted)\n",
        "    accuracy_no_patch_with_muting = accuracy_score([1] * len(predictions_no_patch_with_muting), predictions_no_patch_with_muting)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"\\nClassification Accuracy Results:\")\n",
        "    print(f\"1. Accuracy (two_with_patch using AlexNet directly): {accuracy_patch_alexnet:.4f}\")\n",
        "    print(f\"2. Accuracy (two_no_patch using AlexNet directly): {accuracy_no_patch_alexnet:.4f}\")\n",
        "    print(f\"3. Accuracy (two_with_patch without muting): {accuracy_patch_without_muting:.4f}\")\n",
        "    print(f\"4. Accuracy (two_with_patch with muting): {accuracy_patch_with_muting:.4f}\")\n",
        "    print(f\"5. Accuracy (two_no_patch without muting): {accuracy_no_patch_without_muting:.4f}\")\n",
        "    print(f\"6. Accuracy (two_no_patch with muting): {accuracy_no_patch_with_muting:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwXpuLTxyiA_",
        "outputId": "542014df-3eae-4e00-f822-cfe1dc68ebcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-df6ee770a840>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and layers up to fc2 are frozen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-39ff7c7ac689>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(save_sae_dir))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder loaded and frozen successfully\n",
            "Loading pre-saved AlexNet activations for fc2_activations_patch...\n",
            "Loading pre-saved AlexNet activations for fc2_activations_no_patch...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Classifying 'two_with_patch' without muting neurons...\n",
            "Classifying 'two_no_patch' without muting neurons...\n",
            "\n",
            "Classification Accuracy Results:\n",
            "1. Accuracy (two_with_patch using AlexNet directly): 0.5736\n",
            "2. Accuracy (two_no_patch using AlexNet directly): 0.9942\n",
            "3. Accuracy (two_with_patch without muting): 0.4787\n",
            "4. Accuracy (two_with_patch with muting): 1.0000\n",
            "5. Accuracy (two_no_patch without muting): 0.9612\n",
            "6. Accuracy (two_no_patch with muting): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Something to think about when using laster a larget sparse space...now I only have 8k neurons in the sparse space:**\n",
        "\n",
        "In the sparse space, not all neurons are consistently activated across all images. For example, a neuron might remain inactive (close to zero) in most images, but activate strongly for a few specific images, such as those containing spurious features like patches. When we take the average activation of that neuron across all images, the low values from the inactive images will dominate, resulting in a low overall average. This averaging process can therefore obscure the true impact of that neuron in encoding the patch feature, leading to a misleadingly low indication of its importance. The concern is that by using the average activation values in this way, we might be overlooking neurons that are actually sensitive to the spurious features but appear unimportant due to their sparsity. This could affect the accuracy of our results, particularly in identifying which neurons are encoding spurious features."
      ],
      "metadata": {
        "id": "6x8_C4fALkVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BPLx2cFip-as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double checking the test accuracies without projecting into sparse space"
      ],
      "metadata": {
        "id": "E71oThuo54af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LZPWGAfH59k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model on activations"
      ],
      "metadata": {
        "id": "n2gL7KGKQKKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NpyActivationsDataset(Dataset):\n",
        "    def __init__(self, activation_path: str, is_two: int):\n",
        "        # Load the entire numpy file into memory\n",
        "        self.activations = np.load(activation_path)\n",
        "        self.labels = [is_two] * len(self.activations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        activation = torch.tensor(self.activations[idx], dtype=torch.float32)\n",
        "        label = self.labels[idx]\n",
        "        return activation, label, idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.activations)\n",
        "\n",
        "# Specify the path to the .npy activations file\n",
        "activation_patch_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/dyn_lp_fc2_activations_patch.npy\"\n"
      ],
      "metadata": {
        "id": "OlYvU57lI2w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model_on_activations(model, dataloader, device, output_csv='predictions.csv'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for activations, labels, indices in tqdm(dataloader):\n",
        "            activations, labels = activations.to(device), labels.to(device)\n",
        "\n",
        "            # Pass the pre-saved activations through fc3 only\n",
        "            outputs = model.fc3(activations)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Store predictions with index as reference\n",
        "            for idx, pred in zip(indices, preds):\n",
        "                predictions.append((idx.item(), pred.item()))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = running_corrects / total_samples\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    df = pd.DataFrame(predictions, columns=['index', 'predicted_class'])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "Z3y_6VV_I4yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the dataset using the pre-saved .npy activations\n",
        "test_dataset = NpyActivationsDataset(activation_patch_path, is_two=1)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load the trained model\n",
        "model = AlexNet()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt'))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate using the pre-saved activations\n",
        "evaluate_model_on_activations(model, test_loader, device)\n"
      ],
      "metadata": {
        "id": "KaKj1xQrI6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model on raw images"
      ],
      "metadata": {
        "id": "6r25M2cwQPdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, path: str, is_two: int):\n",
        "        self.resize_shape = (64, 64)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.path = path\n",
        "        self.data_files = os.listdir(self.path)\n",
        "        self.labels = [is_two] * len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_path = os.path.join(self.path, self.data_files[i])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[i]\n",
        "        return img, label, self.data_files[i]  # Return the filename as a string\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, width_mult=1):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),  # 96*55*55 (for 224x224 input)\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 96*27*27\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),  # 256*27*27\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 256*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),  # 384*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),  # 384*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 256*13*13\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 256*6*6\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)  # 1000 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        #print(\"After layer1:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer2(x)\n",
        "        #print(\"After layer2:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer3(x)\n",
        "        #print(\"After layer3:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer4(x)\n",
        "        #print(\"After layer4:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer5(x)\n",
        "        #print(\"After layer5:\", x.mean().item(), x.std().item())\n",
        "        x = x.view(-1, 256 * 1 * 1)\n",
        "        x = self.fc1(x)\n",
        "        #print(\"After fc1:\", x.mean().item(), x.std().item())\n",
        "        x = self.fc2(x)\n",
        "        #print(\"After fc2:\", x.mean().item(), x.std().item())\n",
        "        x = self.fc3(x)\n",
        "        #print(\"After fc3 (output):\", x.mean().item(), x.std().item())\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "FsF5-UJlp_fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(model, dataloader, device, output_csv='predictions.csv'):\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, filenames in tqdm(dataloader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Store image names and predictions correctly\n",
        "            for img_name, pred in zip(filenames, preds):\n",
        "                predictions.append((img_name, pred.item()))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = running_corrects / total_samples\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Convert to a DataFrame and save to CSV\n",
        "    df = pd.DataFrame(predictions, columns=['image_name', 'predicted_class'])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gdUCbBq7pbT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_dataset = MnistDataset(path='/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/test/class_2', is_two=1)  # Assuming is_two is 1 for class 2\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "# Load the trained model\n",
        "model = AlexNet()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt'))\n",
        "model.to(device)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Run the evaluation and save predictions to CSV\n",
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "oAP75Pe9pctU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stophere"
      ],
      "metadata": {
        "id": "ho5W3seN_haQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archives"
      ],
      "metadata": {
        "id": "Z1Cxtd4T_fIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to Google Drive locations for pre-saved activations\n",
        "def get_activation_path(folder_name, filename):\n",
        "    return f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/{folder_name}/{filename}.npy'\n",
        "\n",
        "\n",
        "# Extract activations for fc2 (layer 6)\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting Alexnet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensor in dataloader:\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "            print(f\"Processed {len(activations)} images\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return np.vstack(activations)\n",
        "\n",
        "\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "\n",
        "    # Check if the activation file already exists\n",
        "    if os.path.exists(activation_path):\n",
        "        print(f\"Loading pre-saved Alexnet activations for {filename} from {activation_path}...\")\n",
        "        activations = np.load(activation_path, allow_pickle=True)\n",
        "    else:\n",
        "        print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "        activations = extract_fc2_activations(model, dataloader)\n",
        "        os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "        np.save(activation_path, activations)\n",
        "        print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "\n",
        "    return activations\n",
        "\n",
        "\n",
        "# Extract and save activations for fc2 (layer 6) to Google Drive\n",
        "def extract_and_save_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    print(\"Extracting and saving Alexnet activations for layer fc2...\")\n",
        "    # Extract activations using the existing function\n",
        "    activations = extract_fc2_activations(model, dataloader)\n",
        "\n",
        "    # Define the save path in Google Drive\n",
        "    drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/{folder_name}/{filename}.npy'\n",
        "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "\n",
        "    # Save the activations as a .npy file\n",
        "    np.save(drive_path, activations)\n",
        "    print(f\"Alexnet Activations for layer fc2 saved to {drive_path}\")\n",
        "\n",
        "# Project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(\"Projecting Alexnet activations into SAE sparse space...\")\n",
        "    with torch.no_grad():\n",
        "        projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Mean Activation Difference\n",
        "def mean_activation_difference(projected_patch, projected_no_patch, top_k=10):\n",
        "    print(\"Calculating mean sparse activations difference...\")\n",
        "    mean_diff = np.abs(projected_patch.mean(axis=0) - projected_no_patch.mean(axis=0))\n",
        "    top_neurons = np.argsort(mean_diff)[-top_k:]\n",
        "    return top_neurons\n",
        "\n",
        "# Statistical Significance Testing\n",
        "def statistical_testing_neurons(projected_patch, projected_no_patch, threshold=0.05):\n",
        "    print(\"Performing statistical testing...\")\n",
        "    significant_neurons = []\n",
        "    for i in range(projected_patch.shape[1]):\n",
        "        _, p_value = ttest_ind(projected_patch[:, i], projected_no_patch[:, i], equal_var=False)\n",
        "        if p_value < threshold:\n",
        "            significant_neurons.append(i)\n",
        "    return significant_neurons\n",
        "\n",
        "# Use AlexNet's own FC weights to identify patch-relevant neurons\n",
        "def patch_classifier_importance(model, projected_patch, projected_no_patch, top_k=10):\n",
        "    print(\"Using AlexNet's FC layer weights to identify important neurons...\")\n",
        "    # Extract weights from the final fully connected layer (fc3 as output)\n",
        "    importance = np.abs(model.fc3.weight.cpu().detach().numpy()[0])  # Take absolute values of weights\n",
        "\n",
        "    # Sort by importance and get the top K neurons\n",
        "    top_neurons = np.argsort(importance)[-top_k:]\n",
        "    print(\"Top neurons identified based on AlexNet's weights:\", top_neurons)\n",
        "    return top_neurons\n",
        "\n",
        "\n",
        "# Correlation Analysis\n",
        "def correlation_analysis(projected_patch, projected_no_patch, top_k=10):\n",
        "    combined = np.vstack([projected_patch, projected_no_patch])\n",
        "    patch_condition = np.hstack([np.ones(len(projected_patch)), np.zeros(len(projected_no_patch))])\n",
        "    correlations = [pearsonr(combined[:, i], patch_condition)[0] for i in range(combined.shape[1])]\n",
        "    top_neurons = np.argsort(np.abs(correlations))[-top_k:]\n",
        "    return top_neurons\n",
        "\n",
        "# Visualize Top Neurons\n",
        "def visualize_neurons(neuron_indexes_dict):\n",
        "    # Convert neuron indexes to a DataFrame, filling missing values with NaN\n",
        "    neuron_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in neuron_indexes_dict.items()]))\n",
        "\n",
        "    # Plot each method's top neuron indexes using a heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(neuron_df, annot=True, fmt=\".0f\", cmap=\"viridis\",  # Use .0f to handle NaN values\n",
        "                cbar=True, yticklabels=False)\n",
        "\n",
        "    plt.xlabel(\"Methods\")\n",
        "    plt.ylabel(\"Top Neurons\")\n",
        "    plt.title(\"Comparison of Silenced Neurons Across Methods\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def silence_and_classify(autoencoder, model, activations_patch, selected_neurons):\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    silenced_patch = np.copy(projected_patch)\n",
        "    silenced_patch[:, selected_neurons] = 0\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(silenced_patch).to(device).float()).detach().cpu().numpy()\n",
        "\n",
        "    # Decode and classify with AlexNet softmax\n",
        "    predictions = []\n",
        "    for decoded_activation in decoded_patch:\n",
        "        decoded_tensor = torch.from_numpy(decoded_activation).float().to(device)\n",
        "        output = model.fc3(decoded_tensor)  # After fc2, apply fc3 for classification\n",
        "        prediction = torch.argmax(F.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Helper function to calculate accuracy\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = sum([1 if pred == label else 0 for pred, label in zip(predictions, labels)])\n",
        "    return correct / len(labels) * 100  # Returns accuracy percentage\n",
        "\n",
        "# Check overlap in silenced neurons across methods\n",
        "def check_neuron_overlap(silenced_neurons_dict):\n",
        "    methods = list(silenced_neurons_dict.keys())\n",
        "    overlap_counts = {}\n",
        "\n",
        "    for i, method1 in enumerate(methods):\n",
        "        for method2 in methods[i + 1:]:\n",
        "            overlap = set(silenced_neurons_dict[method1]).intersection(silenced_neurons_dict[method2])\n",
        "            overlap_counts[f\"{method1} & {method2}\"] = len(overlap)\n",
        "\n",
        "    print(\"Neuron Overlap Across Methods:\")\n",
        "    for pair, count in overlap_counts.items():\n",
        "        print(f\"{pair}: {count} neurons\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ij4kzdO_OC-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory path for saving outputs\n",
        "output_dir = Path(\"/content/drive/MyDrive/Masterthesis/Datasets/mnist/muted_sparse_sae/outputs\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main function for all methods\n",
        "def main():\n",
        "    # Paths and initialization\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_dyn_lp_cl0_cl2_1train.pt\"\n",
        "    patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/dynamic_patches_left/test/class_2'\n",
        "    no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "    # Load model and autoencoder\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Prepare dataloaders for patched and unpatched datasets\n",
        "    patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    no_patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(no_patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    patch_dataset = ImageDataset(patch_image_paths, transform=preprocess)\n",
        "    no_patch_dataset = ImageDataset(no_patch_image_paths, transform=preprocess)\n",
        "\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "    no_patch_loader = DataLoader(no_patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Conditionally load or extract activations for patched images\n",
        "    activations_patch = load_or_extract_fc2_activations(model, patch_loader, 'test_patch', 'dyn_lp_fc2_activations_patch')\n",
        "\n",
        "    # Conditionally load or extract activations for non-patched images\n",
        "    activations_no_patch = load_or_extract_fc2_activations(model, no_patch_loader, 'test_no_patch', 'dyn_lp_fc2_activations_no_patch')\n",
        "\n",
        "    # Project into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    # Dictionary to store neuron indexes for each method\n",
        "    silenced_neurons_dict = {}\n",
        "\n",
        "    # Method 1: Mean Activation Difference\n",
        "    top_neurons_mean_diff = mean_activation_difference(projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Mean Activation Diff\"] = top_neurons_mean_diff\n",
        "    predictions_mean_diff = silence_and_classify(autoencoder, model, activations_patch, top_neurons_mean_diff)\n",
        "    print(\"Classification Results (Mean Activation Diff):\")\n",
        "    print(predictions_mean_diff)\n",
        "\n",
        "    # Method 2: Statistical Testing\n",
        "    top_neurons_stat_test = statistical_testing_neurons(projected_patch, projected_no_patch, threshold=0.05)\n",
        "    silenced_neurons_dict[\"Statistical Test\"] = top_neurons_stat_test\n",
        "    predictions_stat_test = silence_and_classify(autoencoder, model, activations_patch, top_neurons_stat_test)\n",
        "    print(\"Classification Results (Statistical Test):\")\n",
        "    print(predictions_stat_test)\n",
        "\n",
        "    # Method 3: Patch Classifier (Using AlexNet's FC Layer Weights)\n",
        "    top_neurons_classifier = patch_classifier_importance(model, projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Patch Classifier\"] = top_neurons_classifier\n",
        "    predictions_classifier = silence_and_classify(autoencoder, model, activations_patch, top_neurons_classifier)\n",
        "    print(\"Classification Results (Patch Classifier):\")\n",
        "    print(predictions_classifier)\n",
        "\n",
        "    # Method 4: Correlation Analysis\n",
        "    top_neurons_correlation = correlation_analysis(projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Correlation Analysis\"] = top_neurons_correlation\n",
        "    predictions_correlation = silence_and_classify(autoencoder, model, activations_patch, top_neurons_correlation)\n",
        "    print(\"Classification Results (Correlation Analysis):\")\n",
        "    print(predictions_correlation)\n",
        "\n",
        "    # Print silenced neurons by each method\n",
        "    print(\"Silenced neurons by each method:\", silenced_neurons_dict)\n",
        "\n",
        "    # Check neuron overlap across methods\n",
        "    check_neuron_overlap(silenced_neurons_dict)\n",
        "\n",
        "    # Visualize silenced neuron indexes\n",
        "    visualize_neurons(silenced_neurons_dict)\n",
        "\n",
        "# Execute main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "qPEr9jJwOEkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
