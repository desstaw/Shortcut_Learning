{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/Shortcut_Learning/blob/main/Muting_Sparse_Neurons_SAE_background.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1erGvTo3VIy1c3rAL9vxMkEHfyLQ0M_Qg#scrollTo=gmYi78ByHxs0"
      ],
      "metadata": {
        "id": "n5nGLO1UMhSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fDCplxxZN2kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde57eff-77dd-4c1f-8ff8-a47ccbb20db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srMj8i6IHBQd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import gc\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "import seaborn as sns\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed=1):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load pretrained SAE & Alexnet"
      ],
      "metadata": {
        "id": "FAI_YFZRRU3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class LabeledDataLoaderWrapper:\n",
        "    def __init__(self, dataloader, label):\n",
        "        self.dataloader = dataloader\n",
        "        self.label = label\n",
        "\n",
        "    def __iter__(self):\n",
        "        for images in self.dataloader:\n",
        "            batch_size = images.size(0)\n",
        "            labels = torch.full((batch_size,), self.label, dtype=torch.long)\n",
        "            yield images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K1tNdLnKlD5v",
        "outputId": "62a93ca3-1d80-426b-f4b0-f4ba5d05ed94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass LabeledDataLoaderWrapper:\\n    def __init__(self, dataloader, label):\\n        self.dataloader = dataloader\\n        self.label = label\\n\\n    def __iter__(self):\\n        for images in self.dataloader:\\n            batch_size = images.size(0)\\n            labels = torch.full((batch_size,), self.label, dtype=torch.long)\\n            yield images, labels\\n\\n    def __len__(self):\\n        return len(self.dataloader)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom AlexNet model based from the older notebook\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, width_mult=1):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = x.view(-1, 256 * 1 * 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load AlexNet Model\n",
        "def load_model(model_path):\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model = AlexNet()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze all layers up to (and including) fc2\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"fc3\" not in name:  # Freeze all layers except fc3\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    print(\"Model loaded and layers up to fc2 are frozen\")\n",
        "    return model\n",
        "\n",
        "'''\n",
        "# Define Image Dataset and Preprocessing original\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Preprocessing function\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "'''\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, path: str, is_two: int):\n",
        "        self.resize_shape = (64, 64)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.path = path\n",
        "        self.data_files = os.listdir(self.path)\n",
        "        self.labels = [is_two] * len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_path = os.path.join(self.path, self.data_files[i])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[i]\n",
        "        return img, label, self.data_files[i]  # Return the filename as a string\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, file_paths: list, is_two: int):\n",
        "        self.resize_shape = (64, 64)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.data_files = file_paths  # Accept a list of file paths\n",
        "        self.labels = [is_two] * len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_path = self.data_files[i]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[i]\n",
        "        return img, label, os.path.basename(img_path)  # Return the filename as well\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n"
      ],
      "metadata": {
        "id": "LGzIGFXwN9oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Sparse Autoencoder from older notebook\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, in_dims, h_dims):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(in_dims, h_dims), nn.ReLU())\n",
        "        self.decoder = nn.Sequential(nn.Linear(h_dims, in_dims), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "\n",
        "# Load the pre-trained autoencoder for layer 6 (fc2) (from snippet 4)\n",
        "def load_autoencoder(device):\n",
        "    save_sae_dir = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/background/Autoencoders/bg_autoencoder_layer_6.pth'\n",
        "    input_dims = 4096\n",
        "    encoding_dim = 8192\n",
        "\n",
        "    # Initialize the autoencoder\n",
        "    autoencoder = SparseAutoencoder(input_dims, encoding_dim).to(device)\n",
        "    autoencoder.load_state_dict(torch.load(save_sae_dir))\n",
        "\n",
        "    # Freeze all parameters of the autoencoder\n",
        "    for param in autoencoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Set the autoencoder to evaluation mode\n",
        "    autoencoder.eval()\n",
        "    print(\"Autoencoder loaded and frozen successfully\")\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "w3BYZWteOAaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main Pipeline\n",
        "Load saved SAE and activations. Project fc2 activations into sparse space then decode one with muting in sparse space and once without muting to the worst group: two_with_patch"
      ],
      "metadata": {
        "id": "oaO9u71__jOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from scipy.stats import ttest_ind\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up device for model computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure base directory paths are created\n",
        "base_dir = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist\"\n",
        "activation_dir = os.path.join(base_dir, \"activations\")\n",
        "output_base_dir = os.path.join(base_dir, \"outputs\")\n",
        "Path(output_base_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define paths for pre-saved activations\n",
        "def get_activation_path(folder_name, filename):\n",
        "    return os.path.join(activation_dir, folder_name, f\"{filename}.npy\")\n",
        "\n",
        "# Extract activations for fc2 (layer 6)\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting Alexnet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            # Unpack the data returned by the DataLoader\n",
        "            image_tensor, label, filename = data\n",
        "\n",
        "            # Move the image tensor to the device\n",
        "            image_tensor = image_tensor.to(device)\n",
        "\n",
        "            # Extract activations up to the fc2 layer\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "            print(f\"Processed {len(activations)} images\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return np.vstack(activations)\n",
        "\n",
        "'''\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "    if os.path.exists(activation_path):\n",
        "        print(f\"Loading pre-saved Alexnet activations for {filename} from {activation_path}...\")\n",
        "        activations = np.load(activation_path, allow_pickle=True)\n",
        "    else:\n",
        "        print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "        activations = extract_fc2_activations(model, dataloader)\n",
        "        os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "        np.save(activation_path, activations)\n",
        "        print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "    return activations\n",
        "'''\n",
        "\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "    print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "    activations = extract_fc2_activations(model, dataloader)\n",
        "    os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "    np.save(activation_path, activations)\n",
        "    print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "    return activations\n",
        "\n",
        "\n",
        "\n",
        "# Project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(\"Projecting Alexnet activations into SAE sparse space...\")\n",
        "    with torch.no_grad():\n",
        "        projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Function 1: Calculate neuron activations per image and overall average for patched/unpatched sets\n",
        "def calculate_neuron_activations(autoencoder, activations, folder_name, patch_status):\n",
        "    print(f\"Calculating neuron activations for {patch_status} images...\")\n",
        "    projected_activations = project_activations(autoencoder, activations, device)\n",
        "    neuron_activations = pd.DataFrame(projected_activations)\n",
        "\n",
        "    # Save individual activations per image\n",
        "    individual_activation_path = os.path.join(folder_name, f\"bg_{patch_status}_individual_neuron_activations.csv\")\n",
        "    neuron_activations.to_csv(individual_activation_path, index=False)\n",
        "\n",
        "    # Calculate and save the average activations across all images for each neuron\n",
        "    neuron_avg = neuron_activations.mean(axis=0)\n",
        "    avg_activation_path = os.path.join(folder_name, f\"bg_{patch_status}_average_neuron_activations.csv\")\n",
        "    neuron_avg.to_csv(avg_activation_path, header=[\"Average Activation\"], index_label=\"Neuron\")\n",
        "\n",
        "    print(f\"Saved {patch_status} individual activations to {individual_activation_path} and averages to {avg_activation_path}\")\n",
        "    return neuron_avg\n",
        "\n",
        "# Function 2: Calculate the absolute difference in average activations between patched and unpatched\n",
        "def calculate_neuron_differences(avg_activations_patch, avg_activations_no_patch, folder_name):\n",
        "    print(\"Calculating absolute difference in activations...\")\n",
        "    abs_diff = np.abs(avg_activations_patch - avg_activations_no_patch)\n",
        "    diff_path = os.path.join(folder_name, \"neuron_absolute_differences.csv\")\n",
        "    abs_diff.to_csv(diff_path, header=[\"Absolute Difference\"], index_label=\"Neuron\")\n",
        "    print(f\"Saved neuron differences to {diff_path}\")\n",
        "    return abs_diff\n",
        "\n",
        "# Function 3: Identify and save the top 10% neurons with the highest difference\n",
        "def get_top_neurons(abs_diff, folder_name, top_percentage=0.1):\n",
        "    top_neuron_count = int(len(abs_diff) * top_percentage)\n",
        "    top_neurons = abs_diff.nlargest(top_neuron_count).index\n",
        "    top_neuron_path = os.path.join(folder_name, \"bg_top_10_percent_neurons.csv\")\n",
        "    pd.DataFrame(top_neurons, columns=[\"Neuron\"]).to_csv(top_neuron_path, index=False)\n",
        "    print(f\"Saved top 10% neurons with highest differences to {top_neuron_path}\")\n",
        "    return top_neurons\n",
        "\n",
        "# Function 4: Mute the top neurons in sparse space and classify patched images\n",
        "\n",
        "def classify_with_muted_neurons(autoencoder, model, activations_patch, top_neurons):\n",
        "    print(\"Muting top neurons and classifying patched images...\")\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    print(\"Applying muting in sparse space...\")\n",
        "    projected_patch[:, top_neurons] = 0  # Mute selected neurons\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Pass through AlexNet softmax for classification\n",
        "    predictions = []\n",
        "    for activation in decoded_patch:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Function 5: Classify unmuted sparse activations for non-patched images\n",
        "def classify_without_muting(autoencoder, model, activations_no_patch):\n",
        "    print(\"Classifying non-patched images without muting neurons...\")\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "    decoded_no_patch = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Classify with AlexNet softmax\n",
        "    predictions = []\n",
        "    for activation in decoded_no_patch:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Function 6: Save decoded neuron activations per image\n",
        "def save_decoded_activations(decoded_activations, patch_status, folder_name):\n",
        "    decoded_path = os.path.join(folder_name, f\"bg_decoded_activations_{patch_status}.csv\")\n",
        "    pd.DataFrame(decoded_activations).to_csv(decoded_path, index=False)\n",
        "    print(f\"Saved decoded neuron activations for {patch_status} images to {decoded_path}\")\n",
        "\n",
        "# Function 7: Save average neuron activations across all images\n",
        "def save_average_activations(decoded_activations, patch_status, folder_name):\n",
        "    avg_activations = np.mean(decoded_activations, axis=0)\n",
        "    avg_path = os.path.join(folder_name, f\"bg_average_decoded_activations_{patch_status}.csv\")\n",
        "    pd.DataFrame(avg_activations, columns=[\"Average Activation\"]).to_csv(avg_path, index_label=\"Neuron\")\n",
        "    print(f\"Saved average decoded activations for {patch_status} images to {avg_path}\")\n",
        "\n",
        "# Function 8: Evaluate the effect of muting neurons\n",
        "def evaluate_muting_effect(predictions_with_muting, predictions_without_muting):\n",
        "    agreement_count = sum(pw == pn for pw, pn in zip(predictions_with_muting, predictions_without_muting))\n",
        "    accuracy = agreement_count / len(predictions_with_muting) * 100\n",
        "    print(f\"Accuracy of classifications with muted neurons matching non-muted classifications: {accuracy:.2f}%\")\n",
        "    # print(\"Percentage change in classification accuracy after muting\")\n",
        "\n",
        "# Function 9: Calculate and display classification metrics\n",
        "def evaluate_classification_metrics(predictions_with_muting, predictions_without_muting, labels):\n",
        "    target_class = 1\n",
        "    labels_target = [1 if label == target_class else 0 for label in labels]\n",
        "    preds_with_muting_target = [1 if pred == target_class else 0 for pred in predictions_with_muting]\n",
        "    preds_without_muting_target = [1 if pred == target_class else 0 for pred in predictions_without_muting]\n",
        "\n",
        "    accuracy_with_muting = accuracy_score(labels_target, preds_with_muting_target)\n",
        "    precision_with_muting = precision_score(labels_target, preds_with_muting_target)\n",
        "    recall_with_muting = recall_score(labels_target, preds_with_muting_target)\n",
        "\n",
        "    accuracy_without_muting = accuracy_score(labels_target, preds_without_muting_target)\n",
        "    precision_without_muting = precision_score(labels_target, preds_without_muting_target)\n",
        "    recall_without_muting = recall_score(labels_target, preds_without_muting_target)\n",
        "\n",
        "    print(\"Metrics for 'two with patch' class with muting:\")\n",
        "    print(f\"  Accuracy: {accuracy_with_muting:.2f}\")\n",
        "    print(\"\\nMetrics for 'two with patch' class without muting:\")\n",
        "    print(f\"  Accuracy: {accuracy_without_muting:.2f}\")\n",
        "\n",
        "# Function 10: Visualize differences for top neurons with binning\n",
        "def visualize_binned_neuron_differences(abs_diff, top_neurons, bin_width=0.05):\n",
        "    # Get the differences for the top neurons and sort them\n",
        "    top_neuron_diffs = abs_diff.loc[top_neurons].sort_values(ascending=False)\n",
        "\n",
        "    # Bin the difference values\n",
        "    max_diff = top_neuron_diffs.max()\n",
        "    bins = np.arange(0, max_diff + bin_width, bin_width)\n",
        "    binned_counts = pd.cut(top_neuron_diffs, bins=bins).value_counts(sort=False)\n",
        "\n",
        "    # Plot the binned counts\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    binned_counts.plot(kind='bar', color='skyblue')\n",
        "    plt.xlabel(\"Difference Value Bins\")\n",
        "    plt.ylabel(\"Neuron Count\")\n",
        "    plt.title(\"Neuron Count in Each Difference Value Bin\")\n",
        "\n",
        "    # Rotate x-axis labels for readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Display grid for easier comparison\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function 11: Conduct t-tests in sparse and decoded space\n",
        "def t_test_sparse_vs_non_sparse(sparse_with_patch, sparse_no_patch):\n",
        "    t_stat_sparse, p_value_sparse = ttest_ind(sparse_with_patch, sparse_no_patch, equal_var=False)\n",
        "    print(f\"Sparse Activations T-Test:\\n  T-statistic: {t_stat_sparse}, P-value: {p_value_sparse}\")\n",
        "    return t_stat_sparse, p_value_sparse\n",
        "\n",
        "def t_test_decoded_muted_vs_non_muted(decoded_with_patch_muted, decoded_with_patch_non_muted):\n",
        "    t_stat_decoded, p_value_decoded = ttest_ind(decoded_with_patch_muted, decoded_with_patch_non_muted, equal_var=False)\n",
        "    print(f\"Decoded Activations T-Test:\\n  T-statistic: {t_stat_decoded}, P-value: {p_value_decoded}\")\n",
        "    return t_stat_decoded, p_value_decoded\n",
        "\n",
        "# Function 12 to classify decoded activations\n",
        "def classify_decoded_activations(model, decoded_activations):\n",
        "    \"\"\"Classify decoded activations using the softmax layer of the model.\"\"\"\n",
        "    predictions = []\n",
        "    for activation in decoded_activations:\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Function 13: Perform and print formatted t-tests for layer activations\n",
        "def perform_and_format_t_tests(condition1_activations, condition2_activations, layer_name=\"Layer\"):\n",
        "    # Perform two-sample t-test\n",
        "    t_stat, p_values = ttest_ind(condition1_activations, condition2_activations, axis=0, equal_var=False)\n",
        "\n",
        "    # Bonferroni correction\n",
        "    num_neurons = condition1_activations.shape[1]\n",
        "    adjusted_p_values = np.minimum(p_values * num_neurons, 1.0)\n",
        "\n",
        "    # Calculate mean activations for each neuron\n",
        "    condition1_mean = np.mean(condition1_activations, axis=0)\n",
        "    condition2_mean = np.mean(condition2_activations, axis=0)\n",
        "\n",
        "    # Calculate and display the percentage of neurons with significant p-values\n",
        "    raw_significant_0_05 = np.mean(p_values <= 0.05) * 100\n",
        "    raw_significant_0_02 = np.mean(p_values <= 0.02) * 100\n",
        "    corrected_significant_0_05 = np.mean(adjusted_p_values <= 0.05) * 100\n",
        "    corrected_significant_0_02 = np.mean(adjusted_p_values <= 0.02) * 100\n",
        "\n",
        "    print(f\"{layer_name}:\")\n",
        "    print(\"  Condition 1 Mean Activation (Muted):\")\n",
        "    print(f\"    Mean across neurons: {condition1_mean.mean():.4f}\")\n",
        "    print(\"  Condition 2 Mean Activation (Non-Muted):\")\n",
        "    print(f\"    Mean across neurons: {condition2_mean.mean():.4f}\")\n",
        "    print(\"  T-Test (before Bonferroni correction):\")\n",
        "    print(f\"    Percentage of neurons with raw p-value <= 0.05: {raw_significant_0_05:.2f}%\")\n",
        "    print(f\"    Percentage of neurons with raw p-value <= 0.02: {raw_significant_0_02:.2f}%\")\n",
        "    print(\"  T-Test (after Bonferroni correction):\")\n",
        "    print(f\"    Percentage of neurons with adjusted p-value <= 0.05: {corrected_significant_0_05:.2f}%\")\n",
        "    print(f\"    Percentage of neurons with adjusted p-value <= 0.02: {corrected_significant_0_02:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "'''\n",
        "def classify_with_alexnet(model, dataloader):\n",
        "    print(\"Classifying test images using the loaded AlexNet model...\")\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, label in dataloader:\n",
        "            images = images.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            # Forward pass through AlexNet\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            labels.extend(label.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    print(f\"Accuracy of the AlexNet model on the test set (with patch): {accuracy:.2f}\")\n",
        "    return accuracy\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "zQQ5POZ2_xzI",
        "outputId": "b41cd56a-830a-4ca4-9682-d4ffe46ff024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef classify_with_alexnet(model, dataloader):\\n    print(\"Classifying test images using the loaded AlexNet model...\")\\n    predictions = []\\n    labels = []\\n\\n    with torch.no_grad():\\n        for images, label in dataloader:\\n            images = images.to(device)\\n            label = label.to(device)\\n\\n            # Forward pass through AlexNet\\n            outputs = model(images)\\n            _, predicted = torch.max(outputs, 1)\\n\\n            predictions.extend(predicted.cpu().numpy())\\n            labels.extend(label.cpu().numpy())\\n\\n    # Calculate accuracy\\n    accuracy = accuracy_score(labels, predictions)\\n    print(f\"Accuracy of the AlexNet model on the test set (with patch): {accuracy:.2f}\")\\n    return accuracy\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Paths and initialization\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt\"\n",
        "    patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/background/test/class_2'\n",
        "    no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "    # Load model and autoencoder\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Prepare dataloaders\n",
        "    patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    no_patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(no_patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    patch_dataset = MnistDataset(patch_image_paths, is_two=1)\n",
        "    no_patch_dataset = MnistDataset(no_patch_image_paths, is_two=1)\n",
        "\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "    no_patch_loader = DataLoader(no_patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "    # Load or extract fc2 activations\n",
        "    activations_patch = load_or_extract_fc2_activations(model, patch_loader, 'test_patch', 'bg_fc2_activations_patch')\n",
        "    activations_no_patch = load_or_extract_fc2_activations(model, no_patch_loader, 'test_no_patch', 'bg_fc2_activations_no_patch')\n",
        "\n",
        "    # Directory for saving results\n",
        "    sparse_output_dir = os.path.join(output_base_dir, \"bg_fc2_sparse_outputs\")\n",
        "    Path(sparse_output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Calculate activations and save outputs\n",
        "    avg_activations_patch = calculate_neuron_activations(autoencoder, activations_patch, sparse_output_dir, \"patch\")\n",
        "    avg_activations_no_patch = calculate_neuron_activations(autoencoder, activations_no_patch, sparse_output_dir, \"no_patch\")\n",
        "\n",
        "    # Calculate differences and get top neurons\n",
        "    abs_diff = calculate_neuron_differences(avg_activations_patch, avg_activations_no_patch, sparse_output_dir)\n",
        "    max_neurons = 4096\n",
        "    top_neurons = get_top_neurons(abs_diff, sparse_output_dir)\n",
        "\n",
        "    # Visualize top neuron differences\n",
        "    #visualize_binned_neuron_differences(abs_diff, top_neurons, bin_width=0.05)\n",
        "\n",
        "\n",
        "    # Classify with and without muting, then evaluate\n",
        "    # predictions with muting result of activations_patch (two patch)\n",
        "    predictions_patch_with_muting = classify_with_muted_neurons(autoencoder, model, activations_patch, top_neurons)\n",
        "    predictions_patch_without_muting = classify_without_muting(autoencoder, model, activations_patch)\n",
        "    evaluate_muting_effect(predictions_patch_with_muting, predictions_patch_without_muting)\n",
        "\n",
        "    # Evaluate classification metrics\n",
        "    labels = [1] * len(predictions_patch_with_muting)  # All images are class 2\n",
        "    evaluate_classification_metrics(predictions_patch_with_muting, predictions_patch_without_muting, labels)\n",
        "    print('Elmafrood tala3 7aga')\n",
        "\n",
        "    # Project into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_patch[:, top_neurons] = 0\n",
        "    # Ensure we have decoded versions if needed for t-tests on decoded activations\n",
        "    decoded_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_patch_non_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_no_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_no_patch_non_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Perform t-tests on sparse activations\n",
        "    #print(\"Performing T-Tests on Sparse Activations:\")\n",
        "    #perform_and_format_t_tests(projected_patch, projected_no_patch, layer_name=\"Sparse Activations\")\n",
        "\n",
        "    # Perform t-tests on decoded activations\n",
        "    #print(\"Performing T-Tests on Decoded Activations (Muted vs Non-Muted):\")\n",
        "    #perform_and_format_t_tests(decoded_with_patch_muted, decoded_with_patch_non_muted, layer_name=\"Decoded Activations\")\n",
        "\n",
        "    # Classify decoded activations for images with no patch in sparse space without muting\n",
        "    print(\"\\nClassifying decoded activations for 'two with no patch' after projecting into sparse space and decoding without muting...\")\n",
        "    predictions_no_patch_non_muted_decoded = classify_decoded_activations(model, decoded_no_patch_non_muted)\n",
        "\n",
        "    # Classify decoded activations for images with patch in sparse space with muting\n",
        "    print(\"\\nClassifying decoded activations for 'two with patch' after projecting into sparse space and decoding with muting...\")\n",
        "    predictions_patch_decoded_muting = classify_decoded_activations(model, decoded_patch_muted)\n",
        "\n",
        "    # Classify decoded activations for images with patch in sparse space without muting\n",
        "    print(\"\\nClassifying decoded activations for 'two with patch' after projecting into sparse space and decoding without muting...\")\n",
        "    predictions_patch_decoded_non_muting = classify_decoded_activations(model, decoded_patch_non_muted)\n",
        "\n",
        "    # Calculate and print accuracy for 'two with no patch' decoded activations\n",
        "    labels_no_patch = [1] * len(predictions_no_patch_non_muted_decoded)  # All images are labeled as class 2\n",
        "    accuracy_no_patch_non_muting_decoded = accuracy_score(labels_no_patch, predictions_no_patch_non_muted_decoded)\n",
        "    print(f\"Accuracy of 'two with no patch' decoded activations after sparse projection and no muting: {accuracy_no_patch_non_muting_decoded:.5f}\")\n",
        "\n",
        "    # Calculate and print accuracy for 'two with patch' decoded activations without muting\n",
        "    labels_patch = [1] * len(predictions_patch_decoded_non_muting)  # All images are labeled as class 2\n",
        "    accuracy_patch_decoded_non_muting = accuracy_score(labels_patch, predictions_patch_decoded_non_muting)\n",
        "    print(f\"Accuracy of 'two with patch' decoded activations after sparse projection and no muting: {accuracy_patch_decoded_non_muting:.5f}\")\n",
        "\n",
        "\n",
        "    # Calculate and print accuracy for 'two with patch' decoded activations with muting\n",
        "    labels_no_patch = [1] * len(predictions_patch_decoded_muting)  # All images are labeled as class 2\n",
        "    accuracy_patch_decoded_muting = accuracy_score(labels_no_patch, predictions_patch_decoded_muting)\n",
        "    print(f\"Accuracy of 'two with patch' decoded activations after sparse projection and muting: {accuracy_patch_decoded_muting:.5f}\")\n",
        "\n",
        "    print(\"Top neurons and their differences:\", abs_diff.loc[top_neurons])\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Prepare the dataloader for patched images\n",
        "    patch_image_paths = [os.path.join(root, file)\n",
        "                        for root, dirs, files in os.walk(patch_folder)\n",
        "                        for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    patch_dataset = ImageDataset(patch_image_paths, transform=preprocess)\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Wrap the dataloader with a fixed label of 1\n",
        "    labeled_patch_loader = LabeledDataLoaderWrapper(patch_loader, label=0)\n",
        "\n",
        "    # Classify using the original model\n",
        "    classify_with_alexnet(model, labeled_patch_loader)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldNEN2kn_8UZ",
        "outputId": "99dc9e14-be14-4d29-a753-b01691a427fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-df6ee770a840>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and layers up to fc2 are frozen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-99d18b6783c5>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(save_sae_dir))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder loaded and frozen successfully\n",
            "No pre-saved Alexnet activations found for bg_fc2_activations_patch. Extracting and saving...\n",
            "Extracting Alexnet activations for layer fc2...\n",
            "Processed 1 images\n",
            "Processed 2 images\n",
            "Processed 3 images\n",
            "Processed 4 images\n",
            "Processed 5 images\n",
            "Processed 6 images\n",
            "Processed 7 images\n",
            "Processed 8 images\n",
            "Processed 9 images\n",
            "Processed 10 images\n",
            "Processed 11 images\n",
            "Processed 12 images\n",
            "Processed 13 images\n",
            "Processed 14 images\n",
            "Processed 15 images\n",
            "Processed 16 images\n",
            "Processed 17 images\n",
            "Processed 18 images\n",
            "Processed 19 images\n",
            "Processed 20 images\n",
            "Processed 21 images\n",
            "Processed 22 images\n",
            "Processed 23 images\n",
            "Processed 24 images\n",
            "Processed 25 images\n",
            "Processed 26 images\n",
            "Processed 27 images\n",
            "Processed 28 images\n",
            "Processed 29 images\n",
            "Processed 30 images\n",
            "Processed 31 images\n",
            "Processed 32 images\n",
            "Processed 33 images\n",
            "Processed 34 images\n",
            "Processed 35 images\n",
            "Processed 36 images\n",
            "Processed 37 images\n",
            "Processed 38 images\n",
            "Processed 39 images\n",
            "Processed 40 images\n",
            "Processed 41 images\n",
            "Processed 42 images\n",
            "Processed 43 images\n",
            "Processed 44 images\n",
            "Processed 45 images\n",
            "Processed 46 images\n",
            "Processed 47 images\n",
            "Processed 48 images\n",
            "Processed 49 images\n",
            "Processed 50 images\n",
            "Processed 51 images\n",
            "Processed 52 images\n",
            "Processed 53 images\n",
            "Processed 54 images\n",
            "Processed 55 images\n",
            "Processed 56 images\n",
            "Processed 57 images\n",
            "Processed 58 images\n",
            "Processed 59 images\n",
            "Processed 60 images\n",
            "Processed 61 images\n",
            "Processed 62 images\n",
            "Processed 63 images\n",
            "Processed 64 images\n",
            "Processed 65 images\n",
            "Processed 66 images\n",
            "Processed 67 images\n",
            "Processed 68 images\n",
            "Processed 69 images\n",
            "Processed 70 images\n",
            "Processed 71 images\n",
            "Processed 72 images\n",
            "Processed 73 images\n",
            "Processed 74 images\n",
            "Processed 75 images\n",
            "Processed 76 images\n",
            "Processed 77 images\n",
            "Processed 78 images\n",
            "Processed 79 images\n",
            "Processed 80 images\n",
            "Processed 81 images\n",
            "Processed 82 images\n",
            "Processed 83 images\n",
            "Processed 84 images\n",
            "Processed 85 images\n",
            "Processed 86 images\n",
            "Processed 87 images\n",
            "Processed 88 images\n",
            "Processed 89 images\n",
            "Processed 90 images\n",
            "Processed 91 images\n",
            "Processed 92 images\n",
            "Processed 93 images\n",
            "Processed 94 images\n",
            "Processed 95 images\n",
            "Processed 96 images\n",
            "Processed 97 images\n",
            "Processed 98 images\n",
            "Processed 99 images\n",
            "Processed 100 images\n",
            "Processed 101 images\n",
            "Processed 102 images\n",
            "Processed 103 images\n",
            "Processed 104 images\n",
            "Processed 105 images\n",
            "Processed 106 images\n",
            "Processed 107 images\n",
            "Processed 108 images\n",
            "Processed 109 images\n",
            "Processed 110 images\n",
            "Processed 111 images\n",
            "Processed 112 images\n",
            "Processed 113 images\n",
            "Processed 114 images\n",
            "Processed 115 images\n",
            "Processed 116 images\n",
            "Processed 117 images\n",
            "Processed 118 images\n",
            "Processed 119 images\n",
            "Processed 120 images\n",
            "Processed 121 images\n",
            "Processed 122 images\n",
            "Processed 123 images\n",
            "Processed 124 images\n",
            "Processed 125 images\n",
            "Processed 126 images\n",
            "Processed 127 images\n",
            "Processed 128 images\n",
            "Processed 129 images\n",
            "Processed 130 images\n",
            "Processed 131 images\n",
            "Processed 132 images\n",
            "Processed 133 images\n",
            "Processed 134 images\n",
            "Processed 135 images\n",
            "Processed 136 images\n",
            "Processed 137 images\n",
            "Processed 138 images\n",
            "Processed 139 images\n",
            "Processed 140 images\n",
            "Processed 141 images\n",
            "Processed 142 images\n",
            "Processed 143 images\n",
            "Processed 144 images\n",
            "Processed 145 images\n",
            "Processed 146 images\n",
            "Processed 147 images\n",
            "Processed 148 images\n",
            "Processed 149 images\n",
            "Processed 150 images\n",
            "Processed 151 images\n",
            "Processed 152 images\n",
            "Processed 153 images\n",
            "Processed 154 images\n",
            "Processed 155 images\n",
            "Processed 156 images\n",
            "Processed 157 images\n",
            "Processed 158 images\n",
            "Processed 159 images\n",
            "Processed 160 images\n",
            "Processed 161 images\n",
            "Processed 162 images\n",
            "Processed 163 images\n",
            "Processed 164 images\n",
            "Processed 165 images\n",
            "Processed 166 images\n",
            "Processed 167 images\n",
            "Processed 168 images\n",
            "Processed 169 images\n",
            "Processed 170 images\n",
            "Processed 171 images\n",
            "Processed 172 images\n",
            "Processed 173 images\n",
            "Processed 174 images\n",
            "Processed 175 images\n",
            "Processed 176 images\n",
            "Processed 177 images\n",
            "Processed 178 images\n",
            "Processed 179 images\n",
            "Processed 180 images\n",
            "Processed 181 images\n",
            "Processed 182 images\n",
            "Processed 183 images\n",
            "Processed 184 images\n",
            "Processed 185 images\n",
            "Processed 186 images\n",
            "Processed 187 images\n",
            "Processed 188 images\n",
            "Processed 189 images\n",
            "Processed 190 images\n",
            "Processed 191 images\n",
            "Processed 192 images\n",
            "Processed 193 images\n",
            "Processed 194 images\n",
            "Processed 195 images\n",
            "Processed 196 images\n",
            "Processed 197 images\n",
            "Processed 198 images\n",
            "Processed 199 images\n",
            "Processed 200 images\n",
            "Processed 201 images\n",
            "Processed 202 images\n",
            "Processed 203 images\n",
            "Processed 204 images\n",
            "Processed 205 images\n",
            "Processed 206 images\n",
            "Processed 207 images\n",
            "Processed 208 images\n",
            "Processed 209 images\n",
            "Processed 210 images\n",
            "Processed 211 images\n",
            "Processed 212 images\n",
            "Processed 213 images\n",
            "Processed 214 images\n",
            "Processed 215 images\n",
            "Processed 216 images\n",
            "Processed 217 images\n",
            "Processed 218 images\n",
            "Processed 219 images\n",
            "Processed 220 images\n",
            "Processed 221 images\n",
            "Processed 222 images\n",
            "Processed 223 images\n",
            "Processed 224 images\n",
            "Processed 225 images\n",
            "Processed 226 images\n",
            "Processed 227 images\n",
            "Processed 228 images\n",
            "Processed 229 images\n",
            "Processed 230 images\n",
            "Processed 231 images\n",
            "Processed 232 images\n",
            "Processed 233 images\n",
            "Processed 234 images\n",
            "Processed 235 images\n",
            "Processed 236 images\n",
            "Processed 237 images\n",
            "Processed 238 images\n",
            "Processed 239 images\n",
            "Processed 240 images\n",
            "Processed 241 images\n",
            "Processed 242 images\n",
            "Processed 243 images\n",
            "Processed 244 images\n",
            "Processed 245 images\n",
            "Processed 246 images\n",
            "Processed 247 images\n",
            "Processed 248 images\n",
            "Processed 249 images\n",
            "Processed 250 images\n",
            "Processed 251 images\n",
            "Processed 252 images\n",
            "Processed 253 images\n",
            "Processed 254 images\n",
            "Processed 255 images\n",
            "Processed 256 images\n",
            "Processed 257 images\n",
            "Processed 258 images\n",
            "Processed 259 images\n",
            "Processed 260 images\n",
            "Processed 261 images\n",
            "Processed 262 images\n",
            "Processed 263 images\n",
            "Processed 264 images\n",
            "Processed 265 images\n",
            "Processed 266 images\n",
            "Processed 267 images\n",
            "Processed 268 images\n",
            "Processed 269 images\n",
            "Processed 270 images\n",
            "Processed 271 images\n",
            "Processed 272 images\n",
            "Processed 273 images\n",
            "Processed 274 images\n",
            "Processed 275 images\n",
            "Processed 276 images\n",
            "Processed 277 images\n",
            "Processed 278 images\n",
            "Processed 279 images\n",
            "Processed 280 images\n",
            "Processed 281 images\n",
            "Processed 282 images\n",
            "Processed 283 images\n",
            "Processed 284 images\n",
            "Processed 285 images\n",
            "Processed 286 images\n",
            "Processed 287 images\n",
            "Processed 288 images\n",
            "Processed 289 images\n",
            "Processed 290 images\n",
            "Processed 291 images\n",
            "Processed 292 images\n",
            "Processed 293 images\n",
            "Processed 294 images\n",
            "Processed 295 images\n",
            "Processed 296 images\n",
            "Processed 297 images\n",
            "Processed 298 images\n",
            "Processed 299 images\n",
            "Processed 300 images\n",
            "Processed 301 images\n",
            "Processed 302 images\n",
            "Processed 303 images\n",
            "Processed 304 images\n",
            "Processed 305 images\n",
            "Processed 306 images\n",
            "Processed 307 images\n",
            "Processed 308 images\n",
            "Processed 309 images\n",
            "Processed 310 images\n",
            "Processed 311 images\n",
            "Processed 312 images\n",
            "Processed 313 images\n",
            "Processed 314 images\n",
            "Processed 315 images\n",
            "Processed 316 images\n",
            "Processed 317 images\n",
            "Processed 318 images\n",
            "Processed 319 images\n",
            "Processed 320 images\n",
            "Processed 321 images\n",
            "Processed 322 images\n",
            "Processed 323 images\n",
            "Processed 324 images\n",
            "Processed 325 images\n",
            "Processed 326 images\n",
            "Processed 327 images\n",
            "Processed 328 images\n",
            "Processed 329 images\n",
            "Processed 330 images\n",
            "Processed 331 images\n",
            "Processed 332 images\n",
            "Processed 333 images\n",
            "Processed 334 images\n",
            "Processed 335 images\n",
            "Processed 336 images\n",
            "Processed 337 images\n",
            "Processed 338 images\n",
            "Processed 339 images\n",
            "Processed 340 images\n",
            "Processed 341 images\n",
            "Processed 342 images\n",
            "Processed 343 images\n",
            "Processed 344 images\n",
            "Processed 345 images\n",
            "Processed 346 images\n",
            "Processed 347 images\n",
            "Processed 348 images\n",
            "Processed 349 images\n",
            "Processed 350 images\n",
            "Processed 351 images\n",
            "Processed 352 images\n",
            "Processed 353 images\n",
            "Processed 354 images\n",
            "Processed 355 images\n",
            "Processed 356 images\n",
            "Processed 357 images\n",
            "Processed 358 images\n",
            "Processed 359 images\n",
            "Processed 360 images\n",
            "Processed 361 images\n",
            "Processed 362 images\n",
            "Processed 363 images\n",
            "Processed 364 images\n",
            "Processed 365 images\n",
            "Processed 366 images\n",
            "Processed 367 images\n",
            "Processed 368 images\n",
            "Processed 369 images\n",
            "Processed 370 images\n",
            "Processed 371 images\n",
            "Processed 372 images\n",
            "Processed 373 images\n",
            "Processed 374 images\n",
            "Processed 375 images\n",
            "Processed 376 images\n",
            "Processed 377 images\n",
            "Processed 378 images\n",
            "Processed 379 images\n",
            "Processed 380 images\n",
            "Processed 381 images\n",
            "Processed 382 images\n",
            "Processed 383 images\n",
            "Processed 384 images\n",
            "Processed 385 images\n",
            "Processed 386 images\n",
            "Processed 387 images\n",
            "Processed 388 images\n",
            "Processed 389 images\n",
            "Processed 390 images\n",
            "Processed 391 images\n",
            "Processed 392 images\n",
            "Processed 393 images\n",
            "Processed 394 images\n",
            "Processed 395 images\n",
            "Processed 396 images\n",
            "Processed 397 images\n",
            "Processed 398 images\n",
            "Processed 399 images\n",
            "Processed 400 images\n",
            "Processed 401 images\n",
            "Processed 402 images\n",
            "Processed 403 images\n",
            "Processed 404 images\n",
            "Processed 405 images\n",
            "Processed 406 images\n",
            "Processed 407 images\n",
            "Processed 408 images\n",
            "Processed 409 images\n",
            "Processed 410 images\n",
            "Processed 411 images\n",
            "Processed 412 images\n",
            "Processed 413 images\n",
            "Processed 414 images\n",
            "Processed 415 images\n",
            "Processed 416 images\n",
            "Processed 417 images\n",
            "Processed 418 images\n",
            "Processed 419 images\n",
            "Processed 420 images\n",
            "Processed 421 images\n",
            "Processed 422 images\n",
            "Processed 423 images\n",
            "Processed 424 images\n",
            "Processed 425 images\n",
            "Processed 426 images\n",
            "Processed 427 images\n",
            "Processed 428 images\n",
            "Processed 429 images\n",
            "Processed 430 images\n",
            "Processed 431 images\n",
            "Processed 432 images\n",
            "Processed 433 images\n",
            "Processed 434 images\n",
            "Processed 435 images\n",
            "Processed 436 images\n",
            "Processed 437 images\n",
            "Processed 438 images\n",
            "Processed 439 images\n",
            "Processed 440 images\n",
            "Processed 441 images\n",
            "Processed 442 images\n",
            "Processed 443 images\n",
            "Processed 444 images\n",
            "Processed 445 images\n",
            "Processed 446 images\n",
            "Processed 447 images\n",
            "Processed 448 images\n",
            "Processed 449 images\n",
            "Processed 450 images\n",
            "Processed 451 images\n",
            "Processed 452 images\n",
            "Processed 453 images\n",
            "Processed 454 images\n",
            "Processed 455 images\n",
            "Processed 456 images\n",
            "Processed 457 images\n",
            "Processed 458 images\n",
            "Processed 459 images\n",
            "Processed 460 images\n",
            "Processed 461 images\n",
            "Processed 462 images\n",
            "Processed 463 images\n",
            "Processed 464 images\n",
            "Processed 465 images\n",
            "Processed 466 images\n",
            "Processed 467 images\n",
            "Processed 468 images\n",
            "Processed 469 images\n",
            "Processed 470 images\n",
            "Processed 471 images\n",
            "Processed 472 images\n",
            "Processed 473 images\n",
            "Processed 474 images\n",
            "Processed 475 images\n",
            "Processed 476 images\n",
            "Processed 477 images\n",
            "Processed 478 images\n",
            "Processed 479 images\n",
            "Processed 480 images\n",
            "Processed 481 images\n",
            "Processed 482 images\n",
            "Processed 483 images\n",
            "Processed 484 images\n",
            "Processed 485 images\n",
            "Processed 486 images\n",
            "Processed 487 images\n",
            "Processed 488 images\n",
            "Processed 489 images\n",
            "Processed 490 images\n",
            "Processed 491 images\n",
            "Processed 492 images\n",
            "Processed 493 images\n",
            "Processed 494 images\n",
            "Processed 495 images\n",
            "Processed 496 images\n",
            "Processed 497 images\n",
            "Processed 498 images\n",
            "Processed 499 images\n",
            "Processed 500 images\n",
            "Processed 501 images\n",
            "Processed 502 images\n",
            "Processed 503 images\n",
            "Processed 504 images\n",
            "Processed 505 images\n",
            "Processed 506 images\n",
            "Processed 507 images\n",
            "Processed 508 images\n",
            "Processed 509 images\n",
            "Processed 510 images\n",
            "Processed 511 images\n",
            "Processed 512 images\n",
            "Processed 513 images\n",
            "Processed 514 images\n",
            "Processed 515 images\n",
            "Processed 516 images\n",
            "Processed 517 images\n",
            "Processed 518 images\n",
            "Processed 519 images\n",
            "Processed 520 images\n",
            "Processed 521 images\n",
            "Processed 522 images\n",
            "Processed 523 images\n",
            "Processed 524 images\n",
            "Processed 525 images\n",
            "Processed 526 images\n",
            "Processed 527 images\n",
            "Processed 528 images\n",
            "Processed 529 images\n",
            "Processed 530 images\n",
            "Processed 531 images\n",
            "Processed 532 images\n",
            "Processed 533 images\n",
            "Processed 534 images\n",
            "Processed 535 images\n",
            "Processed 536 images\n",
            "Processed 537 images\n",
            "Processed 538 images\n",
            "Processed 539 images\n",
            "Processed 540 images\n",
            "Processed 541 images\n",
            "Processed 542 images\n",
            "Processed 543 images\n",
            "Processed 544 images\n",
            "Processed 545 images\n",
            "Processed 546 images\n",
            "Processed 547 images\n",
            "Processed 548 images\n",
            "Processed 549 images\n",
            "Processed 550 images\n",
            "Processed 551 images\n",
            "Processed 552 images\n",
            "Processed 553 images\n",
            "Processed 554 images\n",
            "Processed 555 images\n",
            "Processed 556 images\n",
            "Processed 557 images\n",
            "Processed 558 images\n",
            "Processed 559 images\n",
            "Processed 560 images\n",
            "Processed 561 images\n",
            "Processed 562 images\n",
            "Processed 563 images\n",
            "Processed 564 images\n",
            "Processed 565 images\n",
            "Processed 566 images\n",
            "Processed 567 images\n",
            "Processed 568 images\n",
            "Processed 569 images\n",
            "Processed 570 images\n",
            "Processed 571 images\n",
            "Processed 572 images\n",
            "Processed 573 images\n",
            "Processed 574 images\n",
            "Processed 575 images\n",
            "Processed 576 images\n",
            "Processed 577 images\n",
            "Processed 578 images\n",
            "Processed 579 images\n",
            "Processed 580 images\n",
            "Processed 581 images\n",
            "Processed 582 images\n",
            "Processed 583 images\n",
            "Processed 584 images\n",
            "Processed 585 images\n",
            "Processed 586 images\n",
            "Processed 587 images\n",
            "Processed 588 images\n",
            "Processed 589 images\n",
            "Processed 590 images\n",
            "Processed 591 images\n",
            "Processed 592 images\n",
            "Processed 593 images\n",
            "Processed 594 images\n",
            "Processed 595 images\n",
            "Processed 596 images\n",
            "Processed 597 images\n",
            "Processed 598 images\n",
            "Processed 599 images\n",
            "Processed 600 images\n",
            "Processed 601 images\n",
            "Processed 602 images\n",
            "Processed 603 images\n",
            "Processed 604 images\n",
            "Processed 605 images\n",
            "Processed 606 images\n",
            "Processed 607 images\n",
            "Processed 608 images\n",
            "Processed 609 images\n",
            "Processed 610 images\n",
            "Processed 611 images\n",
            "Processed 612 images\n",
            "Processed 613 images\n",
            "Processed 614 images\n",
            "Processed 615 images\n",
            "Processed 616 images\n",
            "Processed 617 images\n",
            "Processed 618 images\n",
            "Processed 619 images\n",
            "Processed 620 images\n",
            "Processed 621 images\n",
            "Processed 622 images\n",
            "Processed 623 images\n",
            "Processed 624 images\n",
            "Processed 625 images\n",
            "Processed 626 images\n",
            "Processed 627 images\n",
            "Processed 628 images\n",
            "Processed 629 images\n",
            "Processed 630 images\n",
            "Processed 631 images\n",
            "Processed 632 images\n",
            "Processed 633 images\n",
            "Processed 634 images\n",
            "Processed 635 images\n",
            "Processed 636 images\n",
            "Processed 637 images\n",
            "Processed 638 images\n",
            "Processed 639 images\n",
            "Processed 640 images\n",
            "Processed 641 images\n",
            "Processed 642 images\n",
            "Processed 643 images\n",
            "Processed 644 images\n",
            "Processed 645 images\n",
            "Processed 646 images\n",
            "Processed 647 images\n",
            "Processed 648 images\n",
            "Processed 649 images\n",
            "Processed 650 images\n",
            "Processed 651 images\n",
            "Processed 652 images\n",
            "Processed 653 images\n",
            "Processed 654 images\n",
            "Processed 655 images\n",
            "Processed 656 images\n",
            "Processed 657 images\n",
            "Processed 658 images\n",
            "Processed 659 images\n",
            "Processed 660 images\n",
            "Processed 661 images\n",
            "Processed 662 images\n",
            "Processed 663 images\n",
            "Processed 664 images\n",
            "Processed 665 images\n",
            "Processed 666 images\n",
            "Processed 667 images\n",
            "Processed 668 images\n",
            "Processed 669 images\n",
            "Processed 670 images\n",
            "Processed 671 images\n",
            "Processed 672 images\n",
            "Processed 673 images\n",
            "Processed 674 images\n",
            "Processed 675 images\n",
            "Processed 676 images\n",
            "Processed 677 images\n",
            "Processed 678 images\n",
            "Processed 679 images\n",
            "Processed 680 images\n",
            "Processed 681 images\n",
            "Processed 682 images\n",
            "Processed 683 images\n",
            "Processed 684 images\n",
            "Processed 685 images\n",
            "Processed 686 images\n",
            "Processed 687 images\n",
            "Processed 688 images\n",
            "Processed 689 images\n",
            "Processed 690 images\n",
            "Processed 691 images\n",
            "Processed 692 images\n",
            "Processed 693 images\n",
            "Processed 694 images\n",
            "Processed 695 images\n",
            "Processed 696 images\n",
            "Processed 697 images\n",
            "Processed 698 images\n",
            "Processed 699 images\n",
            "Processed 700 images\n",
            "Processed 701 images\n",
            "Processed 702 images\n",
            "Processed 703 images\n",
            "Processed 704 images\n",
            "Processed 705 images\n",
            "Processed 706 images\n",
            "Processed 707 images\n",
            "Processed 708 images\n",
            "Processed 709 images\n",
            "Processed 710 images\n",
            "Processed 711 images\n",
            "Processed 712 images\n",
            "Processed 713 images\n",
            "Processed 714 images\n",
            "Processed 715 images\n",
            "Processed 716 images\n",
            "Processed 717 images\n",
            "Processed 718 images\n",
            "Processed 719 images\n",
            "Processed 720 images\n",
            "Processed 721 images\n",
            "Processed 722 images\n",
            "Processed 723 images\n",
            "Processed 724 images\n",
            "Processed 725 images\n",
            "Processed 726 images\n",
            "Processed 727 images\n",
            "Processed 728 images\n",
            "Processed 729 images\n",
            "Processed 730 images\n",
            "Processed 731 images\n",
            "Processed 732 images\n",
            "Processed 733 images\n",
            "Processed 734 images\n",
            "Processed 735 images\n",
            "Processed 736 images\n",
            "Processed 737 images\n",
            "Processed 738 images\n",
            "Processed 739 images\n",
            "Processed 740 images\n",
            "Processed 741 images\n",
            "Processed 742 images\n",
            "Processed 743 images\n",
            "Processed 744 images\n",
            "Processed 745 images\n",
            "Processed 746 images\n",
            "Processed 747 images\n",
            "Processed 748 images\n",
            "Processed 749 images\n",
            "Processed 750 images\n",
            "Processed 751 images\n",
            "Processed 752 images\n",
            "Processed 753 images\n",
            "Processed 754 images\n",
            "Processed 755 images\n",
            "Processed 756 images\n",
            "Processed 757 images\n",
            "Processed 758 images\n",
            "Processed 759 images\n",
            "Processed 760 images\n",
            "Processed 761 images\n",
            "Processed 762 images\n",
            "Processed 763 images\n",
            "Processed 764 images\n",
            "Processed 765 images\n",
            "Processed 766 images\n",
            "Processed 767 images\n",
            "Processed 768 images\n",
            "Processed 769 images\n",
            "Processed 770 images\n",
            "Processed 771 images\n",
            "Processed 772 images\n",
            "Processed 773 images\n",
            "Processed 774 images\n",
            "Processed 775 images\n",
            "Processed 776 images\n",
            "Processed 777 images\n",
            "Processed 778 images\n",
            "Processed 779 images\n",
            "Processed 780 images\n",
            "Processed 781 images\n",
            "Processed 782 images\n",
            "Processed 783 images\n",
            "Processed 784 images\n",
            "Processed 785 images\n",
            "Processed 786 images\n",
            "Processed 787 images\n",
            "Processed 788 images\n",
            "Processed 789 images\n",
            "Processed 790 images\n",
            "Processed 791 images\n",
            "Processed 792 images\n",
            "Processed 793 images\n",
            "Processed 794 images\n",
            "Processed 795 images\n",
            "Processed 796 images\n",
            "Processed 797 images\n",
            "Processed 798 images\n",
            "Processed 799 images\n",
            "Processed 800 images\n",
            "Processed 801 images\n",
            "Processed 802 images\n",
            "Processed 803 images\n",
            "Processed 804 images\n",
            "Processed 805 images\n",
            "Processed 806 images\n",
            "Processed 807 images\n",
            "Processed 808 images\n",
            "Processed 809 images\n",
            "Processed 810 images\n",
            "Processed 811 images\n",
            "Processed 812 images\n",
            "Processed 813 images\n",
            "Processed 814 images\n",
            "Processed 815 images\n",
            "Processed 816 images\n",
            "Processed 817 images\n",
            "Processed 818 images\n",
            "Processed 819 images\n",
            "Processed 820 images\n",
            "Processed 821 images\n",
            "Processed 822 images\n",
            "Processed 823 images\n",
            "Processed 824 images\n",
            "Processed 825 images\n",
            "Processed 826 images\n",
            "Processed 827 images\n",
            "Processed 828 images\n",
            "Processed 829 images\n",
            "Processed 830 images\n",
            "Processed 831 images\n",
            "Processed 832 images\n",
            "Processed 833 images\n",
            "Processed 834 images\n",
            "Processed 835 images\n",
            "Processed 836 images\n",
            "Processed 837 images\n",
            "Processed 838 images\n",
            "Processed 839 images\n",
            "Processed 840 images\n",
            "Processed 841 images\n",
            "Processed 842 images\n",
            "Processed 843 images\n",
            "Processed 844 images\n",
            "Processed 845 images\n",
            "Processed 846 images\n",
            "Processed 847 images\n",
            "Processed 848 images\n",
            "Processed 849 images\n",
            "Processed 850 images\n",
            "Processed 851 images\n",
            "Processed 852 images\n",
            "Processed 853 images\n",
            "Processed 854 images\n",
            "Processed 855 images\n",
            "Processed 856 images\n",
            "Processed 857 images\n",
            "Processed 858 images\n",
            "Processed 859 images\n",
            "Processed 860 images\n",
            "Processed 861 images\n",
            "Processed 862 images\n",
            "Processed 863 images\n",
            "Processed 864 images\n",
            "Processed 865 images\n",
            "Processed 866 images\n",
            "Processed 867 images\n",
            "Processed 868 images\n",
            "Processed 869 images\n",
            "Processed 870 images\n",
            "Processed 871 images\n",
            "Processed 872 images\n",
            "Processed 873 images\n",
            "Processed 874 images\n",
            "Processed 875 images\n",
            "Processed 876 images\n",
            "Processed 877 images\n",
            "Processed 878 images\n",
            "Processed 879 images\n",
            "Processed 880 images\n",
            "Processed 881 images\n",
            "Processed 882 images\n",
            "Processed 883 images\n",
            "Processed 884 images\n",
            "Processed 885 images\n",
            "Processed 886 images\n",
            "Processed 887 images\n",
            "Processed 888 images\n",
            "Processed 889 images\n",
            "Processed 890 images\n",
            "Processed 891 images\n",
            "Processed 892 images\n",
            "Processed 893 images\n",
            "Processed 894 images\n",
            "Processed 895 images\n",
            "Processed 896 images\n",
            "Processed 897 images\n",
            "Processed 898 images\n",
            "Processed 899 images\n",
            "Processed 900 images\n",
            "Processed 901 images\n",
            "Processed 902 images\n",
            "Processed 903 images\n",
            "Processed 904 images\n",
            "Processed 905 images\n",
            "Processed 906 images\n",
            "Processed 907 images\n",
            "Processed 908 images\n",
            "Processed 909 images\n",
            "Processed 910 images\n",
            "Processed 911 images\n",
            "Processed 912 images\n",
            "Processed 913 images\n",
            "Processed 914 images\n",
            "Processed 915 images\n",
            "Processed 916 images\n",
            "Processed 917 images\n",
            "Processed 918 images\n",
            "Processed 919 images\n",
            "Processed 920 images\n",
            "Processed 921 images\n",
            "Processed 922 images\n",
            "Processed 923 images\n",
            "Processed 924 images\n",
            "Processed 925 images\n",
            "Processed 926 images\n",
            "Processed 927 images\n",
            "Processed 928 images\n",
            "Processed 929 images\n",
            "Processed 930 images\n",
            "Processed 931 images\n",
            "Processed 932 images\n",
            "Processed 933 images\n",
            "Processed 934 images\n",
            "Processed 935 images\n",
            "Processed 936 images\n",
            "Processed 937 images\n",
            "Processed 938 images\n",
            "Processed 939 images\n",
            "Processed 940 images\n",
            "Processed 941 images\n",
            "Processed 942 images\n",
            "Processed 943 images\n",
            "Processed 944 images\n",
            "Processed 945 images\n",
            "Processed 946 images\n",
            "Processed 947 images\n",
            "Processed 948 images\n",
            "Processed 949 images\n",
            "Processed 950 images\n",
            "Processed 951 images\n",
            "Processed 952 images\n",
            "Processed 953 images\n",
            "Processed 954 images\n",
            "Processed 955 images\n",
            "Processed 956 images\n",
            "Processed 957 images\n",
            "Processed 958 images\n",
            "Processed 959 images\n",
            "Processed 960 images\n",
            "Processed 961 images\n",
            "Processed 962 images\n",
            "Processed 963 images\n",
            "Processed 964 images\n",
            "Processed 965 images\n",
            "Processed 966 images\n",
            "Processed 967 images\n",
            "Processed 968 images\n",
            "Processed 969 images\n",
            "Processed 970 images\n",
            "Processed 971 images\n",
            "Processed 972 images\n",
            "Processed 973 images\n",
            "Processed 974 images\n",
            "Processed 975 images\n",
            "Processed 976 images\n",
            "Processed 977 images\n",
            "Processed 978 images\n",
            "Processed 979 images\n",
            "Processed 980 images\n",
            "Processed 981 images\n",
            "Processed 982 images\n",
            "Processed 983 images\n",
            "Processed 984 images\n",
            "Processed 985 images\n",
            "Processed 986 images\n",
            "Processed 987 images\n",
            "Processed 988 images\n",
            "Processed 989 images\n",
            "Processed 990 images\n",
            "Processed 991 images\n",
            "Processed 992 images\n",
            "Processed 993 images\n",
            "Processed 994 images\n",
            "Processed 995 images\n",
            "Processed 996 images\n",
            "Processed 997 images\n",
            "Processed 998 images\n",
            "Processed 999 images\n",
            "Processed 1000 images\n",
            "Processed 1001 images\n",
            "Processed 1002 images\n",
            "Processed 1003 images\n",
            "Processed 1004 images\n",
            "Processed 1005 images\n",
            "Processed 1006 images\n",
            "Processed 1007 images\n",
            "Processed 1008 images\n",
            "Processed 1009 images\n",
            "Processed 1010 images\n",
            "Processed 1011 images\n",
            "Processed 1012 images\n",
            "Processed 1013 images\n",
            "Processed 1014 images\n",
            "Processed 1015 images\n",
            "Processed 1016 images\n",
            "Processed 1017 images\n",
            "Processed 1018 images\n",
            "Processed 1019 images\n",
            "Processed 1020 images\n",
            "Processed 1021 images\n",
            "Processed 1022 images\n",
            "Processed 1023 images\n",
            "Processed 1024 images\n",
            "Processed 1025 images\n",
            "Processed 1026 images\n",
            "Processed 1027 images\n",
            "Processed 1028 images\n",
            "Processed 1029 images\n",
            "Processed 1030 images\n",
            "Processed 1031 images\n",
            "Processed 1032 images\n",
            "Activations for layer fc2 saved to /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/bg_fc2_activations_patch.npy\n",
            "No pre-saved Alexnet activations found for bg_fc2_activations_no_patch. Extracting and saving...\n",
            "Extracting Alexnet activations for layer fc2...\n",
            "Processed 1 images\n",
            "Processed 2 images\n",
            "Processed 3 images\n",
            "Processed 4 images\n",
            "Processed 5 images\n",
            "Processed 6 images\n",
            "Processed 7 images\n",
            "Processed 8 images\n",
            "Processed 9 images\n",
            "Processed 10 images\n",
            "Processed 11 images\n",
            "Processed 12 images\n",
            "Processed 13 images\n",
            "Processed 14 images\n",
            "Processed 15 images\n",
            "Processed 16 images\n",
            "Processed 17 images\n",
            "Processed 18 images\n",
            "Processed 19 images\n",
            "Processed 20 images\n",
            "Processed 21 images\n",
            "Processed 22 images\n",
            "Processed 23 images\n",
            "Processed 24 images\n",
            "Processed 25 images\n",
            "Processed 26 images\n",
            "Processed 27 images\n",
            "Processed 28 images\n",
            "Processed 29 images\n",
            "Processed 30 images\n",
            "Processed 31 images\n",
            "Processed 32 images\n",
            "Processed 33 images\n",
            "Processed 34 images\n",
            "Processed 35 images\n",
            "Processed 36 images\n",
            "Processed 37 images\n",
            "Processed 38 images\n",
            "Processed 39 images\n",
            "Processed 40 images\n",
            "Processed 41 images\n",
            "Processed 42 images\n",
            "Processed 43 images\n",
            "Processed 44 images\n",
            "Processed 45 images\n",
            "Processed 46 images\n",
            "Processed 47 images\n",
            "Processed 48 images\n",
            "Processed 49 images\n",
            "Processed 50 images\n",
            "Processed 51 images\n",
            "Processed 52 images\n",
            "Processed 53 images\n",
            "Processed 54 images\n",
            "Processed 55 images\n",
            "Processed 56 images\n",
            "Processed 57 images\n",
            "Processed 58 images\n",
            "Processed 59 images\n",
            "Processed 60 images\n",
            "Processed 61 images\n",
            "Processed 62 images\n",
            "Processed 63 images\n",
            "Processed 64 images\n",
            "Processed 65 images\n",
            "Processed 66 images\n",
            "Processed 67 images\n",
            "Processed 68 images\n",
            "Processed 69 images\n",
            "Processed 70 images\n",
            "Processed 71 images\n",
            "Processed 72 images\n",
            "Processed 73 images\n",
            "Processed 74 images\n",
            "Processed 75 images\n",
            "Processed 76 images\n",
            "Processed 77 images\n",
            "Processed 78 images\n",
            "Processed 79 images\n",
            "Processed 80 images\n",
            "Processed 81 images\n",
            "Processed 82 images\n",
            "Processed 83 images\n",
            "Processed 84 images\n",
            "Processed 85 images\n",
            "Processed 86 images\n",
            "Processed 87 images\n",
            "Processed 88 images\n",
            "Processed 89 images\n",
            "Processed 90 images\n",
            "Processed 91 images\n",
            "Processed 92 images\n",
            "Processed 93 images\n",
            "Processed 94 images\n",
            "Processed 95 images\n",
            "Processed 96 images\n",
            "Processed 97 images\n",
            "Processed 98 images\n",
            "Processed 99 images\n",
            "Processed 100 images\n",
            "Processed 101 images\n",
            "Processed 102 images\n",
            "Processed 103 images\n",
            "Processed 104 images\n",
            "Processed 105 images\n",
            "Processed 106 images\n",
            "Processed 107 images\n",
            "Processed 108 images\n",
            "Processed 109 images\n",
            "Processed 110 images\n",
            "Processed 111 images\n",
            "Processed 112 images\n",
            "Processed 113 images\n",
            "Processed 114 images\n",
            "Processed 115 images\n",
            "Processed 116 images\n",
            "Processed 117 images\n",
            "Processed 118 images\n",
            "Processed 119 images\n",
            "Processed 120 images\n",
            "Processed 121 images\n",
            "Processed 122 images\n",
            "Processed 123 images\n",
            "Processed 124 images\n",
            "Processed 125 images\n",
            "Processed 126 images\n",
            "Processed 127 images\n",
            "Processed 128 images\n",
            "Processed 129 images\n",
            "Processed 130 images\n",
            "Processed 131 images\n",
            "Processed 132 images\n",
            "Processed 133 images\n",
            "Processed 134 images\n",
            "Processed 135 images\n",
            "Processed 136 images\n",
            "Processed 137 images\n",
            "Processed 138 images\n",
            "Processed 139 images\n",
            "Processed 140 images\n",
            "Processed 141 images\n",
            "Processed 142 images\n",
            "Processed 143 images\n",
            "Processed 144 images\n",
            "Processed 145 images\n",
            "Processed 146 images\n",
            "Processed 147 images\n",
            "Processed 148 images\n",
            "Processed 149 images\n",
            "Processed 150 images\n",
            "Processed 151 images\n",
            "Processed 152 images\n",
            "Processed 153 images\n",
            "Processed 154 images\n",
            "Processed 155 images\n",
            "Processed 156 images\n",
            "Processed 157 images\n",
            "Processed 158 images\n",
            "Processed 159 images\n",
            "Processed 160 images\n",
            "Processed 161 images\n",
            "Processed 162 images\n",
            "Processed 163 images\n",
            "Processed 164 images\n",
            "Processed 165 images\n",
            "Processed 166 images\n",
            "Processed 167 images\n",
            "Processed 168 images\n",
            "Processed 169 images\n",
            "Processed 170 images\n",
            "Processed 171 images\n",
            "Processed 172 images\n",
            "Processed 173 images\n",
            "Processed 174 images\n",
            "Processed 175 images\n",
            "Processed 176 images\n",
            "Processed 177 images\n",
            "Processed 178 images\n",
            "Processed 179 images\n",
            "Processed 180 images\n",
            "Processed 181 images\n",
            "Processed 182 images\n",
            "Processed 183 images\n",
            "Processed 184 images\n",
            "Processed 185 images\n",
            "Processed 186 images\n",
            "Processed 187 images\n",
            "Processed 188 images\n",
            "Processed 189 images\n",
            "Processed 190 images\n",
            "Processed 191 images\n",
            "Processed 192 images\n",
            "Processed 193 images\n",
            "Processed 194 images\n",
            "Processed 195 images\n",
            "Processed 196 images\n",
            "Processed 197 images\n",
            "Processed 198 images\n",
            "Processed 199 images\n",
            "Processed 200 images\n",
            "Processed 201 images\n",
            "Processed 202 images\n",
            "Processed 203 images\n",
            "Processed 204 images\n",
            "Processed 205 images\n",
            "Processed 206 images\n",
            "Processed 207 images\n",
            "Processed 208 images\n",
            "Processed 209 images\n",
            "Processed 210 images\n",
            "Processed 211 images\n",
            "Processed 212 images\n",
            "Processed 213 images\n",
            "Processed 214 images\n",
            "Processed 215 images\n",
            "Processed 216 images\n",
            "Processed 217 images\n",
            "Processed 218 images\n",
            "Processed 219 images\n",
            "Processed 220 images\n",
            "Processed 221 images\n",
            "Processed 222 images\n",
            "Processed 223 images\n",
            "Processed 224 images\n",
            "Processed 225 images\n",
            "Processed 226 images\n",
            "Processed 227 images\n",
            "Processed 228 images\n",
            "Processed 229 images\n",
            "Processed 230 images\n",
            "Processed 231 images\n",
            "Processed 232 images\n",
            "Processed 233 images\n",
            "Processed 234 images\n",
            "Processed 235 images\n",
            "Processed 236 images\n",
            "Processed 237 images\n",
            "Processed 238 images\n",
            "Processed 239 images\n",
            "Processed 240 images\n",
            "Processed 241 images\n",
            "Processed 242 images\n",
            "Processed 243 images\n",
            "Processed 244 images\n",
            "Processed 245 images\n",
            "Processed 246 images\n",
            "Processed 247 images\n",
            "Processed 248 images\n",
            "Processed 249 images\n",
            "Processed 250 images\n",
            "Processed 251 images\n",
            "Processed 252 images\n",
            "Processed 253 images\n",
            "Processed 254 images\n",
            "Processed 255 images\n",
            "Processed 256 images\n",
            "Processed 257 images\n",
            "Processed 258 images\n",
            "Processed 259 images\n",
            "Processed 260 images\n",
            "Processed 261 images\n",
            "Processed 262 images\n",
            "Processed 263 images\n",
            "Processed 264 images\n",
            "Processed 265 images\n",
            "Processed 266 images\n",
            "Processed 267 images\n",
            "Processed 268 images\n",
            "Processed 269 images\n",
            "Processed 270 images\n",
            "Processed 271 images\n",
            "Processed 272 images\n",
            "Processed 273 images\n",
            "Processed 274 images\n",
            "Processed 275 images\n",
            "Processed 276 images\n",
            "Processed 277 images\n",
            "Processed 278 images\n",
            "Processed 279 images\n",
            "Processed 280 images\n",
            "Processed 281 images\n",
            "Processed 282 images\n",
            "Processed 283 images\n",
            "Processed 284 images\n",
            "Processed 285 images\n",
            "Processed 286 images\n",
            "Processed 287 images\n",
            "Processed 288 images\n",
            "Processed 289 images\n",
            "Processed 290 images\n",
            "Processed 291 images\n",
            "Processed 292 images\n",
            "Processed 293 images\n",
            "Processed 294 images\n",
            "Processed 295 images\n",
            "Processed 296 images\n",
            "Processed 297 images\n",
            "Processed 298 images\n",
            "Processed 299 images\n",
            "Processed 300 images\n",
            "Processed 301 images\n",
            "Processed 302 images\n",
            "Processed 303 images\n",
            "Processed 304 images\n",
            "Processed 305 images\n",
            "Processed 306 images\n",
            "Processed 307 images\n",
            "Processed 308 images\n",
            "Processed 309 images\n",
            "Processed 310 images\n",
            "Processed 311 images\n",
            "Processed 312 images\n",
            "Processed 313 images\n",
            "Processed 314 images\n",
            "Processed 315 images\n",
            "Processed 316 images\n",
            "Processed 317 images\n",
            "Processed 318 images\n",
            "Processed 319 images\n",
            "Processed 320 images\n",
            "Processed 321 images\n",
            "Processed 322 images\n",
            "Processed 323 images\n",
            "Processed 324 images\n",
            "Processed 325 images\n",
            "Processed 326 images\n",
            "Processed 327 images\n",
            "Processed 328 images\n",
            "Processed 329 images\n",
            "Processed 330 images\n",
            "Processed 331 images\n",
            "Processed 332 images\n",
            "Processed 333 images\n",
            "Processed 334 images\n",
            "Processed 335 images\n",
            "Processed 336 images\n",
            "Processed 337 images\n",
            "Processed 338 images\n",
            "Processed 339 images\n",
            "Processed 340 images\n",
            "Processed 341 images\n",
            "Processed 342 images\n",
            "Processed 343 images\n",
            "Processed 344 images\n",
            "Processed 345 images\n",
            "Processed 346 images\n",
            "Processed 347 images\n",
            "Processed 348 images\n",
            "Processed 349 images\n",
            "Processed 350 images\n",
            "Processed 351 images\n",
            "Processed 352 images\n",
            "Processed 353 images\n",
            "Processed 354 images\n",
            "Processed 355 images\n",
            "Processed 356 images\n",
            "Processed 357 images\n",
            "Processed 358 images\n",
            "Processed 359 images\n",
            "Processed 360 images\n",
            "Processed 361 images\n",
            "Processed 362 images\n",
            "Processed 363 images\n",
            "Processed 364 images\n",
            "Processed 365 images\n",
            "Processed 366 images\n",
            "Processed 367 images\n",
            "Processed 368 images\n",
            "Processed 369 images\n",
            "Processed 370 images\n",
            "Processed 371 images\n",
            "Processed 372 images\n",
            "Processed 373 images\n",
            "Processed 374 images\n",
            "Processed 375 images\n",
            "Processed 376 images\n",
            "Processed 377 images\n",
            "Processed 378 images\n",
            "Processed 379 images\n",
            "Processed 380 images\n",
            "Processed 381 images\n",
            "Processed 382 images\n",
            "Processed 383 images\n",
            "Processed 384 images\n",
            "Processed 385 images\n",
            "Processed 386 images\n",
            "Processed 387 images\n",
            "Processed 388 images\n",
            "Processed 389 images\n",
            "Processed 390 images\n",
            "Processed 391 images\n",
            "Processed 392 images\n",
            "Processed 393 images\n",
            "Processed 394 images\n",
            "Processed 395 images\n",
            "Processed 396 images\n",
            "Processed 397 images\n",
            "Processed 398 images\n",
            "Processed 399 images\n",
            "Processed 400 images\n",
            "Processed 401 images\n",
            "Processed 402 images\n",
            "Processed 403 images\n",
            "Processed 404 images\n",
            "Processed 405 images\n",
            "Processed 406 images\n",
            "Processed 407 images\n",
            "Processed 408 images\n",
            "Processed 409 images\n",
            "Processed 410 images\n",
            "Processed 411 images\n",
            "Processed 412 images\n",
            "Processed 413 images\n",
            "Processed 414 images\n",
            "Processed 415 images\n",
            "Processed 416 images\n",
            "Processed 417 images\n",
            "Processed 418 images\n",
            "Processed 419 images\n",
            "Processed 420 images\n",
            "Processed 421 images\n",
            "Processed 422 images\n",
            "Processed 423 images\n",
            "Processed 424 images\n",
            "Processed 425 images\n",
            "Processed 426 images\n",
            "Processed 427 images\n",
            "Processed 428 images\n",
            "Processed 429 images\n",
            "Processed 430 images\n",
            "Processed 431 images\n",
            "Processed 432 images\n",
            "Processed 433 images\n",
            "Processed 434 images\n",
            "Processed 435 images\n",
            "Processed 436 images\n",
            "Processed 437 images\n",
            "Processed 438 images\n",
            "Processed 439 images\n",
            "Processed 440 images\n",
            "Processed 441 images\n",
            "Processed 442 images\n",
            "Processed 443 images\n",
            "Processed 444 images\n",
            "Processed 445 images\n",
            "Processed 446 images\n",
            "Processed 447 images\n",
            "Processed 448 images\n",
            "Processed 449 images\n",
            "Processed 450 images\n",
            "Processed 451 images\n",
            "Processed 452 images\n",
            "Processed 453 images\n",
            "Processed 454 images\n",
            "Processed 455 images\n",
            "Processed 456 images\n",
            "Processed 457 images\n",
            "Processed 458 images\n",
            "Processed 459 images\n",
            "Processed 460 images\n",
            "Processed 461 images\n",
            "Processed 462 images\n",
            "Processed 463 images\n",
            "Processed 464 images\n",
            "Processed 465 images\n",
            "Processed 466 images\n",
            "Processed 467 images\n",
            "Processed 468 images\n",
            "Processed 469 images\n",
            "Processed 470 images\n",
            "Processed 471 images\n",
            "Processed 472 images\n",
            "Processed 473 images\n",
            "Processed 474 images\n",
            "Processed 475 images\n",
            "Processed 476 images\n",
            "Processed 477 images\n",
            "Processed 478 images\n",
            "Processed 479 images\n",
            "Processed 480 images\n",
            "Processed 481 images\n",
            "Processed 482 images\n",
            "Processed 483 images\n",
            "Processed 484 images\n",
            "Processed 485 images\n",
            "Processed 486 images\n",
            "Processed 487 images\n",
            "Processed 488 images\n",
            "Processed 489 images\n",
            "Processed 490 images\n",
            "Processed 491 images\n",
            "Processed 492 images\n",
            "Processed 493 images\n",
            "Processed 494 images\n",
            "Processed 495 images\n",
            "Processed 496 images\n",
            "Processed 497 images\n",
            "Processed 498 images\n",
            "Processed 499 images\n",
            "Processed 500 images\n",
            "Processed 501 images\n",
            "Processed 502 images\n",
            "Processed 503 images\n",
            "Processed 504 images\n",
            "Processed 505 images\n",
            "Processed 506 images\n",
            "Processed 507 images\n",
            "Processed 508 images\n",
            "Processed 509 images\n",
            "Processed 510 images\n",
            "Processed 511 images\n",
            "Processed 512 images\n",
            "Processed 513 images\n",
            "Processed 514 images\n",
            "Processed 515 images\n",
            "Processed 516 images\n",
            "Processed 517 images\n",
            "Processed 518 images\n",
            "Processed 519 images\n",
            "Processed 520 images\n",
            "Processed 521 images\n",
            "Processed 522 images\n",
            "Processed 523 images\n",
            "Processed 524 images\n",
            "Processed 525 images\n",
            "Processed 526 images\n",
            "Processed 527 images\n",
            "Processed 528 images\n",
            "Processed 529 images\n",
            "Processed 530 images\n",
            "Processed 531 images\n",
            "Processed 532 images\n",
            "Processed 533 images\n",
            "Processed 534 images\n",
            "Processed 535 images\n",
            "Processed 536 images\n",
            "Processed 537 images\n",
            "Processed 538 images\n",
            "Processed 539 images\n",
            "Processed 540 images\n",
            "Processed 541 images\n",
            "Processed 542 images\n",
            "Processed 543 images\n",
            "Processed 544 images\n",
            "Processed 545 images\n",
            "Processed 546 images\n",
            "Processed 547 images\n",
            "Processed 548 images\n",
            "Processed 549 images\n",
            "Processed 550 images\n",
            "Processed 551 images\n",
            "Processed 552 images\n",
            "Processed 553 images\n",
            "Processed 554 images\n",
            "Processed 555 images\n",
            "Processed 556 images\n",
            "Processed 557 images\n",
            "Processed 558 images\n",
            "Processed 559 images\n",
            "Processed 560 images\n",
            "Processed 561 images\n",
            "Processed 562 images\n",
            "Processed 563 images\n",
            "Processed 564 images\n",
            "Processed 565 images\n",
            "Processed 566 images\n",
            "Processed 567 images\n",
            "Processed 568 images\n",
            "Processed 569 images\n",
            "Processed 570 images\n",
            "Processed 571 images\n",
            "Processed 572 images\n",
            "Processed 573 images\n",
            "Processed 574 images\n",
            "Processed 575 images\n",
            "Processed 576 images\n",
            "Processed 577 images\n",
            "Processed 578 images\n",
            "Processed 579 images\n",
            "Processed 580 images\n",
            "Processed 581 images\n",
            "Processed 582 images\n",
            "Processed 583 images\n",
            "Processed 584 images\n",
            "Processed 585 images\n",
            "Processed 586 images\n",
            "Processed 587 images\n",
            "Processed 588 images\n",
            "Processed 589 images\n",
            "Processed 590 images\n",
            "Processed 591 images\n",
            "Processed 592 images\n",
            "Processed 593 images\n",
            "Processed 594 images\n",
            "Processed 595 images\n",
            "Processed 596 images\n",
            "Processed 597 images\n",
            "Processed 598 images\n",
            "Processed 599 images\n",
            "Processed 600 images\n",
            "Processed 601 images\n",
            "Processed 602 images\n",
            "Processed 603 images\n",
            "Processed 604 images\n",
            "Processed 605 images\n",
            "Processed 606 images\n",
            "Processed 607 images\n",
            "Processed 608 images\n",
            "Processed 609 images\n",
            "Processed 610 images\n",
            "Processed 611 images\n",
            "Processed 612 images\n",
            "Processed 613 images\n",
            "Processed 614 images\n",
            "Processed 615 images\n",
            "Processed 616 images\n",
            "Processed 617 images\n",
            "Processed 618 images\n",
            "Processed 619 images\n",
            "Processed 620 images\n",
            "Processed 621 images\n",
            "Processed 622 images\n",
            "Processed 623 images\n",
            "Processed 624 images\n",
            "Processed 625 images\n",
            "Processed 626 images\n",
            "Processed 627 images\n",
            "Processed 628 images\n",
            "Processed 629 images\n",
            "Processed 630 images\n",
            "Processed 631 images\n",
            "Processed 632 images\n",
            "Processed 633 images\n",
            "Processed 634 images\n",
            "Processed 635 images\n",
            "Processed 636 images\n",
            "Processed 637 images\n",
            "Processed 638 images\n",
            "Processed 639 images\n",
            "Processed 640 images\n",
            "Processed 641 images\n",
            "Processed 642 images\n",
            "Processed 643 images\n",
            "Processed 644 images\n",
            "Processed 645 images\n",
            "Processed 646 images\n",
            "Processed 647 images\n",
            "Processed 648 images\n",
            "Processed 649 images\n",
            "Processed 650 images\n",
            "Processed 651 images\n",
            "Processed 652 images\n",
            "Processed 653 images\n",
            "Processed 654 images\n",
            "Processed 655 images\n",
            "Processed 656 images\n",
            "Processed 657 images\n",
            "Processed 658 images\n",
            "Processed 659 images\n",
            "Processed 660 images\n",
            "Processed 661 images\n",
            "Processed 662 images\n",
            "Processed 663 images\n",
            "Processed 664 images\n",
            "Processed 665 images\n",
            "Processed 666 images\n",
            "Processed 667 images\n",
            "Processed 668 images\n",
            "Processed 669 images\n",
            "Processed 670 images\n",
            "Processed 671 images\n",
            "Processed 672 images\n",
            "Processed 673 images\n",
            "Processed 674 images\n",
            "Processed 675 images\n",
            "Processed 676 images\n",
            "Processed 677 images\n",
            "Processed 678 images\n",
            "Processed 679 images\n",
            "Processed 680 images\n",
            "Processed 681 images\n",
            "Processed 682 images\n",
            "Processed 683 images\n",
            "Processed 684 images\n",
            "Processed 685 images\n",
            "Processed 686 images\n",
            "Processed 687 images\n",
            "Processed 688 images\n",
            "Processed 689 images\n",
            "Processed 690 images\n",
            "Processed 691 images\n",
            "Processed 692 images\n",
            "Processed 693 images\n",
            "Processed 694 images\n",
            "Processed 695 images\n",
            "Processed 696 images\n",
            "Processed 697 images\n",
            "Processed 698 images\n",
            "Processed 699 images\n",
            "Processed 700 images\n",
            "Processed 701 images\n",
            "Processed 702 images\n",
            "Processed 703 images\n",
            "Processed 704 images\n",
            "Processed 705 images\n",
            "Processed 706 images\n",
            "Processed 707 images\n",
            "Processed 708 images\n",
            "Processed 709 images\n",
            "Processed 710 images\n",
            "Processed 711 images\n",
            "Processed 712 images\n",
            "Processed 713 images\n",
            "Processed 714 images\n",
            "Processed 715 images\n",
            "Processed 716 images\n",
            "Processed 717 images\n",
            "Processed 718 images\n",
            "Processed 719 images\n",
            "Processed 720 images\n",
            "Processed 721 images\n",
            "Processed 722 images\n",
            "Processed 723 images\n",
            "Processed 724 images\n",
            "Processed 725 images\n",
            "Processed 726 images\n",
            "Processed 727 images\n",
            "Processed 728 images\n",
            "Processed 729 images\n",
            "Processed 730 images\n",
            "Processed 731 images\n",
            "Processed 732 images\n",
            "Processed 733 images\n",
            "Processed 734 images\n",
            "Processed 735 images\n",
            "Processed 736 images\n",
            "Processed 737 images\n",
            "Processed 738 images\n",
            "Processed 739 images\n",
            "Processed 740 images\n",
            "Processed 741 images\n",
            "Processed 742 images\n",
            "Processed 743 images\n",
            "Processed 744 images\n",
            "Processed 745 images\n",
            "Processed 746 images\n",
            "Processed 747 images\n",
            "Processed 748 images\n",
            "Processed 749 images\n",
            "Processed 750 images\n",
            "Processed 751 images\n",
            "Processed 752 images\n",
            "Processed 753 images\n",
            "Processed 754 images\n",
            "Processed 755 images\n",
            "Processed 756 images\n",
            "Processed 757 images\n",
            "Processed 758 images\n",
            "Processed 759 images\n",
            "Processed 760 images\n",
            "Processed 761 images\n",
            "Processed 762 images\n",
            "Processed 763 images\n",
            "Processed 764 images\n",
            "Processed 765 images\n",
            "Processed 766 images\n",
            "Processed 767 images\n",
            "Processed 768 images\n",
            "Processed 769 images\n",
            "Processed 770 images\n",
            "Processed 771 images\n",
            "Processed 772 images\n",
            "Processed 773 images\n",
            "Processed 774 images\n",
            "Processed 775 images\n",
            "Processed 776 images\n",
            "Processed 777 images\n",
            "Processed 778 images\n",
            "Processed 779 images\n",
            "Processed 780 images\n",
            "Processed 781 images\n",
            "Processed 782 images\n",
            "Processed 783 images\n",
            "Processed 784 images\n",
            "Processed 785 images\n",
            "Processed 786 images\n",
            "Processed 787 images\n",
            "Processed 788 images\n",
            "Processed 789 images\n",
            "Processed 790 images\n",
            "Processed 791 images\n",
            "Processed 792 images\n",
            "Processed 793 images\n",
            "Processed 794 images\n",
            "Processed 795 images\n",
            "Processed 796 images\n",
            "Processed 797 images\n",
            "Processed 798 images\n",
            "Processed 799 images\n",
            "Processed 800 images\n",
            "Processed 801 images\n",
            "Processed 802 images\n",
            "Processed 803 images\n",
            "Processed 804 images\n",
            "Processed 805 images\n",
            "Processed 806 images\n",
            "Processed 807 images\n",
            "Processed 808 images\n",
            "Processed 809 images\n",
            "Processed 810 images\n",
            "Processed 811 images\n",
            "Processed 812 images\n",
            "Processed 813 images\n",
            "Processed 814 images\n",
            "Processed 815 images\n",
            "Processed 816 images\n",
            "Processed 817 images\n",
            "Processed 818 images\n",
            "Processed 819 images\n",
            "Processed 820 images\n",
            "Processed 821 images\n",
            "Processed 822 images\n",
            "Processed 823 images\n",
            "Processed 824 images\n",
            "Processed 825 images\n",
            "Processed 826 images\n",
            "Processed 827 images\n",
            "Processed 828 images\n",
            "Processed 829 images\n",
            "Processed 830 images\n",
            "Processed 831 images\n",
            "Processed 832 images\n",
            "Processed 833 images\n",
            "Processed 834 images\n",
            "Processed 835 images\n",
            "Processed 836 images\n",
            "Processed 837 images\n",
            "Processed 838 images\n",
            "Processed 839 images\n",
            "Processed 840 images\n",
            "Processed 841 images\n",
            "Processed 842 images\n",
            "Processed 843 images\n",
            "Processed 844 images\n",
            "Processed 845 images\n",
            "Processed 846 images\n",
            "Processed 847 images\n",
            "Processed 848 images\n",
            "Processed 849 images\n",
            "Processed 850 images\n",
            "Processed 851 images\n",
            "Processed 852 images\n",
            "Processed 853 images\n",
            "Processed 854 images\n",
            "Processed 855 images\n",
            "Processed 856 images\n",
            "Processed 857 images\n",
            "Processed 858 images\n",
            "Processed 859 images\n",
            "Processed 860 images\n",
            "Processed 861 images\n",
            "Processed 862 images\n",
            "Processed 863 images\n",
            "Processed 864 images\n",
            "Processed 865 images\n",
            "Processed 866 images\n",
            "Processed 867 images\n",
            "Processed 868 images\n",
            "Processed 869 images\n",
            "Processed 870 images\n",
            "Processed 871 images\n",
            "Processed 872 images\n",
            "Processed 873 images\n",
            "Processed 874 images\n",
            "Processed 875 images\n",
            "Processed 876 images\n",
            "Processed 877 images\n",
            "Processed 878 images\n",
            "Processed 879 images\n",
            "Processed 880 images\n",
            "Processed 881 images\n",
            "Processed 882 images\n",
            "Processed 883 images\n",
            "Processed 884 images\n",
            "Processed 885 images\n",
            "Processed 886 images\n",
            "Processed 887 images\n",
            "Processed 888 images\n",
            "Processed 889 images\n",
            "Processed 890 images\n",
            "Processed 891 images\n",
            "Processed 892 images\n",
            "Processed 893 images\n",
            "Processed 894 images\n",
            "Processed 895 images\n",
            "Processed 896 images\n",
            "Processed 897 images\n",
            "Processed 898 images\n",
            "Processed 899 images\n",
            "Processed 900 images\n",
            "Processed 901 images\n",
            "Processed 902 images\n",
            "Processed 903 images\n",
            "Processed 904 images\n",
            "Processed 905 images\n",
            "Processed 906 images\n",
            "Processed 907 images\n",
            "Processed 908 images\n",
            "Processed 909 images\n",
            "Processed 910 images\n",
            "Processed 911 images\n",
            "Processed 912 images\n",
            "Processed 913 images\n",
            "Processed 914 images\n",
            "Processed 915 images\n",
            "Processed 916 images\n",
            "Processed 917 images\n",
            "Processed 918 images\n",
            "Processed 919 images\n",
            "Processed 920 images\n",
            "Processed 921 images\n",
            "Processed 922 images\n",
            "Processed 923 images\n",
            "Processed 924 images\n",
            "Processed 925 images\n",
            "Processed 926 images\n",
            "Processed 927 images\n",
            "Processed 928 images\n",
            "Processed 929 images\n",
            "Processed 930 images\n",
            "Processed 931 images\n",
            "Processed 932 images\n",
            "Processed 933 images\n",
            "Processed 934 images\n",
            "Processed 935 images\n",
            "Processed 936 images\n",
            "Processed 937 images\n",
            "Processed 938 images\n",
            "Processed 939 images\n",
            "Processed 940 images\n",
            "Processed 941 images\n",
            "Processed 942 images\n",
            "Processed 943 images\n",
            "Processed 944 images\n",
            "Processed 945 images\n",
            "Processed 946 images\n",
            "Processed 947 images\n",
            "Processed 948 images\n",
            "Processed 949 images\n",
            "Processed 950 images\n",
            "Processed 951 images\n",
            "Processed 952 images\n",
            "Processed 953 images\n",
            "Processed 954 images\n",
            "Processed 955 images\n",
            "Processed 956 images\n",
            "Processed 957 images\n",
            "Processed 958 images\n",
            "Processed 959 images\n",
            "Processed 960 images\n",
            "Processed 961 images\n",
            "Processed 962 images\n",
            "Processed 963 images\n",
            "Processed 964 images\n",
            "Processed 965 images\n",
            "Processed 966 images\n",
            "Processed 967 images\n",
            "Processed 968 images\n",
            "Processed 969 images\n",
            "Processed 970 images\n",
            "Processed 971 images\n",
            "Processed 972 images\n",
            "Processed 973 images\n",
            "Processed 974 images\n",
            "Processed 975 images\n",
            "Processed 976 images\n",
            "Processed 977 images\n",
            "Processed 978 images\n",
            "Processed 979 images\n",
            "Processed 980 images\n",
            "Processed 981 images\n",
            "Processed 982 images\n",
            "Processed 983 images\n",
            "Processed 984 images\n",
            "Processed 985 images\n",
            "Processed 986 images\n",
            "Processed 987 images\n",
            "Processed 988 images\n",
            "Processed 989 images\n",
            "Processed 990 images\n",
            "Processed 991 images\n",
            "Processed 992 images\n",
            "Processed 993 images\n",
            "Processed 994 images\n",
            "Processed 995 images\n",
            "Processed 996 images\n",
            "Processed 997 images\n",
            "Processed 998 images\n",
            "Processed 999 images\n",
            "Processed 1000 images\n",
            "Processed 1001 images\n",
            "Processed 1002 images\n",
            "Processed 1003 images\n",
            "Processed 1004 images\n",
            "Processed 1005 images\n",
            "Processed 1006 images\n",
            "Processed 1007 images\n",
            "Processed 1008 images\n",
            "Processed 1009 images\n",
            "Processed 1010 images\n",
            "Processed 1011 images\n",
            "Processed 1012 images\n",
            "Processed 1013 images\n",
            "Processed 1014 images\n",
            "Processed 1015 images\n",
            "Processed 1016 images\n",
            "Processed 1017 images\n",
            "Processed 1018 images\n",
            "Processed 1019 images\n",
            "Processed 1020 images\n",
            "Processed 1021 images\n",
            "Processed 1022 images\n",
            "Processed 1023 images\n",
            "Processed 1024 images\n",
            "Processed 1025 images\n",
            "Processed 1026 images\n",
            "Processed 1027 images\n",
            "Processed 1028 images\n",
            "Processed 1029 images\n",
            "Processed 1030 images\n",
            "Processed 1031 images\n",
            "Processed 1032 images\n",
            "Activations for layer fc2 saved to /content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_no_patch/bg_fc2_activations_no_patch.npy\n",
            "Calculating neuron activations for patch images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Saved patch individual activations to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/bg_fc2_sparse_outputs/bg_patch_individual_neuron_activations.csv and averages to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/bg_fc2_sparse_outputs/bg_patch_average_neuron_activations.csv\n",
            "Calculating neuron activations for no_patch images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Saved no_patch individual activations to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/bg_fc2_sparse_outputs/bg_no_patch_individual_neuron_activations.csv and averages to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/bg_fc2_sparse_outputs/bg_no_patch_average_neuron_activations.csv\n",
            "Calculating absolute difference in activations...\n",
            "Saved neuron differences to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/bg_fc2_sparse_outputs/neuron_absolute_differences.csv\n",
            "Saved top 10% neurons with highest differences to /content/drive/MyDrive/Masterthesis/Datasets/mnist/outputs/bg_fc2_sparse_outputs/bg_top_10_percent_neurons.csv\n",
            "Muting top neurons and classifying patched images...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Applying muting in sparse space...\n",
            "Classifying non-patched images without muting neurons...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Accuracy of classifications with muted neurons matching non-muted classifications: 0.00%\n",
            "Metrics for 'two with patch' class with muting:\n",
            "  Accuracy: 1.00\n",
            "\n",
            "Metrics for 'two with patch' class without muting:\n",
            "  Accuracy: 0.00\n",
            "Elmafrood tala3 7aga\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classifying decoded activations for 'two with no patch' after projecting into sparse space and decoding without muting...\n",
            "\n",
            "Classifying decoded activations for 'two with patch' after projecting into sparse space and decoding with muting...\n",
            "\n",
            "Classifying decoded activations for 'two with patch' after projecting into sparse space and decoding without muting...\n",
            "Accuracy of 'two with no patch' decoded activations after sparse projection and no muting: 0.99419\n",
            "Accuracy of 'two with patch' decoded activations after sparse projection and no muting: 0.99419\n",
            "Accuracy of 'two with patch' decoded activations after sparse projection and muting: 1.00000\n",
            "Top neurons and their differences: 7216    1.563951\n",
            "7527    1.414505\n",
            "7167    1.397315\n",
            "6756    1.287517\n",
            "8134    1.228547\n",
            "          ...   \n",
            "5465    0.358660\n",
            "4933    0.358117\n",
            "3538    0.357912\n",
            "3323    0.357901\n",
            "3243    0.357396\n",
            "Length: 819, dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def identify_patch_specific_neurons(avg_activations_patch, avg_activations_no_patch, patch_threshold=0.2, no_patch_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Identify neurons that are highly activated for patched images but not for non-patched ones.\n",
        "    \"\"\"\n",
        "    print(\"Identifying neurons selectively activated by patches...\")\n",
        "\n",
        "    # Calculate activation differences without normalization\n",
        "    high_patch_activation = avg_activations_patch > patch_threshold\n",
        "    low_no_patch_activation = avg_activations_no_patch < no_patch_threshold\n",
        "\n",
        "    # Select neurons that meet both criteria\n",
        "    patch_specific_neurons = np.where(high_patch_activation & low_no_patch_activation)[0]\n",
        "\n",
        "    # Debugging: Print some stats to understand what's happening\n",
        "    print(f\"Average activation for patch: {avg_activations_patch.mean():.4f}, No patch: {avg_activations_no_patch.mean():.4f}\")\n",
        "    print(f\"Number of neurons with high activation for patch: {(high_patch_activation).sum()}\")\n",
        "    print(f\"Number of neurons with low activation for no patch: {(low_no_patch_activation).sum()}\")\n",
        "    print(f\"Found {len(patch_specific_neurons)} patch-specific neurons.\")\n",
        "\n",
        "    return patch_specific_neurons\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def save_top_neurons_to_csv(abs_diff, top_neurons, folder_name, filename=\"bg_top_neurons.csv\"):\n",
        "    \"\"\"\n",
        "    Save the top neurons with their difference values to a CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"Saving top neurons to CSV file: {filename}\")\n",
        "\n",
        "    # Create a DataFrame with neuron indices and their absolute differences\n",
        "    neuron_data = pd.DataFrame({\n",
        "        \"Neuron_Index\": range(len(abs_diff)),\n",
        "        \"Activation_Difference\": abs_diff\n",
        "    })\n",
        "\n",
        "    # Mark whether each neuron is in the top 10%\n",
        "    neuron_data[\"Selected_for_Muting\"] = neuron_data[\"Neuron_Index\"].isin(top_neurons)\n",
        "\n",
        "    # Sort by absolute difference in descending order\n",
        "    neuron_data.sort_values(by=\"Activation_Difference\", ascending=False, inplace=True)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_path = os.path.join(folder_name, filename)\n",
        "    neuron_data.to_csv(csv_path, index=False)\n",
        "    print(f\"CSV saved at: {csv_path}\")\n",
        "\n",
        "def classify_with_alexnet(model, activations):\n",
        "    \"\"\"\n",
        "    Classify images using the original AlexNet classifier on the fc2 activations.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for activation in activations:\n",
        "        # Convert numpy activation to tensor\n",
        "        output = model.fc3(torch.from_numpy(activation).float().to(device))\n",
        "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load the pre-trained models\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt\"\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Define paths to pre-saved activations\n",
        "    activation_patch_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/bg_fc2_activations_patch.npy\"\n",
        "    activation_no_patch_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_no_patch/bg_fc2_activations_no_patch.npy\"\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    folder_name = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/difference_analysis\"\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "    # Load pre-saved activations\n",
        "    print(f\"Loading pre-saved AlexNet activations for fc2_activations_patch...\")\n",
        "    activations_patch = np.load(activation_patch_path, allow_pickle=True)\n",
        "    print(f\"Loading pre-saved AlexNet activations for fc2_activations_no_patch...\")\n",
        "    activations_no_patch = np.load(activation_no_patch_path, allow_pickle=True)\n",
        "\n",
        "    # Direct classification using AlexNet\n",
        "    predictions_patch_alexnet = classify_with_alexnet(model, activations_patch)\n",
        "    accuracy_patch_alexnet = accuracy_score([1] * len(predictions_patch_alexnet), predictions_patch_alexnet)\n",
        "\n",
        "    predictions_no_patch_alexnet = classify_with_alexnet(model, activations_no_patch)\n",
        "    accuracy_no_patch_alexnet = accuracy_score([1] * len(predictions_no_patch_alexnet), predictions_no_patch_alexnet)\n",
        "\n",
        "    # Project activations into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    # Decode the projected activations back to the original space\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    decoded_no_patch = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "\n",
        "    # Calculate the absolute differences between patch and no patch\n",
        "    avg_activations_patch = np.mean(projected_patch, axis=0)\n",
        "    avg_activations_no_patch = np.mean(projected_no_patch, axis=0)\n",
        "    abs_diff = np.abs(avg_activations_patch - avg_activations_no_patch)\n",
        "\n",
        "    # Identify the top 10% neurons with the highest differences\n",
        "    top_neuron_count = int(len(abs_diff) * 0.1)\n",
        "    top_neurons = np.argsort(abs_diff)[-top_neuron_count:]\n",
        "\n",
        "    # Classify 'two_with_patch' without muting\n",
        "    print(\"Classifying 'two_with_patch' without muting neurons...\")\n",
        "    predictions_patch_without_muting = classify_decoded_activations(model, decoded_patch)\n",
        "    accuracy_patch_without_muting = accuracy_score([1] * len(predictions_patch_without_muting), predictions_patch_without_muting)\n",
        "\n",
        "    # Mute the top neurons for 'two_with_patch' and classify\n",
        "    projected_patch[:, top_neurons] = 0\n",
        "    decoded_patch_muted = autoencoder.decoder(torch.from_numpy(projected_patch).to(device).float()).cpu().detach().numpy()\n",
        "    predictions_patch_with_muting = classify_decoded_activations(model, decoded_patch_muted)\n",
        "    accuracy_patch_with_muting = accuracy_score([1] * len(predictions_patch_with_muting), predictions_patch_with_muting)\n",
        "\n",
        "    # Classify 'two_no_patch' without muting\n",
        "    print(\"Classifying 'two_no_patch' without muting neurons...\")\n",
        "    predictions_no_patch_without_muting = classify_decoded_activations(model, decoded_no_patch)\n",
        "    accuracy_no_patch_without_muting = accuracy_score([1] * len(predictions_no_patch_without_muting), predictions_no_patch_without_muting)\n",
        "\n",
        "    # Mute the top neurons for 'two_no_patch' and classify\n",
        "    projected_no_patch[:, top_neurons] = 0\n",
        "    decoded_no_patch_muted = autoencoder.decoder(torch.from_numpy(projected_no_patch).to(device).float()).cpu().detach().numpy()\n",
        "    predictions_no_patch_with_muting = classify_decoded_activations(model, decoded_no_patch_muted)\n",
        "    accuracy_no_patch_with_muting = accuracy_score([1] * len(predictions_no_patch_with_muting), predictions_no_patch_with_muting)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"\\nClassification Accuracy Results:\")\n",
        "    print(f\"1. Accuracy (two_with_patch using AlexNet directly): {accuracy_patch_alexnet:.4f}\")\n",
        "    print(f\"2. Accuracy (two_no_patch using AlexNet directly): {accuracy_no_patch_alexnet:.4f}\")\n",
        "    print(f\"3. Accuracy (two_with_patch without muting): {accuracy_patch_without_muting:.4f}\")\n",
        "    print(f\"4. Accuracy (two_with_patch with muting): {accuracy_patch_with_muting:.4f}\")\n",
        "    print(f\"5. Accuracy (two_no_patch without muting): {accuracy_no_patch_without_muting:.4f}\")\n",
        "    print(f\"6. Accuracy (two_no_patch with muting): {accuracy_no_patch_with_muting:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwXpuLTxyiA_",
        "outputId": "b777f658-a10a-41ac-c0de-18c8f9c457eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-df6ee770a840>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and layers up to fc2 are frozen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-99d18b6783c5>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(save_sae_dir))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder loaded and frozen successfully\n",
            "Loading pre-saved AlexNet activations for fc2_activations_patch...\n",
            "Loading pre-saved AlexNet activations for fc2_activations_no_patch...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Projecting Alexnet activations into SAE sparse space...\n",
            "Classifying 'two_with_patch' without muting neurons...\n",
            "Classifying 'two_no_patch' without muting neurons...\n",
            "\n",
            "Classification Accuracy Results:\n",
            "1. Accuracy (two_with_patch using AlexNet directly): 0.0000\n",
            "2. Accuracy (two_no_patch using AlexNet directly): 0.9971\n",
            "3. Accuracy (two_with_patch without muting): 0.0000\n",
            "4. Accuracy (two_with_patch with muting): 1.0000\n",
            "5. Accuracy (two_no_patch without muting): 0.9942\n",
            "6. Accuracy (two_no_patch with muting): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Something to think about when using laster a larget sparse space...now I only have 8k neurons in the sparse space:**\n",
        "\n",
        "In the sparse space, not all neurons are consistently activated across all images. For example, a neuron might remain inactive (close to zero) in most images, but activate strongly for a few specific images, such as those containing spurious features like patches. When we take the average activation of that neuron across all images, the low values from the inactive images will dominate, resulting in a low overall average. This averaging process can therefore obscure the true impact of that neuron in encoding the patch feature, leading to a misleadingly low indication of its importance. The concern is that by using the average activation values in this way, we might be overlooking neurons that are actually sensitive to the spurious features but appear unimportant due to their sparsity. This could affect the accuracy of our results, particularly in identifying which neurons are encoding spurious features."
      ],
      "metadata": {
        "id": "6x8_C4fALkVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BPLx2cFip-as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double checking the test accuracies without projecting into sparse space"
      ],
      "metadata": {
        "id": "E71oThuo54af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LZPWGAfH59k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model on activations"
      ],
      "metadata": {
        "id": "n2gL7KGKQKKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NpyActivationsDataset(Dataset):\n",
        "    def __init__(self, activation_path: str, is_two: int):\n",
        "        # Load the entire numpy file into memory\n",
        "        self.activations = np.load(activation_path)\n",
        "        self.labels = [is_two] * len(self.activations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        activation = torch.tensor(self.activations[idx], dtype=torch.float32)\n",
        "        label = self.labels[idx]\n",
        "        return activation, label, idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.activations)\n",
        "\n",
        "# Specify the path to the .npy activations file\n",
        "activation_patch_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/test_patch/bg_fc2_activations_patch.npy\"\n"
      ],
      "metadata": {
        "id": "OlYvU57lI2w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model_on_activations(model, dataloader, device, output_csv='predictions.csv'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for activations, labels, indices in tqdm(dataloader):\n",
        "            activations, labels = activations.to(device), labels.to(device)\n",
        "\n",
        "            # Pass the pre-saved activations through fc3 only\n",
        "            outputs = model.fc3(activations)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Store predictions with index as reference\n",
        "            for idx, pred in zip(indices, preds):\n",
        "                predictions.append((idx.item(), pred.item()))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = running_corrects / total_samples\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    df = pd.DataFrame(predictions, columns=['index', 'predicted_class'])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "Z3y_6VV_I4yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the dataset using the pre-saved .npy activations\n",
        "test_dataset = NpyActivationsDataset(activation_patch_path, is_two=1)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load the trained model\n",
        "model = AlexNet()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt'))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate using the pre-saved activations\n",
        "evaluate_model_on_activations(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaKj1xQrI6_-",
        "outputId": "1d77b59f-b876-40c8-833c-1ec386be3237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-6fcf10831c53>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt'))\n",
            "100%|| 33/33 [00:00<00:00, 85.36it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0000\n",
            "Predictions saved to predictions.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model on raw images"
      ],
      "metadata": {
        "id": "6r25M2cwQPdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, path: str, is_two: int):\n",
        "        self.resize_shape = (64, 64)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.resize_shape),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.path = path\n",
        "        self.data_files = os.listdir(self.path)\n",
        "        self.labels = [is_two] * len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_path = os.path.join(self.path, self.data_files[i])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[i]\n",
        "        return img, label, self.data_files[i]  # Return the filename as a string\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, width_mult=1):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),  # 96*55*55 (for 224x224 input)\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 96*27*27\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),  # 256*27*27\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 256*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),  # 384*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),  # 384*13*13\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 256*13*13\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # 256*6*6\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)  # 1000 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        #print(\"After layer1:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer2(x)\n",
        "        #print(\"After layer2:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer3(x)\n",
        "        #print(\"After layer3:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer4(x)\n",
        "        #print(\"After layer4:\", x.mean().item(), x.std().item())\n",
        "        x = self.layer5(x)\n",
        "        #print(\"After layer5:\", x.mean().item(), x.std().item())\n",
        "        x = x.view(-1, 256 * 1 * 1)\n",
        "        x = self.fc1(x)\n",
        "        #print(\"After fc1:\", x.mean().item(), x.std().item())\n",
        "        x = self.fc2(x)\n",
        "        #print(\"After fc2:\", x.mean().item(), x.std().item())\n",
        "        x = self.fc3(x)\n",
        "        #print(\"After fc3 (output):\", x.mean().item(), x.std().item())\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "FsF5-UJlp_fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(model, dataloader, device, output_csv='predictions.csv'):\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, filenames in tqdm(dataloader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Store image names and predictions correctly\n",
        "            for img_name, pred in zip(filenames, preds):\n",
        "                predictions.append((img_name, pred.item()))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = running_corrects / total_samples\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Convert to a DataFrame and save to CSV\n",
        "    df = pd.DataFrame(predictions, columns=['image_name', 'predicted_class'])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gdUCbBq7pbT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_dataset = MnistDataset(path='/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/background/test/class_2', is_two=1)  # Assuming is_two is 1 for class 2\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "# Load the trained model\n",
        "model = AlexNet()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt'))\n",
        "model.to(device)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Run the evaluation and save predictions to CSV\n",
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAP75Pe9pctU",
        "outputId": "86f4b6e9-8fb1-48aa-d48d-04b5414dcb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-6920bdb85cd4>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt'))\n",
            "100%|| 516/516 [00:07<00:00, 68.67it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0000\n",
            "Predictions saved to predictions.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stophere"
      ],
      "metadata": {
        "id": "ho5W3seN_haQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archives"
      ],
      "metadata": {
        "id": "Z1Cxtd4T_fIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to Google Drive locations for pre-saved activations\n",
        "def get_activation_path(folder_name, filename):\n",
        "    return f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/{folder_name}/{filename}.npy'\n",
        "\n",
        "\n",
        "# Extract activations for fc2 (layer 6)\n",
        "def extract_fc2_activations(model, dataloader):\n",
        "    print(\"Extracting Alexnet activations for layer fc2...\")\n",
        "    activations = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensor in dataloader:\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            tensor = model.layer5(model.layer4(model.layer3(model.layer2(model.layer1(image_tensor)))))\n",
        "            tensor = tensor.view(-1, 256 * 1 * 1)\n",
        "            tensor = model.fc2(model.fc1(tensor))\n",
        "            activations.append(tensor.cpu().numpy())\n",
        "            print(f\"Processed {len(activations)} images\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return np.vstack(activations)\n",
        "\n",
        "\n",
        "# Function to load activations if they exist or extract and save them if not\n",
        "def load_or_extract_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    activation_path = get_activation_path(folder_name, filename)\n",
        "\n",
        "    # Check if the activation file already exists\n",
        "    if os.path.exists(activation_path):\n",
        "        print(f\"Loading pre-saved Alexnet activations for {filename} from {activation_path}...\")\n",
        "        activations = np.load(activation_path, allow_pickle=True)\n",
        "    else:\n",
        "        print(f\"No pre-saved Alexnet activations found for {filename}. Extracting and saving...\")\n",
        "        activations = extract_fc2_activations(model, dataloader)\n",
        "        os.makedirs(os.path.dirname(activation_path), exist_ok=True)\n",
        "        np.save(activation_path, activations)\n",
        "        print(f\"Activations for layer fc2 saved to {activation_path}\")\n",
        "\n",
        "    return activations\n",
        "\n",
        "\n",
        "# Extract and save activations for fc2 (layer 6) to Google Drive\n",
        "def extract_and_save_fc2_activations(model, dataloader, folder_name, filename):\n",
        "    print(\"Extracting and saving Alexnet activations for layer fc2...\")\n",
        "    # Extract activations using the existing function\n",
        "    activations = extract_fc2_activations(model, dataloader)\n",
        "\n",
        "    # Define the save path in Google Drive\n",
        "    drive_path = f'/content/drive/MyDrive/Masterthesis/Datasets/mnist/activations/{folder_name}/{filename}.npy'\n",
        "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "\n",
        "    # Save the activations as a .npy file\n",
        "    np.save(drive_path, activations)\n",
        "    print(f\"Alexnet Activations for layer fc2 saved to {drive_path}\")\n",
        "\n",
        "# Project activations into sparse space\n",
        "def project_activations(autoencoder, activations, device):\n",
        "    print(\"Projecting Alexnet activations into SAE sparse space...\")\n",
        "    with torch.no_grad():\n",
        "        projected = autoencoder.encoder(torch.from_numpy(activations).to(device).float())\n",
        "    return projected.cpu().numpy()\n",
        "\n",
        "# Mean Activation Difference\n",
        "def mean_activation_difference(projected_patch, projected_no_patch, top_k=10):\n",
        "    print(\"Calculating mean sparse activations difference...\")\n",
        "    mean_diff = np.abs(projected_patch.mean(axis=0) - projected_no_patch.mean(axis=0))\n",
        "    top_neurons = np.argsort(mean_diff)[-top_k:]\n",
        "    return top_neurons\n",
        "\n",
        "# Statistical Significance Testing\n",
        "def statistical_testing_neurons(projected_patch, projected_no_patch, threshold=0.05):\n",
        "    print(\"Performing statistical testing...\")\n",
        "    significant_neurons = []\n",
        "    for i in range(projected_patch.shape[1]):\n",
        "        _, p_value = ttest_ind(projected_patch[:, i], projected_no_patch[:, i], equal_var=False)\n",
        "        if p_value < threshold:\n",
        "            significant_neurons.append(i)\n",
        "    return significant_neurons\n",
        "\n",
        "# Use AlexNet's own FC weights to identify patch-relevant neurons\n",
        "def patch_classifier_importance(model, projected_patch, projected_no_patch, top_k=10):\n",
        "    print(\"Using AlexNet's FC layer weights to identify important neurons...\")\n",
        "    # Extract weights from the final fully connected layer (fc3 as output)\n",
        "    importance = np.abs(model.fc3.weight.cpu().detach().numpy()[0])  # Take absolute values of weights\n",
        "\n",
        "    # Sort by importance and get the top K neurons\n",
        "    top_neurons = np.argsort(importance)[-top_k:]\n",
        "    print(\"Top neurons identified based on AlexNet's weights:\", top_neurons)\n",
        "    return top_neurons\n",
        "\n",
        "\n",
        "# Correlation Analysis\n",
        "def correlation_analysis(projected_patch, projected_no_patch, top_k=10):\n",
        "    combined = np.vstack([projected_patch, projected_no_patch])\n",
        "    patch_condition = np.hstack([np.ones(len(projected_patch)), np.zeros(len(projected_no_patch))])\n",
        "    correlations = [pearsonr(combined[:, i], patch_condition)[0] for i in range(combined.shape[1])]\n",
        "    top_neurons = np.argsort(np.abs(correlations))[-top_k:]\n",
        "    return top_neurons\n",
        "\n",
        "# Visualize Top Neurons\n",
        "def visualize_neurons(neuron_indexes_dict):\n",
        "    # Convert neuron indexes to a DataFrame, filling missing values with NaN\n",
        "    neuron_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in neuron_indexes_dict.items()]))\n",
        "\n",
        "    # Plot each method's top neuron indexes using a heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(neuron_df, annot=True, fmt=\".0f\", cmap=\"viridis\",  # Use .0f to handle NaN values\n",
        "                cbar=True, yticklabels=False)\n",
        "\n",
        "    plt.xlabel(\"Methods\")\n",
        "    plt.ylabel(\"Top Neurons\")\n",
        "    plt.title(\"Comparison of Silenced Neurons Across Methods\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def silence_and_classify(autoencoder, model, activations_patch, selected_neurons):\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    silenced_patch = np.copy(projected_patch)\n",
        "    silenced_patch[:, selected_neurons] = 0\n",
        "    decoded_patch = autoencoder.decoder(torch.from_numpy(silenced_patch).to(device).float()).detach().cpu().numpy()\n",
        "\n",
        "    # Decode and classify with AlexNet softmax\n",
        "    predictions = []\n",
        "    for decoded_activation in decoded_patch:\n",
        "        decoded_tensor = torch.from_numpy(decoded_activation).float().to(device)\n",
        "        output = model.fc3(decoded_tensor)  # After fc2, apply fc3 for classification\n",
        "        prediction = torch.argmax(F.softmax(output, dim=0)).item()\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Helper function to calculate accuracy\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = sum([1 if pred == label else 0 for pred, label in zip(predictions, labels)])\n",
        "    return correct / len(labels) * 100  # Returns accuracy percentage\n",
        "\n",
        "# Check overlap in silenced neurons across methods\n",
        "def check_neuron_overlap(silenced_neurons_dict):\n",
        "    methods = list(silenced_neurons_dict.keys())\n",
        "    overlap_counts = {}\n",
        "\n",
        "    for i, method1 in enumerate(methods):\n",
        "        for method2 in methods[i + 1:]:\n",
        "            overlap = set(silenced_neurons_dict[method1]).intersection(silenced_neurons_dict[method2])\n",
        "            overlap_counts[f\"{method1} & {method2}\"] = len(overlap)\n",
        "\n",
        "    print(\"Neuron Overlap Across Methods:\")\n",
        "    for pair, count in overlap_counts.items():\n",
        "        print(f\"{pair}: {count} neurons\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ij4kzdO_OC-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory path for saving outputs\n",
        "output_dir = Path(\"/content/drive/MyDrive/Masterthesis/Datasets/mnist/muted_sparse_sae/outputs\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main function for all methods\n",
        "def main():\n",
        "    # Paths and initialization\n",
        "    model_path = \"/content/drive/MyDrive/Masterthesis/Datasets/mnist/models/initial_classifier/alexnet_mnist_bg_cl0_cl2_1train.pt\"\n",
        "    patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/background/test/class_2'\n",
        "    no_patch_folder = '/content/drive/MyDrive/Masterthesis/Datasets/mnist/dataset_splits/original/test/class_2'\n",
        "\n",
        "    # Load model and autoencoder\n",
        "    model = load_model(model_path)\n",
        "    autoencoder = load_autoencoder(device)\n",
        "\n",
        "    # Prepare dataloaders for patched and unpatched datasets\n",
        "    patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "    no_patch_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(no_patch_folder) for file in files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    patch_dataset = ImageDataset(patch_image_paths, transform=preprocess)\n",
        "    no_patch_dataset = ImageDataset(no_patch_image_paths, transform=preprocess)\n",
        "\n",
        "    patch_loader = DataLoader(patch_dataset, batch_size=1, shuffle=False)\n",
        "    no_patch_loader = DataLoader(no_patch_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Conditionally load or extract activations for patched images\n",
        "    activations_patch = load_or_extract_fc2_activations(model, patch_loader, 'test_patch', 'fc2_activations_patch')\n",
        "\n",
        "    # Conditionally load or extract activations for non-patched images\n",
        "    activations_no_patch = load_or_extract_fc2_activations(model, no_patch_loader, 'test_no_patch', 'fc2_activations_no_patch')\n",
        "\n",
        "    # Project into sparse space\n",
        "    projected_patch = project_activations(autoencoder, activations_patch, device)\n",
        "    projected_no_patch = project_activations(autoencoder, activations_no_patch, device)\n",
        "\n",
        "    # Dictionary to store neuron indexes for each method\n",
        "    silenced_neurons_dict = {}\n",
        "\n",
        "    # Method 1: Mean Activation Difference\n",
        "    top_neurons_mean_diff = mean_activation_difference(projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Mean Activation Diff\"] = top_neurons_mean_diff\n",
        "    predictions_mean_diff = silence_and_classify(autoencoder, model, activations_patch, top_neurons_mean_diff)\n",
        "    print(\"Classification Results (Mean Activation Diff):\")\n",
        "    print(predictions_mean_diff)\n",
        "\n",
        "    # Method 2: Statistical Testing\n",
        "    top_neurons_stat_test = statistical_testing_neurons(projected_patch, projected_no_patch, threshold=0.05)\n",
        "    silenced_neurons_dict[\"Statistical Test\"] = top_neurons_stat_test\n",
        "    predictions_stat_test = silence_and_classify(autoencoder, model, activations_patch, top_neurons_stat_test)\n",
        "    print(\"Classification Results (Statistical Test):\")\n",
        "    print(predictions_stat_test)\n",
        "\n",
        "    # Method 3: Patch Classifier (Using AlexNet's FC Layer Weights)\n",
        "    top_neurons_classifier = patch_classifier_importance(model, projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Patch Classifier\"] = top_neurons_classifier\n",
        "    predictions_classifier = silence_and_classify(autoencoder, model, activations_patch, top_neurons_classifier)\n",
        "    print(\"Classification Results (Patch Classifier):\")\n",
        "    print(predictions_classifier)\n",
        "\n",
        "    # Method 4: Correlation Analysis\n",
        "    top_neurons_correlation = correlation_analysis(projected_patch, projected_no_patch, top_k=10)\n",
        "    silenced_neurons_dict[\"Correlation Analysis\"] = top_neurons_correlation\n",
        "    predictions_correlation = silence_and_classify(autoencoder, model, activations_patch, top_neurons_correlation)\n",
        "    print(\"Classification Results (Correlation Analysis):\")\n",
        "    print(predictions_correlation)\n",
        "\n",
        "    # Print silenced neurons by each method\n",
        "    print(\"Silenced neurons by each method:\", silenced_neurons_dict)\n",
        "\n",
        "    # Check neuron overlap across methods\n",
        "    check_neuron_overlap(silenced_neurons_dict)\n",
        "\n",
        "    # Visualize silenced neuron indexes\n",
        "    visualize_neurons(silenced_neurons_dict)\n",
        "\n",
        "# Execute main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "qPEr9jJwOEkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}